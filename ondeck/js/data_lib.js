var lib = [
    {
        "key": "T3MSTN42",
        "version": 65,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/T3MSTN42",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/T3MSTN42",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/A7SX6BSV",
                "type": "application/json"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            }
        },
        "citation": "<span>Amazon.com Link, https://www.amazon.com/Forms-that-Work-Interactive-Technologies/dp/1558607102 (last visited Feb 1, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "A7SX6BSV",
        "version": 65,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/A7SX6BSV",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/A7SX6BSV",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Jarrett et al.",
            "parsedDate": "2008-11-26",
            "numChildren": 1
        },
        "citation": "<span><span style=\"font-variant:small-caps;\">Caroline Jarrett, Gerry Gaffney &#38; Steve Krug</span>, <span style=\"font-variant:small-caps;\">Forms that Work: Designing Web Forms for Usability</span> (1st edition ed. 2008).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "WI6IZ4HG",
        "version": 63,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/WI6IZ4HG",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/WI6IZ4HG",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/KA2ST4NL",
                "type": "application/json"
            },
            "enclosure": {
                "type": "text/html",
                "href": "https://api.zotero.org/groups/4848934/items/WI6IZ4HG/file/view"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            }
        },
        "citation": "<span>Snapshot, https://onlinelibrary.wiley.com/doi/epdf/10.1002/dys.290 (last visited Feb 1, 2023).</span>",
        "fulltext": " \nClose this dialog \nThis website stores data such as cookies to enable essential site functionality, as well as marketing, personalization, and analytics. By remaining on this website you indicate your consent. Privacy Policy \n \nClose Cookie Preferences \nPublication loaded \nerror_outline \nJavaScript disabled \n \nYou have to enable JavaScript in your browser's settings in order to use the eReader. \n \nOr try downloading the content offline \nDOWNLOAD \ninfo list link \n \n    Details \n    Cover Image \n    Dyslexia \n    Volume 11 , Issue 2 \n    May 2005 \n    Pages 79-154 \n    ARTICLE \n    The characteristics of young and adult dyslexics readers on reading and reading related cognitive tasks as compared to normal readers \n    View article page \n    Shelley Miller-Shaul \n    format_quote CITE \n    Copyright \u00a9 2005 John Wiley & Sons, Ltd. \n    https://doi.org/10.1002/dys.290 open_in_new \n    Publisher John Wiley & Sons, Ltd \n    ISSN 1076-9242 \n    eISSN 1099-0909 \n    Online 21 April 2005 \n    Pages 132 - 151 \n    Abstract \n \n    Most of the research into dyslexia has been carried out among children and has raised the question whether the characteristics of young dyslexics are similar to those of adult dyslexics. The aim of this research was, therefore, to confirm whether the cognitive deficits, which appear among young dyslexics on reading and reading related tasks, are similar among adult dyslexics. \n \n    Four groups of subjects were tested in this study: two groups of fourth graders, dyslexic and normal readers, and two groups of students, compensated dyslexics and normal readers. \n \n    A comparison of the differences in research measures between the young dyslexics and their control group, and between the adult dyslexics and their control group, clearly indicates that the difference between regular readers and dyslexics is significantly smaller in the adult group on orthographic tasks, and this difference increases in adults on phonological tasks. \n \n    The findings of this study reinforce the assumption that dyslexics have particular difficulty with the phonological-auditory channel. Another main finding is the slow speed of processing in verbal and non-verbal tasks. It can be assumed that these problems start at a young age and persist in compensated adult dyslexics. Copyright \u00a9 2005 John Wiley & Sons, Ltd. \n    navigate_before BACK list Outline \n    navigate_before BACK perm_media Materials \n    navigate_before BACK link Links \n \nhome wol-logo \nPage 1 / 20 \nformat_size \nfullscreen remove_circle_outline add_circle_outline \nsearch \nSEARCH search \ngroup_add \nmore_horiz \nget_app \nCopyright # 2005 John Wiley & Sons, Ltd. DYSLEXIA 11: 132\u2013151 (2005) Published online in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/dys.290 & The Characteristics of Young and Adult Dyslexics Readers on Reading and Reading Related Cognitive Tasks as Compared to Normal Readers Shelley Miller-Shaul* Neurocognitive Research laboratory, Faculty of Education, University of Haifa, Israel Most of the research into dyslexia has been carried out among children and has raised the question whether the characteristics of young dyslexics are similar to those of adult dyslexics. The aim of this research was, therefore, to confirm whether the cognitive deficits, which appear among young dyslexics on reading and reading related tasks, are similar among adult dyslexics. Four groups of subjects were tested in this study: two groups of fourth graders, dyslexic and normal readers, and two groups of students, compensated dyslexics and normal readers. A comparison of the differences in research measures between the young dyslexics and their control group, and between the adult dyslexics and their control group, clearly indicates that the difference between regular readers and dyslexics is significantly smaller in the adult group on orthographic tasks, and this difference increases in adults on phonological tasks. The findings of this study reinforce the assumption that dyslexics have particular difficulty with the phonological-auditory channel. Another main finding is the slow speed of processing in verbal and non-verbal tasks. It can be assumed that these problems start at a young age and persist in compensated adult dyslexics. Copyright # 2005 John Wiley & Sons, Ltd. Keywords : dyslexia; age differences; cognitive tasks *Correspondence to: S. Miller-Shaul, Laboratory for Neurocognitive Research, Faculty of Education, University of Haifa, Mt. Carmel, Haifa, 31905, Israel. E-mail: shelleys@ construct.haifa.ac.il. \nINTRODUCTION A working definition of dyslexia (British Psychological Society, 1999) has suggested that \u2018dyslexia is evident when accurate and fluent word reading and/or spelling develops very incompletely or with great difficulty\u2019 (p. 18). This difficulty is not a result of low intelligence, neurological defects or the absence of typical learning opportunities. This deficiency adversely affects academic achievements and daily activities involving reading (WHO, 1993). Abundant research has demonstrated the existence of a core phonological deficit in the reading and spelling difficulties of dyslexic children (Share, 1994 for review; Mann & Brady, 1988; Wagner & Torgesen, 1987). Phonological skills require knowledge of the sound structure of language in order to translate the letter into its sound (grapheme to phoneme correspondence). These skills were found to be deficient among dyslexics during several types of tasks such as rhyming, synthesis and analysis of phonemes and non-word reading (Foorman, 1994; Helenius, Uutela, & Hari, 1999; Wagner & Torgesen, 1987). Data also pointed to the fact that dyslexic children exhibit poor and slow orthographic skills (Zecker, 1991; Manis, Szezulski, Holt, & Graves, 1990 ; Bjaalid, Hoien, & Lundberg, 1995). Orthographic processing is the ability to recognize words without relying on their phonology. The identification is accomplished using structural patterns consisting of letter shapes and their unique order in the written word pattern (Corcos & Willows, 1993). These skills are also found to be deficient among dyslexics on tasks such as detection of letter clusters in a word and spelling of exception words (Baker, Torgesen, & Wagner, 1992). More recent studies have suggested that dyslexic readers also exhibit slowness in reading and in performing reading related skills. Dyslexics have been found slower when reading words and non-words, on naming tasks and on phonological and orthographic tasks. (Wolf, Michel, & Ovrut, 1990; Wolf & Bowers, 1999; Nicolson & Fawcett, 1993; Farmer & Klein, 1993; Tallal, Miller, & Fitch, 1993). The majority of the studies carried out in the field of dyslexia have focused on children in the stages of acquiring and establishing their reading skills, when their difficulties start increasing. In the last few years studies have started to deal with adult dyslexics as well. It is common practice to divide the adult dyslexic population diagnosed as dyslexic in childhood into two groups. A group of \u2018compensated\u2019 dyslexics whose reading achievements are good enough for them to continue on to academic education as opposed to the group of \u2018non-compensated\u2019 dyslexics whose reading achievements remain very low, and thus find formal study difficult (Pennington et al ., 1986). The academic skills of compensated dyslexics improve over the years, however, despite the improvement in their reading level they continue to show cognitive deficits (Bruck, 1998). Research dealing with adult \u2018compensated\u2019 dyslexics has claimed that this population remains dyslexic their entire lives (Lefly & Pennington, 1991). This raised the question as to what extent the characteristics of childhood dyslexia are similar to those of adult compensated dyslexia. This is the focus of the present research study. Young and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks 133 Copyright # 2005 John Wiley & Sons, Ltd. DYSLEXIA 11: 132\u2013151 (2005) \nMore [o] \n",
        "hash_id": "f7efd7b644ae7b969523b1d306551312"
    },
    {
        "key": "M87XQX4L",
        "version": 62,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/M87XQX4L",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/M87XQX4L",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/KA2ST4NL",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/M87XQX4L/file/view",
                "title": "Miller-Shaul - 2005 - The characteristics of young and adult dyslexics r.pdf",
                "length": 133209
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/dys.290 (last visited Feb 1, 2023).</span>",
        "fulltext": "10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n& The Characteristics of Young and Adult Dyslexics Readers on Reading and Reading Related Cognitive Tasks as Compared to Normal Readers\nShelley Miller-Shaul*\nNeurocognitive Research laboratory, Faculty of Education, University of Haifa, Israel\nMost of the research into dyslexia has been carried out among children and has raised the question whether the characteristics of young dyslexics are similar to those of adult dyslexics. The aim of this research was, therefore, to confirm whether the cognitive deficits, which appear among young dyslexics on reading and reading related tasks, are similar among adult dyslexics.\nFour groups of subjects were tested in this study: two groups of fourth graders, dyslexic and normal readers, and two groups of students, compensated dyslexics and normal readers.\nA comparison of the differences in research measures between the young dyslexics and their control group, and between the adult dyslexics and their control group, clearly indicates that the difference between regular readers and dyslexics is significantly smaller in the adult group on orthographic tasks, and this difference increases in adults on phonological tasks.\nThe findings of this study reinforce the assumption that dyslexics have particular difficulty with the phonological-auditory channel. Another main finding is the slow speed of processing in verbal and non-verbal tasks. It can be assumed that these problems start at a young age and persist in compensated adult dyslexics. Copyright # 2005 John Wiley & Sons, Ltd.\nKeywords: dyslexia; age differences; cognitive tasks\n\n*Correspondence to: S. Miller-Shaul, Laboratory for Neurocognitive Research, Faculty of Education, University of Haifa, Mt. Carmel, Haifa, 31905, Israel. E-mail: shelleys@ construct.haifa.ac.il.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\nPublished online in Wiley InterScience (www.interscience.wiley.com). DOI: 10.1002/dys.290\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n133\n\nINTRODUCTION\nA working definition of dyslexia (British Psychological Society, 1999) has suggested that \u2018dyslexia is evident when accurate and fluent word reading and/or spelling develops very incompletely or with great difficulty\u2019 (p. 18). This difficulty is not a result of low intelligence, neurological defects or the absence of typical learning opportunities. This deficiency adversely affects academic achievements and daily activities involving reading (WHO, 1993).\nAbundant research has demonstrated the existence of a core phonological deficit in the reading and spelling difficulties of dyslexic children (Share, 1994 for review; Mann & Brady, 1988; Wagner & Torgesen, 1987). Phonological skills require knowledge of the sound structure of language in order to translate the letter into its sound (grapheme to phoneme correspondence). These skills were found to be deficient among dyslexics during several types of tasks such as rhyming, synthesis and analysis of phonemes and non-word reading (Foorman, 1994; Helenius, Uutela, & Hari, 1999; Wagner & Torgesen, 1987).\nData also pointed to the fact that dyslexic children exhibit poor and slow orthographic skills (Zecker, 1991; Manis, Szezulski, Holt, & Graves, 1990 ; Bjaalid, Hoien, & Lundberg, 1995). Orthographic processing is the ability to recognize words without relying on their phonology. The identification is accomplished using structural patterns consisting of letter shapes and their unique order in the written word pattern (Corcos & Willows, 1993). These skills are also found to be deficient among dyslexics on tasks such as detection of letter clusters in a word and spelling of exception words (Baker, Torgesen, & Wagner, 1992).\nMore recent studies have suggested that dyslexic readers also exhibit slowness in reading and in performing reading related skills. Dyslexics have been found slower when reading words and non-words, on naming tasks and on phonological and orthographic tasks. (Wolf, Michel, & Ovrut, 1990; Wolf & Bowers, 1999; Nicolson & Fawcett, 1993; Farmer & Klein, 1993; Tallal, Miller, & Fitch, 1993).\nThe majority of the studies carried out in the field of dyslexia have focused on children in the stages of acquiring and establishing their reading skills, when their difficulties start increasing. In the last few years studies have started to deal with adult dyslexics as well.\nIt is common practice to divide the adult dyslexic population diagnosed as dyslexic in childhood into two groups. A group of \u2018compensated\u2019 dyslexics whose reading achievements are good enough for them to continue on to academic education as opposed to the group of \u2018non-compensated\u2019 dyslexics whose reading achievements remain very low, and thus find formal study difficult (Pennington et al., 1986).\nThe academic skills of compensated dyslexics improve over the years, however, despite the improvement in their reading level they continue to show cognitive deficits (Bruck, 1998).\nResearch dealing with adult \u2018compensated\u2019 dyslexics has claimed that this population remains dyslexic their entire lives (Lefly & Pennington, 1991). This raised the question as to what extent the characteristics of childhood dyslexia are similar to those of adult compensated dyslexia. This is the focus of the present research study.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n134\n\nS. Miller-Shaul\n\nA slow and inaccurate word recognition process characterizes young dyslexics. The research findings show that dyslexic children have problems with phonological awareness, the ability to recognize the sound structure of the spoken word (Manis, Cus-odio, & Szeszulski, 1993) and a problem segmenting according to sound (Bruck, 1990). Their main phonological difficulty is expressed via phonological decoding (transferring from grapheme to phoneme or from letter to sound). When dyslexic children are required to read rare words or pseudowords their accuracy is lower (Rack, Snowling, & Olson, 1992) and their performance time is longer (Manis et al., 1990) than that of regular readers.\nYoung dyslexics have additional difficulties that are not restricted to the phonological domain. They have short-term verbal memory difficulties (Torgeson, Rashotte, Greenstein, Houck & Portes, 1988), and long-term verbal memory problems (Hulme, 1981). Thus, the dyslexic has difficulty creating word representations in memory and as a result finds it hard to retrieve words from memory (Lovette, 1987).\nAmong other things, it has been found that dyslexic children are unsuccessful at creating orthographic codes to use automatically during reading. Therefore, their word recognition process is not automatic (Bowers, Golden, Kennedy, & Young, 1994). Another area researched in connection to young dyslexics is semantics. It has been found that dyslexic children depend more on context during reading that experienced readers (Stanovich, 1991).\nMany studies have reported a connection between rapid automatic naming (RAN) tasks and reading achievement. This connection tends to be stronger where reading speed is concerned (Doi & Manis, 1996; Manis, Seidenberg, & Doi, 1999; Bowers, 1995, 2001; de Jong & Van der Leij, 1999; Wolf, 2001). A consistent finding regarding the characteristics of young dyslexics is related to the problem that dyslexics have with naming single discrete stimuli and naming sequential stimuli of different types (letters, colours, numbers and familiar objects). Young dyslexics are slower (Bowers, 1994; Bowers & Swanson, 1991) and less accurate (Catts, 1986; Wolf et al., 1990) than experienced readers on these measures.\nIt can be concluded that young dyslexics suffer from a wide range of problems related to different word recognition processes. First and foremost are the phonological processes as well as the orthographic, semantic, memory and other processes. These difficulties are expressed by a word recognition process that is slower and less accurate than that of their peers.\nResearch studies that focused on examining the characteristics of adult dyslexics among the \u2018compensated\u2019 group indicate a number of findings in which this group continued to show lower results than regular reading age matched readers. In a number of studies evidence was found that these adult dyslexics continue to have difficulty with phonological ability. Among other things, it was found that the performance achievement of dyslexics on different phonological tasks continues to be low (Snowling, 1995). They were found to be slower and less accurate than the control group during vocal reading of familiar and unfamiliar, single and multi syllabic words (Bruk, 1990), during pseudoword reading (Ben-Dror, Pollatsek, & Scarpati, 1991), and performance of phoneme segmentation tasks (Brande, 1998). They are insensitive to omissions at the beginning and end of the word (Bruck, 1992). They have a rhyming task difficulty when the stimuli are presented in an auditory manner (Nicolson & Fawcett, 1993) and have low sensitivity to phonological disharmony (Levinthal & Hornurg,\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n135\n\n1992). The ability to acquire phonological decoding remains slower for this group (Yap & Van der Leij, 1993).\nIn a recent study conducted by Ramus et al. (2003) similar findings were found among a group of dyslexic university students. The dyslexics were significantly poor than controls in all measures of literacy: reading speed and accuracy, spelling, pseudoword reading accuracy and time. In addition all dyslexics were poorer than controls in the different phonological tests: their naming time was longer, spoonerisems was less accurate and production time was longer.\nThe data from numerous studies have indicated that dyslexic readers, both children and adults are characterized by a naming speed deficit (see Wolf & Katzir-Cohen, 2001 for review). Traditionally, RAN has been viewed as part of phonological processing abilities. It has been suggested that naming is related to the retrieval of phonological codes from long-term memory (Wagner, Torgesen, Laughon, Simmons, & Rashotte, 1993) or phonological encoding with lexical access (Wagner & Torgesen, 1987).\nIt can be concluded from these studies that the basic phonological ability of compensated dyslexics is similar to that of young readers in the early stages of reading acquisition (Bruck, 1990). They have difficulties in phonological tasks (accuracy and speed of performance) and their naming speed is much longer. There is a claim that adult compensated dyslexics rely more on context, which helps them reduce gaps during reading and thus does not impair their understanding. Therefore, the dyslexics reading time was longer than that of their peers when reading connected text comprised of meaningless words (GrossGlen, Jallad, Nova, Helgen-Lempesis, & Lubs, 1990).\nA contradictory finding is one, which claims that adult dyslexics also have a visual-orthographic processing difficulty. On one hand, different research studies have found that adult dyslexics are weak at perceiving global information and at brain organization that includes visual information. In other words they have a visual perceptual problem that influences the orthographic channel (Lovegrove, 1992). These studies found, among other things, that similarity of orthographic patterns creates confusion in this population and prolongs reaction time in a given task as compared to regular readers (Levinthal & Hornurg, 1992). Some studies have found that spelling difficulties characterize most adult dyslexics (Brunswick, McCrory, Price, Frith, & Frith, 1999; Gallagher, Laxon, Armstrong, & Frith, 1996; Shaywitz et al., 1999). On the other hand, different studies found that because adult dyslexics are sensitive to orthographic patterns their recognition time of words in a letter sequence was found to be similar to that of regular readers (Bruck, 1990).\nEven though examination of this conflicting evidence requires additional research it is possible that these contradictory findings indicate a processing difference in the reading of compensated and non-compensated adult dyslexics. The non-compensated dyslexics have a problem with both the phonological and orthographic channels. These readers are unable to create orthographic patterns for themselves and continue to rely on phonological decoding, even though it is deficient.\nThe group of compensated dyslexics, however, activates an accurate orthographic system that compensates for the deficient phonological system (Ben-Dror et al., 1991; Siegel, Share, & Geva, 1995), but this activation is not as automatic as among regular readers (Bruck, 1990, 1992). They also activate compensation\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n136\n\nS. Miller-Shaul\n\nstrategies within the phonological system to circumvent their phonological difficulty, such as separating the word into syllables (Bruck, 1990). What remains is the compensated group\u2019s difficulty with recognizing rare and pseudowords, which require phonological information in order to be recognized.\nThe central questions of this study are whether the impairments of adult compensated dyslexics are held to be mainly in the phonological domain and moreover whether the differences between dyslexic and age matched regular readers in childhood are similar to those in the adulthood. It can be assumed that differences found in adult dyslexics are likely to reflect long term characteristics of the phenomena, which may represent the core deficit of dyslexia.\nIn an attempt to verify the differences between young and adult dyslexics, this study employed a wide range of reading and reading related measures on a sample of children and compensated adult dyslexics readers and their age matched control groups.\n\nMETHOD\nSubjects Participants included 100 subjects who were divided into 4 groups: 2 groups of children and 2 groups of adults. Each age group included a group of dyslexics and a group of normal aged matched readers.\n\nAdults Dyslexics included 25 compensated (Lefly, 1991) university student dyslexic readers, aged 20\u201327, who were diagnosed as dyslexic in childhood and as dyslexic in adulthood. The subjects were diagnosed upon entering the university in an attempt to receive learning accommodations. The Student Support Service at the University of Haifa, which aids students with learning disabilities, performed the diagnosis. All subjects participating in the study merited receiving learning accommodations. All subjects were at least 1 SD below average in word and pseudoword reading accuracy and rate as compared to the control group.\nNormal readers included 25 normal readers, aged 20\u201327, who were students at Haifa University.\n\nChildren Dyslexics included 25 fourth grade dyslexic children, who were taken from the municipal centre for learning disabilities. All subjects were at least 1 SD below average in word and pseudoword reading accuracy and rate as compared to the control group.\nNormal readers included 25 fourth grade normal readers, who were recruited from a middle class school in the north of Israel.\nSubjects in each age group were matched for age and in the four groups on IQ, gender, SES, and handedness. All subjects were Hebrew speakers (mono-lingual), with normal hearing and vision with no neurological problems. All were paid volunteers.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n137\n\nMEASURES\n\nIn an attempt to measure accuracy and fluency in word reading the present study measures included reading, word recognition and speed of processing measures. The different ages of the participants required adjusting the tests according to age. Some of the measures were similar, and the rest were age matched (the same tests with different items).\nI. General ability: General ability was assessed for all subjects using the Raven Standard Progressive Matrices (Raven, 1974).\nII. Reading tests: For reading level verification all subjects were given several reading tests measuring (a) decoding accuracy, (b) reading time and (c) comprehension.\n\nChildren\n(a) Decoding and (b) Reading time measured by percentage of decoding errors and per word reading time in:\n1. Word list test for children (Sarid, 1997c). A test containing 100 single words which the subject must read one after the other, time and accuracy are measured.\n2. Connected text (a 15 line paragraph total of 150 words) test for children (Sarid, 1997c).\n(c) Reading comprehension measured by percentage of correct items answered in:\nReading comprehension test for fourth graders (Ministry of Education, 1995). This test includes text reading with multiple choice questions about the text.\n\nAdults\n(a) Decoding and (b) Reading time measured by percentage of decoding errors and per word reading in:\n1. Word list test for adults (Shatil, 1995a). A test containing 209 words which the subject has to read, the experimenter measures time and accuracy.\n2. Connected text (a 20 line paragraph total of 200 words) test for adults (Israeli psychometric test (1996)}a test which is used in order to determine who is able to enter the university and to which faculty, it is equivalent to the SAT test in the USA).\n\n(c) Reading comprehension measured by percentage correct items answered in: Reading comprehension test adult version (Psychometric test, 1996). This test\nincludes text reading with multiple choice questions about the text. III. Word recognition skills (a) Lexical decision. 40 words (high, moderate and low frequency words, Balgor,\n1996) and 40 pseudowords presented visually at random in the centre of a computer screen. Stimuli were composed of Hebrew letters 1/400 in diameter each. Presentation length of each stimuli depended on the subject\u2019s reaction,\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n138\n\nS. Miller-Shaul\n\n700 ms after the subject responded the next stimuli appeared on the screen. Subjects pressed the right button of a joystick in response to a word with right thumb, and the left button with the left thumb in response to a pseudoword.\n(b) Auditory phonological test (Sarid, 1997b). Phonological ability was tested for both age groups by a rhyming decision task.\nThis task required subjects to make a rhyme\u2013non-rhyme decision. A list of 120 pairs of voweled Hebrew words were presented auditorily via headphones to the subjects. The stimuli were divided into four types:\n1. Orthographically and phonologically similar rhyming word pairs. Example: h\\k \u2013 h\\b bama \u2013 kama.\n2. Orthographically dissimilar and phonologically similar rhyming word pairs. Example: dxda \u2013 dwu otzar \u2013 azar.\n3. Orthographically and phonologically dissimilar non-rhyming word pairs. Example: bwhk \u2013 swcf katuv \u2013 pashot.\n4. Orthographically similar and phonologically dissimilar non-rhyming word pairs. Example: klh \u2013 klh halav \u2013 helev.\n\nThe rhyme\u2013non-rhyme decision was made by pressing a joystick button. The presentation of the categories were according to a fully counterbalanced design. When the joystick button was pressed a new pair of words was presented after a 700 ms break. Accuracy and reaction time were measured.\n(c) Orthographic ability (Sarid, 1997a). Orthographic ability was tested for both groups by an orthographic decision task. A sound blaster presented 90 pairs of Hebrew words auditorily to the subjects. The word pairs were divided into two categories:\n1. Words written with the same letters but sound different (orthographic similarity and phonological dissimilarity). Example: klh \u2013 klh halav \u2013 helev.\n2. Words written with different letters and sound different (orthographically and phonologically dissimilar). Example: hbwk \u2013 w \\w boba \u2013 yuman.\n\nThe subject was required to decide whether the two words were written with the same letters or not by pressing a button of a joystick. Presentation time was dependent on subject\u2019s response. When one of the buttons was pressed a new pair of words was presented after 700 ms. Accuracy and reaction time were measured.\nBy using an auditory task the subjects had to use their orthographic knowledge of how words are written in order to get the right answer.\nIV. Speed of processing. Speed of processing was measured for children and adults using a similar battery of speed of processing tests:\n(a) Sign-number test}WAIS 3. Each sub-test includes nine sign-number pairs. The subject must match up 133 numbers to the sign that they belong to according to a legend. The subjects must copy the signs into the empty squares under the numbers. The subjects are given 90 s to complete as many sign-number pairs as possible. The grade is based on the number of correct pairs that the subject completes.\n(b) Symbol search}WAIS 3. The test is taken from the Weschler test and consists of symbol shaped stimuli. There are two target stimuli and the subject must\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n139\n\ndetermine whether one or both can be found in a group of five stimuli. The test\n\nincludes 60 items and the subject must perform as many as possible without\n\nskipping in the 120 s provided.\n\n(c) Naming tests. The naming tests include a series of five exams; each test\n\ncontains 50 different stimuli (numbers, typed letters, written letters, colours and\n\nobjects). The stimuli are arranged in a table of five rows and 10 columns, and in\n\neach table there are five stimuli that appear 10 times at random in no particular\n\norder. The subject must name each stimulus.\n\nThe examiner measures the performance time of each test, and checks the\n\naccuracy of the naming.\n\n(d) Word fluency test. The test includes three letters (\n\n) and the subject\n\nmust write down as many words beginning with each letter in a minute. The\n\nexaminer checks the subjects retrieval ability based on a particular criterion\n\n(finding words that start with a particular letter).\n\nV. Memory test\n\nDigit span. In order to measure each subject\u2019s memory capacity the digit span\n\ntest from the WISC (1993) and WAIS (1981) tests. The test in compromised from\n\ntwo subtests one digit span forward (the subject repeats the digits in the order\n\nthey were read to him) and digit span backwards (the subject repeats the digit in\n\nthe opposite order they were read to him from the last to the first).\n\nThe maximum number of digits reached forwards and backwards is measured.\n\nProcedure. Each participant was administered the testing procedures over 2\u20133\n\nsessions.\n\nEach\n\nsession\n\nlasted\n\napproximately\n\n2\n\nh,\n\nwith\n\na\n\n1 2\n\nh\n\nbreak\n\nin\n\nbetween.\n\nAll\n\ntesting was conducted individually, in a quiet room.\n\nInstrumentation. All experimental tasks were carried out using an IBM-PC 586.\n\nAuditory tasks were derived from a Sound Blaster Pro and delivered via\n\nearphones. Visual tasks were presented on the 1700 computer screen.\n\nResults. 2 \u00c2 2 ANOVA\u2019s [group (dyslexics \u00c2 controls) \u00c2 age (young \u00c2 adults)]\n\nanalyses were computed for each of the experimental measures separately.\n\nI. General ability. No significant differences were found between dyslexic and\n\nnormal readers at any age group on the measure of general ability. See Table 1 for\n\nmean and S.D.\n\nII. Reading tests. As indicated by Table 1, the young and adult groups data\n\nrevealed that the dyslexics were significantly slower in reading connected text\n\nand less accurate in list and connected text word reading.\n\nSignificant age \u00c2 group interaction (F\u00f03,75\u00de \u00bc 52:30, p50:00) was also obtained\n\nfor the word reading list. The interaction stemmed from lower discrepancy\n\nbetween word reading accuracy among the adult subjects as compared to the\n\nchild subjects.\n\nIn both age groups no significant differences were found between the dyslexics\n\nand the controls regarding reading comprehension.\n\nIII. Word recognition skills\n\n(a) Lexical decision. 2 \u00c2 2 Manova analyses [group (dyslexic vs control), by age\n\n(adults vs children)] were performed for the reaction time and accuracy for each\n\ntype of stimuli, words and pseudowords.\n\nAccuracy: a main effect for group for words (F\u00f03,97\u00de \u00bc 37:52, p50:001),\n\nand for pseudowords (F\u00f03,97\u00de \u00bc 13:52, p50:001) was found. The dyslexics\n\nin both age groups were less accurate in identifying the words and\n\npseudowords.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nTable 1. Mean and standard deviation of the reading measures (In Z scores)\n\nChildren\n\nMean and (S.D.)\n\nDyslexic readers\n\nNormal readers\n\nF\n\nOral reading time of text\nReading comprehension\nWord reading\nWord reading Errors\n\n0.71 (1.05) 0.00 (1.37) \u00c00.64 (1.10) 0.65 (1.07)\n\n\u00c00.60 (0.31) 0.00 (0.58) 0.60 (0.26) \u00c00.61 (0.27)\n\n24.02*** 0.16 20.43*** 22.43***\n\n*p50.05; **p50.01; ***p50.001.\n\nDyslexic readers\n0.26 (1.30) 0.00 (1.00) \u00c00.65 (0.79) 0.35 (1.33)\n\nAdult\nNormal readers\n\u00c00.26 (0.47) 0.00 (1.00) 0.65 (0.72) \u00c00.35 (0.11)\n\nF 3.42* 0.38 33.95*** 6.27*\n\nS. Miller-Shaul\n\n140\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n141\n\nA main effect for age was also obtained for the accuracy of pseudowords (F\u00f03,97\u00de \u00bc 6:39, p50:02). The children in both groups were more accurate in identifying the pseudowords as compared to the adults.\nReaction time: a main effect for group was found for reaction time to words (F\u00f03,97\u00de \u00bc 15:84, p50:001), and to pseudowords (F\u00f03,97\u00de \u00bc 12:77, p50:001). The dyslexics in both age groups read the words and pseudowords more slowly than the controls.\nA main effect for age was also obtained for reaction time to words (F\u00f03,97\u00de \u00bc 27:44, p50:001), and pseudowords (F\u00f03,97\u00de \u00bc 12:36, p50:001). The children in both groups read the words and pseudowords more slowly than the adults.\nA significant group by age interaction was found for the reaction time to pseudowords (F\u00f03,97\u00de \u00bc 7:32, p50:02). The interaction stemmed from larger group differences in reaction time for pseudowords among the children as compared to the adults. The young dyslexics were significantly slower.\nTable 2 presents means and standard deviations for lexical decision behavioral measures among dyslexic and normal readers in both age groups.\n(b) Phonological measures. Accuracy: a main group effect of accuracy was found in the rhymed and looked similar (F\u00f03,97\u00de \u00bc 10:40, p50:002) and in the nonrhymes and looked similar (F\u00f03,97\u00de \u00bc 15:20, p50:001) categories. The dyslexics were less accurate then the controls on both tasks.\nReaction time: a main effect for group was obtained. In the rhymes, which look alike (F\u00f03,97\u00de \u00bc 11:16, p50:001), rhymes which don\u2019t look alike (F\u00f03,971\u00de \u00bc 10:66, p50:002), non-rhymes which look different (F\u00f03,97\u00de \u00bc 13:37, p50:001) and nonrhymes which look alike (F\u00f03,97\u00de \u00bc 12:86, p50:001). In all four categories the dyslexics were slower than the controls.\nSee Table 3 for means and standard deviations. (c) Orthographic measures. Accuracy: no differences were obtained between the groups in any of the orthographic measures. Reaction time: a significant group x age interaction was found in the orthographic choice for similar words (F\u00f03,97\u00de \u00bc 9:09, p50:004) and different words (F\u00f03,97\u00de \u00bc 15:57, p50:001). The interaction stemmed from significant differences in reaction time on the two tasks, in the young subjects as compared to the adults. In both categories the young dyslexics were slower then the age match controls. See Table 3 for means and standard deviations. IV. General speed of processing. In all of the speed of processing tasks a main effect for group was found. The dyslexics in the two age groups were slower than the control groups. The main effects of group were obtained in naming objects (F\u00f03,97\u00de \u00bc 15:01, p50:001), naming colours (F\u00f03,97\u00de \u00bc 11:80, p50:001), naming numbers (F\u00f03,97\u00de \u00bc 13:24, p50:001), naming block letters (F\u00f03,97\u00de \u00bc 16:45, p50:001) and naming script letters (F\u00f03,97\u00de \u00bc 20:85, p50:001). In addition, all dyslexics produced less words in the verbal fluency test (F\u00f03,97\u00de \u00bc 35:07, p50:000) and had a lower score in the coding test (F\u00f03,97\u00de \u00bc 37:52, p50:001). See means and standard deviations in Table 2. V. Memory test. A main effect for group (F\u00f03,97\u00de \u00bc 8:90, p50:004) and a main effect for age (F\u00f03,97\u00de \u00bc 51:27, p50:00) were found in the memory tasks. All adults performed better than the children, and in addition, all dyslexics memory span was found to be smaller than the control group. See means and standard deviations in Table 2.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nTable 2. Mean and standard deviation of the lexical decision, speed of processing and memory tasks\n\nChildren\n\nMean and (S.D.)\n\nDyslexic readers\n\nNormal readers\n\nF\n\nDyslexic readers\n\nWords Time Words Accuracy Pseudowords Time Pseudowords Accuracy Naming time Objects Naming time Colours Naming time Numbers Naming time Letters (defus) Naming time Letters (chtav) Verbal fluency Number of words Coding Wisc, wais Memory span\n\n1598 (735) 25.25 (4.17) 2107 (1230) 24.63 (3.32) 60.65 (27.65) 48.12 (26.99) 37.33 (14.61) 47.00 (14.61) 56.25 (17.90) 5.41 (1.69) 22.81 (2.57) 9.76 (2.88)\n\n1006 (223) 28.35 (1.37) 1239 (499) 26.88 (2.00) 46.24 (10.59) 36.71 (8.79) 24.19 (4.76) 30.31 (4.38) 33.07 (6.06) 8.55 (2.93) 40.62 (3.21) 11.82 (3.54)\n\n10.03** 8.45** 7.22* 5.67* 5.56* 3.78* 11.47** 9.55** 22.16*** 14.80*** 33.18** 3.45\n\n841 (190) 24.50 (6.80) 976 (192) 20.25 (7.05) 47.05 (15.85) 34.77 (14.10) 25.01 (17.18) 28.20 (13.54) 30.90 (21.18) 12.11 (3.38) 51.39 (11.01) 14.52 (2.47)\n\n*p50.05; **p50.01; ***p50.001.\n\nAdult\nNormal readers\n693 (98) 28.92 (2.63) 809 (169) 25.31 (5.19) 35.93 (5.10) 25.66 (3.38) 18.25 (3.04) 20.02 (4.89) 21.14 (2.74) 15.83 (3.31) 61.22 (10.74) 16.32 (2.57)\n\nF 6.25* 5.40* 5.69* 3.90* 10.25** 9.08** 3.45 7.43** 4.75* 21.20*** 11.30** 5.72*\n\nS. Miller-Shaul\n\n142\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nTable 3. Mean and standard deviation of auditory rhyming and orthographic choice tasks\n\nChildren\n\nMean and (S.D)\n\nDyslexic readers Normal readers F\n\nRhymes which look similar Accuracy Rhymes which look similar Reaction time Rhymes which look different Accuracy Rhymes which look different Reaction time Non-rhymes which look different Accuracy Non-rhymes which look different Reaction time Non-rhymes which look similar Accuracy Non-rhymes which look similar Reaction time Same letters Reaction time Same letters Accuracy Different letters Reaction time Different letters Accuracy\n\n20.19 (4.59) 2211.01 (357.42) 19.20 (3.95) 2295.78 (261.58) 22.38 (1.96) 2348.18 (278.41) 18.19 (6.46) 2444.24 (290.72) 2483.70 (432.41) 24.87 (4.73) 2429.28 (497.93) 54.93 (9.78)\n\n23.06 (1.48) 1986.06 (224.16) 20.47 (3.91) 2206.36 (201.19) 22.94 (1.43) 2614.27 (210.42) 23.59 (1.62) 2303.10 (216.54) 2116.94 (242.82) 27.12 (2.03) 2026.69 (205.28) 59.29 (2.64)\n\n5.99** 4.75** 0.83 1.19 0.90 4.61* 11.15** 2.52 9.03** 3.19 9.34** 3.13\n\n*p50.05; **p50.01; ***p50.001.\n\nDyslexic readers\n22.53 (1.68) 1838.85 (135.92) 23.07 (3.03) 2187.47 (167.47) 21.00 (4.71) 2187.47 (167.14) 18.93 (7.13) 2121.93 (154.86) 2093.75 (187.50) 26.54 (3.20) 2032.92 (153.30) 57.85 (2.08)\n\nAdult\nNormal readers\n23.58 (0.79) 1706.87 (93.59) 23.92 (1.44) 1958.17 (144.39) 23.33 (1.56) 1958.17 (144.39) 23.50 (1.57) 1929.14 (145.39) 2005.70 (130.16) 26.54 (4.57) 1991.02 (161.31) 54.33 (13.51)\n\nF 4.52* 6.52* 2.66 14.23*** 0.72 9.34** 5.03* 13.25*** 1.94 0.16 1.51 0.78\n\n143\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n144\n\nS. Miller-Shaul\n\nThe gaps (effect size): In order to examine the accumulative differences between the young and adult dyslexics and their age-matched normal readers a new score (effect size) was calculated for each experimental measure. The effect size was used because it is a cross-sectional study and not a longitudinal study. With the use of the effect size we can compare the differences in the means of two different groups on the same measure. \u2018Effect size\u2019 is simply a way of quantifying the difference between two groups (Stevens, 1992).\nThe calculation was obtained by calculating the gap between the dyslexic group\u2019s mean and that of the regular readers as expressed by the standard deviation of both groups together. The measure provides us with the number of standard deviations included in the difference between the means of each age group. As the measures increases, the gap between groups widened. A comparison of the gap size in each age group indicates the effect size of the achievement gap beyond years for young and adult dyslexics. Any effect over 0.2 is considered a significant effect, they are divided into three groups: 0.2\u20130.49}small effect, 0.5\u20130.79}medium effect and over 0.80}a large effect (Stevens, 1992).\nThe data in Table 4 indicate that the effect size between the groups increased with age in naming time of colours and objects. The gap between adult dyslexics and the control group on these measures increased over the years.\n\nTable 4. The intensity effect for behavioural measures and reading skill tasks\n\nTest\n\nYoung\n\nAdult\n\neffect\n\neffect\n\nintensity\n\nintensity\n\nObject naming time\n\n0.77\n\n0.85\n\nColor naming time\n\n0.65\n\n0.82\n\nNumber naming time\n\n1.10\n\n0.53\n\nPrinted letter naming time\n\n1.00\n\n0.76\n\nWritten letter naming time\n\n1.35\n\n0.63\n\nVerbal fluency\n\n1.11\n\n1.13\n\nWord reading without context\n\n1.24\n\n1.30\n\nWord reading errors\n\n1.26\n\n0.70\n\nConnected text reading time\n\n1.31\n\n0.52\n\nLexical decision reaction time}words\n\n0.97\n\n0.87\n\nLexical decision accuracy}words\n\n0.91\n\n0.82\n\nLexical decision reaction time}pseudowords\n\n0.85\n\n0.85\n\nLexical decision accuracy}pseudowords\n\n0.76\n\n0.72\n\nAuditory rhyming decision time}category 1\n\n0.71\n\n0.88\n\nAuditory rhyming decision accuracy}category 1\n\n0.78\n\n0.75\n\nAuditory rhyming decision time}category 2\n\n0.38\n\n1.16\n\nAuditory rhyming decision accuracy}category 2\n\n0.32\n\n0.60\n\nAuditory rhyming decision time}category 3\n\n0.70\n\n1.01\n\nAuditory rhyming decision accuracy}category 3\n\n0.33\n\n0.32\n\nAuditory rhyming decision time}category 4\n\n0.53\n\n1.14\n\nAuditory rhyming decision accuracy}category 4\n\n1.01\n\n0.78\n\nOrthographic decision}identical words}time\n\n0.94\n\n0.53\n\nOrthographic decision}identical words}accuracy\n\n0.61\n\n0.00\n\nOrthographic decision}different words}time\n\n0.96\n\n0.30\n\nOrthographic decision \u2013different words}accuracy\n\n0.60\n\n0.34\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n145\n\nThe effect size between the groups decreased with age for number and letter naming time, number of decoding errors and reading time of connected text. The gap between adult dyslexics and regular readers decreased over the years.\nEffect size was maintained over time for verbal fluency and reading words in a list. In other words, the gap between the adult dyslexics and the control group was maintained over the years.\nPhonological measures gap: the results from Table 4 indicate that the effect size increased in most of the auditory rhyming decision categories except for the category of different words in which the effect size of accuracy among adults was smaller. In addition, the effect size of accuracy was identical for both groups in the categories in which the words rhymed and looked different and did not rhyme and looked similar. The effect size was maintained over the years on lexical decision reaction time to pseudowords. In other words, the gap was maintained and remained identical for both age groups. Thus, it can be seen that the gap increased with age among the adults on most of the phonological tasks in reaction time and some tasks in accuracy.\nOrthographic measures gap: the effect size between groups decreased with age on all the orthographic tasks in both accuracy and reaction time measures, and on lexical decision in decision accuracy to words and pseudowords and in reaction time to words only. Thus, it can be seen that the gap decreased with age among the adults on most of the orthographic tasks.\n\nDISCUSSION\n\nThe aim of the present paper was to examine the differences between young and adult dyslexics as compared to regular readers in performing reading and reading related skills. The hypothesis of the study was that after years of print exposure and remedial teaching the difficulties that adult compensated dyslexics continue to exhibit while reading might be seen as a characteristic of the dyslexia phenomenon.\nReading abilities: the different researchers concerned with studying the dyslexia phenomenon agree that dyslexics are characterized by deficient word recognition. That is, they have difficulty decoding the written word. Their comprehension difficulties are based on this deficit (Adams, 1990). The results of this study validate the fact that young and adult dyslexics are characterized by deficient decoding of single real words. This gap measure (effect size) was not only retained between experimental groups but also enlarged. On the other hand, the gap between young and adult dyslexics pertaining to the ability to recognize words from context (vocal reading) was significantly reduced. It is possible that this is evidence for an additional compensation mechanism developed among adult dyslexics during their years of print exposure. The context becomes a tool that simplifies their reading and helps them read faster. Thus, the affect of context on accurate word decoding exists for dyslexics, as opposed to the control group of regular readers, not in recognition of single words but in reading a meaningful connected text.\nOn the other hand, in the ability to recognize real words (as opposed to reading them see above lexical decision task) the gap between performance time and accuracy is significantly reduced among adults as opposed to children. In other\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n146\n\nS. Miller-Shaul\n\nwords, when an adult dyslexic is required to recognize the word pattern of a real word he does so more efficiently than a young dyslexic, more accurately and faster. This finding indicates that during years of print exposure the dyslexic manages to create a database of word patterns in the lexicon. Thus, in adulthood when the dyslexic reader is only requested to recognize them (reliance on orthographic processing}a more stable and perhaps compensating channel) and not articulate them he does so with greater efficiency than a young dyslexic. Despite this when recognizing pseudowords the gap between young and adult dyslexics remains almost identical. Reaction time is completely identical, but it is not significantly reduced on the accuracy measure. This finding indicates that when the adult dyslexic subject must recognize an unfamiliar pseudoword pattern he must use the phonological processing channel and decode the word. Here he is at the same level as the young dyslexic, and this leads to a pseudoword pattern recognition deficit.\nPhonological processing: the difficulty in pseudoword reading among both age groups of dyslexics which stems from a phonological deficit is validated by phonological rhyming tasks. There the gap in quality and speed of processing increased among the adult dyslexics as compared to the youngsters. With regard to processing of phonological tasks, in this study it seems that the dyslexics reach a certain level}asymptote beyond which they cannot improve the quality of their performance.\nThus, the phonological task gap between accurate and dyslexic readers increases over the years. Indeed, a wide research consensus exists regarding the function of phonology in reading. There is a substantial amount of evidence indicating a close connection between low phonological skills and reading difficulties. Phonological awareness is considered to be one of the conditions essential to development of proper reading skills. The findings of this research study together with the findings of others show that the phonological abilities of adult dyslexics remain low (Snowling, 1995; Bruck, 1990), that is, a phonological processing deficit is possible and constitutes a trait of the dyslexic reader, a trait that remains in adulthood. The trait of slowness of processing in the phonological channel is more salient in the Hebrew language, which has a shallow orthography, as compared to the accuracy of the phonological channel.\nOrthographic processing: the situation is different regarding the quality and speed of orthographic processing tasks in this study. There the gaps both in accuracy and reaction time between the experimental groups are reduced. This finding indicates that the processing ability of dyslexics in the orthographic channel improves over the years. Training and exposure to print has an obvious positive influence, however, it must be noted that their performance level and speed in this channel does not reach the level of adult regular readers.\nThere is contradictory evidence in a number of studies regarding the orthographic skills of adult dyslexics. Bruck (1998) and Corcos and Willows (1993) found that the gap in orthographic knowledge between dyslexic and normal control readers increased with time. As opposed to this, the present study data supported the results which indicated development and improvement of the orthographic processing channel among adult dyslexics (see also e.g., Ehri, 1991; Stanovich, West, & Cunningham, 1991). It has previously been suggested (see Stanovich et al., 1991; Breznitz, 1997) that the dyslexics might rely on the orthographic route during reading. It is conceivable that after years of print\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n147\n\nexposure adult dyslexics developed compensation techniques in the route that for them is more stable. This compensatory route might help the adult dyslexics to overcome some of the difficulties exhibited by young dyslexics. The adult dyslexics have created patterns for words which they have seen during the years of exposure to print, but words which rely on phonological decoding such as uncommon words and words which do not have regular patterns are problematic for the adult dyslexic too.\nSpeed of processing: due to slowness of the phonological channel there is asynchrony between the phonological and orthographic information of words, and new word patterns are not created (Breznitz, 2002). Tests that examined speed of information processing found differential evidence regarding the speed of processing gap between young and adult dyslexics. On naming measures, those that constitute evidence for retrieval speed or processing of representation in the mental lexicon, the results regarding the processing gap between groups were inconclusive. On the number and letter naming tests (alphanumeric) the gap between experimental groups was reduced. This finding supports the assumption that over the years naming of letters and numbers becomes automatic for the dyslexic and the normal reader adult population. On the other hand, on naming of colours and objects the gap between groups increased. The gap in this measure increased due to the fact that the adult readers improve their speed of processing on these tasks with a higher ratio than dyslexics (see Felton et al., 1990). As Van der Leij (1999) suggested, over the years among the adult normal readers as opposed to adult dyslexics, naming of objects and colours like reading words becomes an automatic process and the retrieval of these representations of word and other mental representations in the lexicon becomes an automatic task. It is also plausible that for young and adult dyslexics colours are an abstract concept, and objects have various options and print exposure does not affect the retrieval speed of these categories for people with lexicon representation difficulties. Support for the slowness and the difficulties of dyslexics to retrieve patterns from the mental lexicon can be seen in our data, which show that verbal fluency remains problematic for adult dyslexics. In addition retrieval of colours and objects is a retrieval of verbal representations based on phonology of words, and the slowness of the phonological channel affects the naming retrieval too.\n\nCONCLUSIONS\n\nIn conclusion, it can be argued that the present study supports the notion that the dyslexia phenomenon can be characterized as an inaccurate and slow word reading process (see also British Psychological Society, 1999), including difficulties in phonological processing and slowness in performing various reading related tasks which might represent more general slowness in retrieval of mental representations from the lexicon and affects the entire reading process (see Breznitz, 2001, 2002 for review).\nIt is important to note that the information provided by conducting this study as longitudinal instead of a cross-sectional study will be more reliable regarding development of reading and reading related skills. In addition, it is possible that there are subgroups within the dyslexic group, and it is possible that some use\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n148\n\nS. Miller-Shaul\n\nthe orthographic channel to compensate while others do not. The personal profiles of each subject should be the topic of further study in order to receive more information on this issue. Moreover, it is also possible that if the phonological channel is impaired among dyslexics and the performance of the orthographic channel differs in each subject, then the source of the problem may stem from integration of information essential to the reading process that flows from these channels}a future study could investigate this matter. In order to verify whether the orthographic skills among adult dyslexics are more stable a further study needs to examine orthographic knowledge without use of semantic knowledge. In this study orthography was examined using real words with meaning; this factor may have influenced the results obtained regarding processing in this channel.\nTo sum up, it is possible to see conclusively based on the measures themselves and the effect size analysis that the gap between accurate readers and dyslexics increased significantly in the phonological channel. That is, this channel is damaged among young and adult dyslexics, and it is characterized by slow processing in comparison with the control group. It can be concluded that work needs to be done on improving processing in the phonological channel (accuracy and speed) for all dyslexics in order to help them attain faster speed of processing in this channel, and as a result their reading may also be improved.\n\nReferences\nAdams, M. J. (1990). Beginning to read: Thinking and learning about print. Cambridge, MA: MIT Press.\nBalgor, A. (1996). The frequency of words in Hebrew. Tel Aviv, Israel: Hapoalem.\nBarker, T. A., Torgesen, J. K., & Wagner, R. K. (1992). The role of orthographic processing skills on five different reading tasks. Reading Research Quarterly, 27(4), 334\u2013335.\nBen-Dror, I., Pollatsek, A., & Scarpati, S. (1991). Word identification in isolation and in context by college dyslexic students. Brain and Language, 31, 308\u2013327.\nBjaalid, I. K., Hoien, T., & Lundberg, I. (1995). A comparison of components in word recognition between dyslexic and normal readers. Scandinavian Journal of Educational Research, 39, 51\u201359.\nBowers, P. G. (1994). Tracing symbol naming speed\u2019s unique contribution to reading disabilities over time. Reading and Writing, 7, 189\u2013216.\nBowers, P. G. (1995). Re-examining selected reading research from the viewpoint of the \u2018double-deficit hypothesis\u2019. Paper presented at the Society for research in child development, Indianapolis, IN.\nBowers, P. G. (2001). Exploration of the basis for rapid naming\u2019s relationship to reading. In M. Wolf (Ed.), Dyslexia, fluency and the brain (pp. 41\u201363). Cambridge, MA: York Press.\nBowers, P., Golden, J., Kennedy, A., & Young, A. (1994). Limits upon orthographic knowledge due to processing indexed by naming speed. In V. W. Berninger (Ed.), The varieties of orthographic knowledge I: Theoretical and developmental issues (pp. 173\u2013218). Dordrecht: Kluwer.\nBowers, P. G., & Swanson, L. B. (1991). Naming speed deficits in reading disability: Multiple measures if a singular process. Journal of Experimental Child Psychology, 51, 195\u2013219.\nBrande, S. (1998). Characteristics of reading proficiency in adult dyslexics as compared to young dyslexics. Unpublished thesis, Haifa University, Haifa, Israel.\nBreznitz, Z. (1997). Enhancing the reading of dyslexics by reading acceleration and auditory masking. Journal of Educational Psychology, 89(1), 236\u2013246.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n149\n\nBreznitz, Z. (2001). The determinants of reading fluency: A comparison of dyslexic and average readers. In M. Wolf (Ed.), Dyslexia, fluency and the brain (pp. 245\u2013276). Timonium, Maryland: York Press.\nBreznitz, Z. (2002). Asynchrony of visual-orthographic and auditory}phonological word recognition processes: An underlying factor in dyslexia. Reading and Writing, 15, 15\u201342.\nBritish Psychological Society. (1999). Dyslexia literacy and psychological assessment. Report by a working party of the division of educational and child psychology. Leicester: British Psychological Society.\nBruck, M. (1990). Word recognition skills of adults with childhood diagnoses of dyslexia. Developmental Psychology, 26, 439\u2013454.\nBruck, M. (1992). Persistence of dyslexics, phonological awareness deficits. Developmental Psychology, 28, 874\u2013886.\nBruck, M. (1998). Outcomes of adults with childhood histories of dyslexia. In C. Hulme, & J. R. Malatesha (Eds.), Reading and spelling: Development and disorders (pp. 179\u2013200). Mahwah, NJ: Lawrence Erlbaum.\nBrunswick, N., McCrory, E., Price, C. J., Frith, C. D., & Frith, U. (1999). Explicit and implicit processing of words and pseudowords by adult developmental dyslexics. Brain, 122(10), 1901\u20131917.\nCatts, H. W. (1986). Speech production/phonological deficits in reading-disordered children. Journal of Learning Disabilities, 19, 504\u2013508.\nComprehension Test. (1996). Israeli psychometric scholastic aptitude test. The Centre for Psychometric Tests, Tel Aviv.\nCorcos, E., & Willows, D. M. (1993). The processing of orthographic information. In D. N. Willows, R. S. Kruk, & E. Corcos (Eds.), Visual processing in reading and reading disabilities (pp. 163\u2013190). Hillsdale, NJ: Lawrence Erlbaum.\nde Jong, P. F., & Van der Leij, A. (1999). Specific contributions of phonological abilities to early reading acquisition: Results from a Dutch latent variable longitudinal study. Journal of Educational Psychology, 91(3), 450\u2013476.\nDoi, L. M., & Manis, F. R. (1996). The impact of speeded naming ability on reading performance. Paper presented at the meeting of the Society for the Scientific Study of Reading, New York, NY.\nEhri, L. C. (1991). Development of the ability to read words. In R. Barr, M. L. Kamil, P. Mosenthal, & P. D. Pearson (Eds.), Handbook of reading research, Vol. 2 (pp. 385\u2013419). New York: Longman.\nFarmer, M. E., & Klein, R. K. (1993). Auditory and visual temporal processing in dyslexic and normal readers. In P. Tallal, A. M. Galaburda, R. R. Llinas, & C. von Eurler (Eds.), Temporal information processing in the nervous system. (Annals of the New York Academy of Sciences, 682, 339\u2013341).\nFelton, R. H., Nylor, C. E., & Wood, F. B. (1990). Neuropsychological profile of adults dyslexic. Brain and Language, 39, 485\u2013497.\nFoorman, B. (1994). Phonological and orthographic processing: Separate but equal? In V. W. Berninger (Ed.), The varieties of orthographic knowledge I: Theoretical and developmental issues. (pp. 321\u2013357).\nGallagher, A. M., Laxon, V., Armstrong, E., & Frith, U. (1996). Phonological difficulties in high-functioning dyslexics. Reading and Writing: An Interdisciplinary Journal, 8, 499\u2013509.\nGross-Glen, K., Jallad, B., Nova, L., Helgen-Lempesis, V., & Lubs, H. A. (1990). Nonsense passage reading as a diagnostic aid in the adult familial dyslexia. Reading and Writing: An Interdisciplinary Journal, 2, 161\u2013173.\nHelenius, P., Tarkiainen, A., Cornelissen, P., Hansen, P. C., & Salmelin, R. (1999). Dissociation of normal feature analysis and deficient processing of letter strings in dyslexic adults. Cerebral Cortex, 9(5), 476\u2013483.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n150\n\nS. Miller-Shaul\n\nHelenius, P., Uutela, K., & Hari, R. (1999). Auditory stream segregation in dyslexic adults. Brain, 122(5), 903\u2013917.\nHulme, C. (1981). Reading retardation and multi-sensory teaching. London: Routledge and Kegan Paul.\nLefly, D. L., & Pennington, B. F. (1991). Spelling errors and reading fluency in compensated adult dyslexics. Annals of Dyslexia, 41, 143\u2013163.\nLevinthal, C. F., & Hornung, M. (1992). Orthographic and phonological coding during visual word matching as related to reading and spelling abilities in college students. Reading and Writing: An Interdisciplinary Journal, 4, 231\u2013243.\nLovegrove, W. (1992). The visual deficit hypothesis. In N. Singh & Beale (Eds.), Learning disabilities: Nature, theory and treatment (pp. 246\u2013269). New York: Springer Verlag.\nLovett, M. W. (1987). A developmental approach to reading disability: Accuracy and speed criteria of normal and deficient reading skill. Child Development, 58, 234\u2013260.\nManis, F. R., Cus-odio, R., & Szeszulski, P. A. (1993). Development of phonological and orthographic skill: A 2 year longitudinal study of dyslexic children. Journal of Experimental Child Psychology, 56, 64\u201386.\nManis, F. R., Seidenberg, M. S., & Doi, L. M. (1999). See Dick RAN: Rapid naming and the longitudinal prediction of reading subskills in first and second graders. Scientific Studies of Reading, 3, 129\u2013157.\nManis, F. R., Szeszulski, P. A., Holt, L. K., & Graves, K. (1990). In T. H. Carr, & B. A. Levy (Eds.), Reading and its development: Component skills approaches (pp. 207\u2013259). NY: Academic Press.\nMann, V. A., & Brady, S. (1988). Reading disability: The role of language deficiencies. Journal of Consulting and Clinical Psychology, 56, 811\u2013816.\nMinistry of Education. (1995). Reading achievement test (RAT) for 4th graders. Jerusalem, Israel.\nNicolson, R. I., & Fawcett, A. J. (1993). Toward the origin of dyslexia. In S. F. Wright & R. Groner (Eds.), Facets of dyslexia and its remediation (pp. 371\u2013391). Amsterdam: NorthHolland; Amsterdam: Elsevier Science.\nPennington, B. F., McCabe, L. L., Smith, S. D., Lefly, D. L., Bookman, M. D., Kimberling, W. J., & Lubs, H. A. (1986). Spelling errors in adults with a form of familial dyslexia. Child Development, 57, 1001\u20131013.\nRack, J. P., Snowling, M. J., & Olson, R. K. (1992). The nonword reading deficit in developmental dyslexia: A review. Reading Research Quarterly, 27, 29\u201353.\nRamus, F., Rosen, S., Dakin, S. C., Day, B. L., Castellote, J. M., White, S., & Frith, U. (2003). Theories of developmental dyslexia: Insights from a multiple case study of dyslexic adults. Brain, 126, 841\u2013865.\nRaven, J. C. (1974). Advanced progressive matrices sets I and II. London: H. K. Lewis.\nSarid M. (1997a). Orthographic processing test. Unpublished test. Haifa, Israel: Haifa University.\nSarid M. (1997b). Phonological processing test. Unpublished test. Haifa, Israel: Haifa University.\nSarid, M. (1997c). Word and text reading test. Unpublished test. Haifa, Israel: Haifa University.\nShare, D. L. (1994). Deficient phonological processing in disabled readers implicates processing deficits beyond the phonological module. In K. P. van den Bos, L. S. Siegal, D. J. Bakker, & D. L. Share (Eds.), Current directions in dyslexia research. Lisse: Swets & Zeitlinger B. V.\nShatil, E. (1995a). One-minute test for words. Unpublished test. Haifa: University of Haifa.\nShatil, E. (1995b). One-minute test for pseudowords. Unpublished test. Haifa: University of Haifa.\nShaywitz, S. E., Fletcher, J. M., Holahan, J. M., Shneider, A. E., Marchione, K. E., Stueberg, K. K., Francis, D. J., Pugh, K. R., & Shaywitz, B. A. (1999). Persistence\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n10990909, 2005, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/dys.290 by Suffolk University, Wiley Online Library on [01/02/2023]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nYoung and Adult Dyslexics Readers, Reading, Reading Related Cognitive Tasks\n\n151\n\nof dyslexia: The Connecticut Longitudinal Study at adolescence. Pediatrics, 104(6), 1351\u20131359.\nSiegel, L. S., Share, D., & Geva. E. (1995). Evidence for superior orthographic skills in dyslexics. American Psychological Society, 6, 250\u2013253.\nSnowling, M. J. (1995). Phonological processing and developmental dyslexia. Journal of Research in Reading, 18, 132\u2013138.\nStanovich, K. E. (1991). Word recognition: Changing perspectives. In P. D. Pearson (Ed.), Handbook of reading research, Vol. 2 (pp. 418\u2013452). White Plains, NY: Longman.\nStanovich, K. E., West, R. F., & Cunningham, A. E. (1991). Beyond phonological processes: Print exposure and orthographic processing. In S. A. Brady, & D. P. Shankweiler (Eds.), Phonological processes in literacy: A tribute to Isabelle Y. Liberman (pp. 219\u2013235). Hillsdale, NJ: Lawrence Erlbaum Associates.\nStevens, J. P. (1992). Applied multivariate statistics for the social sciences. Hillsdale, NJ: Erlbaum Associates.\nTallal, P., Miller, S., & Fitch, R. H. (1993). Neurobiological basis of speech: A case of the preeminence of temporal processing. In P. Tallal, A. M. Galaburda, R. R. Llinas, & C. von Eurler (Eds.), Temporal information processing in the nervous system (Annals of the New York Academy of Sciences, 682, 421\u2013423).\nTorgeson, J., Rashotte, C., Greenstein, J., Houck, G., & Portes, P. (1988). Academic difficulties of learning disabled children who perform poorly on memory span tasks. In H. L. Swanson (Ed.), Memory and learning disabilities: Advances in learning and behavior disabilities. Greenwich, Conn: JAI Press.\nVan der Leij, A., & Van Daal, V. H. P. (1999). Automatization aspects of dyslexia: Speed limitations in word identification, sensitivity to increasing task demands, and orthographic compensation. Journal of Learning Disabilities, 32(5), 417\u2013428.\nWagner, R. K., & Torgesen, J. K. (1987). The nature of phonological processing and its causal role in the acquisition of reading skills. Psychological Bulletin, 101, 192\u2013212.\nWagner, R. K., Torgesen, J. K., Laughon, P. L., Simmons, K., & Rashhotte, C. A. (1993). Development of young readers phonological processing abilities. Journal of Educational Psychology, 85, 83\u2013103.\nWechsler, D. (1981). Wechsler intelligence scale for adults}revised. Cleveland, OH: Psychological Corp.\nWechsler, D. (1993). Wechsler intelligence scale for children}III. Cleveland, OH: Psychological Corp.\nWolf, M. (2001). Seven dimensions of time. In M. Wolf (Ed.), Dyslexia, fluency and the brain. Cambridge, MA: York Press.\nWolf, M., & Bowers, P. (1999). The double deficit hypothesis for the developmental dyslexias. Journal of Educational Psychology, 91, 415\u2013438.\nWolf, M., & Katzir-Cohen, T. (2001). Reading fluency and its intervention. Scientific Studies of Reading, 5(3), 211\u2013238.\nWolf, P. H., Michel, G. F., & Ovrut, M. (1990). The timing of syllable repetitions in developmental dyslexia. Journal of Speech and Hearing Research, 33, 281\u2013289.\nWorld Health Organization (WHO). (1993). ICD-10, classification of mental and behavioral disorders. Diagnostic Criteria for Research. Geneva: World Health Organisation.\nYap, R., & van der Leij, A. (1993). Word processing in dyslexics. Reading and Writing, 5, 261\u2013279.\nZecker, S. G. (1991). The orthographic code: Developmental trends in reading-disabled and normally achieving children. Annals of Dyslexia, 41, 178\u2013192.\n\nCopyright # 2005 John Wiley & Sons, Ltd.\n\nDYSLEXIA 11: 132\u2013151 (2005)\n\n",
        "hash_id": "2f987ad2fa98add06744b014daf661c2"
    },
    {
        "key": "KA2ST4NL",
        "version": 60,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/KA2ST4NL",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/KA2ST4NL",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/M87XQX4L",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 133209
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Miller-Shaul",
            "parsedDate": "2005",
            "numChildren": 2
        },
        "citation": "<span>Shelley Miller-Shaul, <i>The characteristics of young and adult dyslexics readers on reading and reading related cognitive tasks as compared to normal readers</i>, 11 <span style=\"font-variant:small-caps;\">Dyslexia</span> 132\u2013151 (2005), https://onlinelibrary.wiley.com/doi/abs/10.1002/dys.290 (last visited Feb 1, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "MCNCUB84",
        "version": 58,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/MCNCUB84",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/MCNCUB84",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/I46N8RYW",
                "type": "application/json"
            },
            "enclosure": {
                "type": "text/html",
                "href": "https://api.zotero.org/groups/4848934/items/MCNCUB84/file/view"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            }
        },
        "citation": "<span>Snapshot, https://iovs.arvojournals.org/article.aspx?articleid=2166061 (last visited Feb 1, 2023).</span>",
        "fulltext": " \niovs \n \n    Journals Home \n \nMANAGE ALERTS \nForgot password? \nCreate an Account \nAdvanced Search \nAll Journals \n \n    Issues \n    Topics \n    For Authors \n    About \n \nAugust 2012 \nVolume 53, Issue 9 \n \n    \u2039 \n    Issue \n    \u203a \n \nJump To... \n \n    Introduction \n    Methods \n    Results \n    Discussion \n    Acknowledgments \n    References \n    Appendix \n \nFree \nLow Vision   |   August 2012 \nStandardized Assessment of Reading Performance: The New International Reading Speed Texts IReST \nSusanne Trauzettel-Klosinski ; Klaus Dietz ; the IReST Study Group \nAuthor Affiliations & Notes \n \nInvestigative Ophthalmology & Visual Science August 2012, Vol.53, 5452-5461. doi: https://doi.org/10.1167/iovs.11-8284 \n \n    Views \n        Figures \n        Tables \n    PDF \n    Share \n    Tools \n        Alerts \n        or \n        Get Citation \n        Get Permissions \n \nAbstract \n \nPurpose. : There is a need for standardized texts to assess reading performance, for multiple equivalent texts for repeated measurements, and for texts equated across languages for multi-language studies. Paragraphs are preferable to single sentences for accurate speed measurement. We developed such texts previously in 6 languages. The aim of our current study was to develop texts in more languages for a wide range of countries and users, and to assess the reading speeds of normally-sighted readers. \n \nMethods. : Ten texts were designed for 17 languages each by a linguist who matched content, length, difficulty, and linguistic complexity. The texts then were used to assess reading speeds of 436 normally-sighted native speakers (age 18\u201335 years, 25 per language, 36 in Japanese), presented at a distance of 40 cm and size 1 M, that is 10-point Times New Roman font. Reading time (aloud) was measured by stopwatch. \n \nResults. : For all 17 languages, average mean reading speed was 1.42 \u00b1 0.13 texts/min (\u00b1SD), 184 \u00b1 29 words/min, 370 \u00b1 80 syllables/min, and 863 \u00b1 234 characters/min. For 14 languages, mean reading time was 68 ms/character (95% confidence interval [CI] 65\u201371 ms). Our analysis focussed on words per minute. The variability of reading speed within subjects accounts only for an average of 11.5%, between subjects for 88.5%. \n \nConclusions. : The low within-subject variability shows the equivalence of the texts. The IReST (second edition) can now be provided in 17 languages allowing standardized assessment of reading speed, as well as comparability of results before and after interventions, and is a useful tool for multi-language studies (for further information see www.amd-read.net ). \nIntroduction \nFor most people reading is a key function in everyday life. Standardized assessment of reading performance is necessary to perform repeated measurements based on equivalent texts within one language. Furthermore, standardized texts are a prerequisite for multi-language reading studies. Assessment of reading performance is important in reading disorders, such as low vision, neurologic reading disorders, and developmental dyslexia, but also after multifocal intraocular lens implantation.  \nWell developed texts with single sentences for measuring reading acuity, reading speed, and critical print size are available, such as MN Read or Radner's charts. 1 \u2013 5 However, for measuring reading speed, a whole paragraph of text is preferable to single sentences, because the percentage error of reading time measurements in seconds is smaller for longer texts. Furthermore, reading whole paragraphs is closer to the demands of everyday reading.  \nTo our knowledge, no comparable texts currently are available (except for young children) that have been standardized according to linguistic criteria. We developed such texts previously in 6 different languages, 6 , 7 that is the International Reading Speed Texts (IReST), first edition. The IReST are not in competition with existing reading tests. Rather, they are a supplemental tool that closes a gap in reading diagnostics.  \nThe purpose of our current study was to develop texts in a wide range of additional languages to allow international multi-language studies in the future, and to provide benchmarks for reading speeds in normally-sighted readers.  \nMethods \nDevelopment of the Texts: The Linguistic Work \nTen paragraphs of German text were designed by a linguist from material for sixth grade reading (age 10\u201312 years) with a mean length of 132 words (SD \u00b1 3.2). They were matched for difficulty and linguistic/syntactic complexity according to the theory of Gibson. 8 , 9   \nThe paragraphs of texts (in the following named \u201ctexts\u201d) were translated into 16 languages (Arabic, Chinese, English, Finnish, French, Hebrew, Italian, Japanese, Dutch, Polish, Portuguese/Brazilian, Swedish, Slovenian, Spanish, Russian, and Turkish), and adapted by linguists, all native speakers of the respective language, to be similar in difficulty and linguistic complexity to the German original. Additionally, the original demand was to make them similar in word and character counts. The number of words for all 10 Swedish texts (146) and the number of characters for all 10 Chinese texts (153) were identical.  \nThe Chinese language is logographic and monosyllabic. A Chinese character represents a morpheme with one syllable, and a Chinese word in our texts consists of 1 to 4 Chinese characters. Unlike alphabetic languages, there are no demarcations between Chinese words, for example a space, and consequently it often is difficult to identify a word. Traditionally, reading speed of Chinese text has been measured in number of characters per minute, which we believe is the best measure for Chinese speakers. However, for studies comparing different languages, measuring reading speed in words per minute is desirable. For this purpose, we developed a two-step approach to identify a Chinese word: In step 1, we identified all the words that were found in two major Chinese dictionaries 10 , 11 and meaningful in the context. In step 2, for those Chinese characters that were not included in words identified in step 1, we used those characters to construct words based on grammatical rules 12 , 13 and meaning in the context.  \nA Chinese reader recognizes a character by its pattern, not by counting strokes. Thus, the number or spatial configuration of strokes does not affect reading speed. Therefore, we do not use the information about the number of strokes per character for calculating reading speeds, because these depend more on the frequency of the characters in the texts.  \nFor the Arabic language, it was necessary to find a way of formulating the texts so that they would match the different Arabic speaking countries. We tested this in a group, where 25 students from eight different Arabic countries participated. There were no significant differences between their reading speeds. 14 For French (France/Canada) and Portuguese (Portugal/Brazil) we made sure that the texts also were familiar to speakers of the respective country, mainly by careful word selection and pilot testing with subjects who were not part of the main study. The Japanese texts were written in standard KanaKanji-Script, including Hiragana, Katakana, and Kanji. Table 1 shows the number of words, syllables, and characters per text.  \nTable 1.\u2003 \n  \nView Table \n  \nCounts per Text \nDuring the analysis it became clear that the most important issue was the same content in the different languages\u2014as the amount of information that must be processed. Thus, we accepted unavoidable differences in text length, especially if alphabetic and nonalphabetic languages are compared.  \nAfter developing the first version of the texts in each language, the linguists conducted a pilot study with five normal subjects. Texts were presented at a viewing distance of 40 cm and a size of 1 M, or 10-point Times New Roman font, for the 15 alphabetic languages; 1 M-unit subtends 5 minutes of arc at 1 m. It measures 1.454 mm or 1/7 cm or almost 1/16 inch. 15 These texts were not designed for determining reading acuity and, therefore, are provided only in 1 M letter size, which corresponds to many newspaper print sizes. 16 Texts were printed in black on white paper at high contrast (Michelson contrast 90% or higher). The paragraphs had a maximum line length of 8.5 to 10.0 cm. The mean number of lines/text was 14.3, for Chinese 8.5. Languages read from left to right were left justified and vice versa.  \nThe texts were read aloud, and if there were parts where the readers hesitated or struggled, the texts were modified in the pilot study.  \nProcedure for Establishing Normal Reading Speeds \nIn the main study, the newly designed texts then were used to assess the reading speeds of 436 normally-sighted subjects who were native speakers of the respective languages (25 per language, 36 in Japanese). All texts were unknown to the readers. Participants were 18 to 35 years of age with normal or corrected-to-normal vision as determined by an ophthalmologic/optometric examination.  \nThe texts were printed in the same format as described above for the pilot study and were read aloud at a distance of 40 cm. A stopwatch was used to measure reading time. Words read incorrectly or omitted were counted. However, such error counts were ignored for the following calculations because they were not documented completely in all languages and they occurred very rarely.  \nFigure 1 shows an example of a text with the basic statistics.  \nFigure 1.\u2003 \nSample text in English together with the basic statistics. \nView Original Download Slide \n  \nSample text in English together with the basic statistics. \nThe research adhered to the tenets of the declaration of Helsinki.  \nStatistical Evaluation \nFor calculation of reading speed, in our previous reports we used characters (with spaces and punctuation marks) to measure reading speed because the texts were nearly identical with respect to this variable. 6 , 7 However, considering the different language characteristics of the 17 languages (e.g., in Hebrew and Arabic the vowels are not or only partly written, while Chinese and Japanese are nonalphabetic), in our current study we assessed all reading speeds in four different ways: texts/min, words/min, syllables/min, and characters (without spaces and punctuation marks)/min. Means and SDs were computed for each language using these four measures of reading speed.  \nTo test for differences among the 10 texts for each language separately, we performed a mixed-effects model analysis of variance (ANOVA) of words per minute with the fixed factor \u201ctext\u201d (10 levels) and the random factor \u201csubject\u201d (25 levels in 16 languages and 36 subjects in Japanese). The parameters were estimated using Restricted Maximum Likelihood (REML) with the statistical package JMP 9.01 (SAS, Cary, NC). We tested for normality of the residuals and found, after adjustment of the significance level according to Bonferroni-Holm, only 3 of 17 significant departures from this assumption. A departure from normality does not invalidate the ANOVA due to the large number of observations according to the central limit theorem.  \nDifferences between the texts in each language were assessed by Tukey's HSD post hoc test for 765 comparisons (45 pair-wise comparisons for 17 languages). 17 We considered differences of >10 words/min as clinically relevant and present only these results. Of the total 4359 reading time values (10 texts in each language, 25 subjects in 16 languages, and 36 in Japanese, one missing value), five outliers were excluded from further calculations by the following criterion\u2014if the absolute value of the residuals exceeded 4 SDs of the residuals, a value was declared an outlier.  \nFor analysis of reading speeds dependent on the different units, we apply linear regression through the origin and display the 95% confidence limits of the regression lines together with the 95% confidence intervals (CIs) for the individual observations. For individual reading times we performed an ANOVA to estimate the variance components between and within subjects.  \nResults \nThe mean (\u00b1SD) number of words, syllables and characters for the different languages are shown in Table 1 (see above), and the mean reading speeds are shown in Table 2 .  \nTable 2.\u2003 \n  \nView Table \n  \nReading Speeds per Minute Expressed in Four Units for Each of 17 Languages \nThe average mean for all languages is 1.42 \u00b1 0.13 texts/min (\u00b1SD), 184 \u00b1 29 words/min, 370 \u00b1 80 syllables/min, and 863 \u00b1 234 characters/min. Reading speed in texts/min corresponded to a mean reading time of 43.7 \u00b1 7.8 sec/text. For the nonalphabetic languages Japanese and Chinese, characters per minute show speeds that are similar to syllables per minute in the other languages. In Hebrew and Arabic, where vowels are not or only partially written, the reading speed in characters per minute is lower compared to other alphabetic languages.  \nFigure 2 shows the reading speeds in three different units (words/min, syllables/min, char/min), dependent on the number of words, syllables, or characters per text in the 17 languages. To prevent too many overlaps of data points, the 17 languages have been divided into two sets of 8 or 9 languages for each of the three units of speed. It becomes clear that the different languages are characterized differently depending on the unit used. Positive correlations are expected because the counts of words, syllables, and characters appear simultaneously in the abscissa and the ordinate. However, because there are only 10 texts per language these correlations are not expected to be significant.  \nFigure 2.\u2003 \nDetailed overview of the summary data from Tables 1 and 2 showing the individual 170 texts for the three units that express the length of a text: words (a, b), syllables (c, d), and characters (e, f). All six figures (a\u2013f) show a linear regression through the origin together with the 95% CI of the estimated slope (inner dashed curves) and the 95% confidence region of the individual values (outer dashed curves). The slope for all three figures is identical, namely 1.415 texts/min (95% CI 1.407\u20131.422 texts/min), which corresponds to the overall average for the third column of Table 2. Since the reading time and reading speed per text are nearly identical for all 17 languages within the 95% CIs for the individual texts from 1.1 to 1.7 texts/min, there is a trivial linear relationship for the units per minute with the number of units per text. The 90% density ellipses in all three plots for each language are constructed such that they contain in the mean 9 of 10 texts. (a\u2013f) The different languages are characterized differently depending on the unit used. Few significant correlations were expected because of only 10 texts per language. (a, b) Reading speed in words per minute dependent on the number of words per text. The more words per text, the higher the number of words per minute because the reading time for all texts is very similar (the number of words for all 10 Swedish texts was the same: 146). (c, d) Reading speed in syllables per minute dependent on the number of syllables per text. Here the Spanish texts are read with the highest number of syllables per minute because Spanish has many short syllables (Fig. 3). The opposite is true for Slovenian. (e, f) Reading speed in characters per minute dependent on the number of characters per text. Even if we include the nonalphabetic languages Chinese and Japanese, we obtain the same regression line through the origin with a slope of 1.4 texts/min. Arabic and Hebrew have the lowest number of characters/text among the alphabetic languages and correspondingly the lowest reading speed expressed in characters per min, because the vowels are not or only partly written. All the other languages are very close together in number of characters per text and characters per minute. \nView Original Download Slide \n  \nDetailed overview of the summary data from Tables 1 and 2 showing the individual 170 texts for the three units that express the length of a text: words ( a , b ), syllables ( c , d ), and characters ( e , f ). All six figures ( a \u2013 f ) show a linear regression through the origin together with the 95% CI of the estimated slope ( inner dashed curves ) and the 95% confidence region of the individual values ( outer dashed curves ). The slope for all three figures is identical, namely 1.415 texts/min (95% CI 1.407\u20131.422 texts/min), which corresponds to the overall average for the third column of Table 2 . Since the reading time and reading speed per text are nearly identical for all 17 languages within the 95% CIs for the individual texts from 1.1 to 1.7 texts/min, there is a trivial linear relationship for the units per minute with the number of units per text. The 90% density ellipses in all three plots for each language are constructed such that they contain in the mean 9 of 10 texts. ( a \u2013 f ) The different languages are characterized differently depending on the unit used. Few significant correlations were expected because of only 10 texts per language. ( a , b ) Reading speed in words per minute dependent on the number of words per text. The more words per text, the higher the number of words per minute because the reading time for all texts is very similar (the number of words for all 10 Swedish texts was the same: 146). ( c , d ) Reading speed in syllables per minute dependent on the number of syllables per text. Here the Spanish texts are read with the highest number of syllables per minute because Spanish has many short syllables ( Fig. 3 ). The opposite is true for Slovenian. ( e , f ) Reading speed in characters per minute dependent on the number of characters per text. Even if we include the nonalphabetic languages Chinese and Japanese, we obtain the same regression line through the origin with a slope of 1.4 texts/min. Arabic and Hebrew have the lowest number of characters/text among the alphabetic languages and correspondingly the lowest reading speed expressed in characters per min, because the vowels are not or only partly written. All the other languages are very close together in number of characters per text and characters per minute. \nFigure 3.\u2003 \nReading time (ms) per syllable dependent on the number of characters per syllable. The reading time per syllable is proportional to the number of characters per syllable. The slope of this line corresponds to the mean reading time per character for all 17 languages, which turns out to be 68 ms per character (95% CI of the means 65\u201371 ms) for 14 languages (except Arabic, Chinese, and Japanese). The SD of the residuals is \u00b150 ms/syllable. In Japanese, one character can consist of more than one syllable and in Chinese, one character corresponds exactly to one syllable. Among the alphabetic languages, Arabic and Hebrew have the lowest number of characters per syllable because of the missing vowels. For the remaining alphabetic languages, Spanish has the lowest number of characters per syllable (1.95) and correspondingly the lowest reading time per syllable (116 ms). In Slovenian we observe the longest reading time per syllable (266 ms) together with the highest number of characters per syllable (3.8). \nView Original Download Slide \n  \nReading time (ms) per syllable dependent on the number of characters per syllable. The reading time per syllable is proportional to the number of characters per syllable. The slope of this line corresponds to the mean reading time per character for all 17 languages, which turns out to be 68 ms per character (95% CI of the means 65\u201371 ms) for 14 languages (except Arabic, Chinese, and Japanese). The SD of the residuals is \u00b150 ms/syllable. In Japanese, one character can consist of more than one syllable and in Chinese, one character corresponds exactly to one syllable. Among the alphabetic languages, Arabic and Hebrew have the lowest number of characters per syllable because of the missing vowels. For the remaining alphabetic languages, Spanish has the lowest number of characters per syllable (1.95) and correspondingly the lowest reading time per syllable (116 ms). In Slovenian we observe the longest reading time per syllable (266 ms) together with the highest number of characters per syllable (3.8). \nFigures 2 a\u2013 2 f display a linear regression through the origin together with the 95% CI of the fit and the 95% confidence region of the individual values. The slopes for all figures are identical, namely 1.415 texts/min (95% CI 1.407\u20131.422 texts/min), which corresponds to the overall average of the third column in Table 2 . The 90% density ellipses are shown in all six plots for each language. Figures 2 a and 2 b show that the more words per text, the higher the number of words per minute because reading speed for all texts is approximately 1.4 texts/min, that is a reading time of 44 seconds/text.  \nFigure 3 displays the reading time in ms per syllable, which is proportional to the number of characters per syllable. The slope of this line specifies the mean reading time per character for all 17 languages. For 14 languages this is 68 ms (95% CI 65\u201371 ms).  \nThe relative variability of reading speeds in words per minute within and between subjects for each language is shown in Table 3 . The variability is caused mainly by differences between subjects (75%\u201393%), that is the differences between the readers of the 10 texts, whereas the texts account only for an average of 11.5% of the variability.  \nTable 3.\u2003 \n  \nView Table \n  \nSummary Statistics for the Comparison of Texts within a Language with Respect to the Reading Speed in Words per Minute \nFigure 4 represents the raw data for Spanish for each subject and all texts. As pointed out in Table 3 , the variability between subjects is much larger than within each subject (only 11.2% of the total variability for Spanish).  \nFigure 4.\u2003 \nIndividual reading times (sec) for the 25 Spanish subjects and the 10 texts. The total variability (SD \u00b1 5.17 sec) mainly is due to the variability among the subjects (88.8%). The SD within one subject is only 1.76 seconds. \nView Original Download Slide \n  \nIndividual reading times (sec) for the 25 Spanish subjects and the 10 texts. The total variability (SD \u00b1 5.17 sec) mainly is due to the variability among the subjects (88.8%). The SD within one subject is only 1.76 seconds. \nThe comparison between the texts in each language showed 331 statistically significant differences out of 765 differences (45 pairwise comparisons for each of the 17 languages) after adjustment for multiple testing according to Tukey's HSD test. (Without adjustment the number of significant differences would have been 468.) This large number is explained by the fact that we performed the comparisons within one subject, and that the variability within one subject is much smaller than the variability between subjects. Only 81 of the 765 comparisons showed a difference between the criterion \u201cstatistically significant\u201d and \u201cdifference >10 words/min.\u201d In the following, only the clinically relevant differences (>10 words/min) are considered. The correlations of reading time among the 10 texts were high: mean pairwise \u201c r \u201d ranged from 0.772 in Chinese to 0.934 in Swedish. In Table A1 the reading speeds expressed as words per minute for each language are shown for each text. Texts with the same letter do not differ by more than 10 words/min.  \nIn Table 4 we provide the lower normal 2.5% quantiles of reading speed (words per minute) taking into account that reading speed is influenced by age (here 18\u201335 years) and reading skill.  \nTable 4.\u2003 \n  \nView Table \n  \nNormal Lower Reference Values for Reading Speeds (Words per Minute) for 17 Languages \nDiscussion \nOur study was performed to develop equivalent and standardized text passages in different languages, and to assess reading speeds of normally-sighted young adults during reading unknown texts. The IReST differs from prior text charts that used single sentences by employing linguistically standardized paragraphs. They close a gap in the diagnostics of reading performance. The paragraphs are designed to resemble everyday life situations of reading continuous text, such as in books or newspapers. The level of difficulty corresponds to sixth grade reading (10\u201312 years), which is comprehensible for teenagers and adults. Due to lower variance, measuring reading time of a complete paragraph rather than a single sentence or random words is more reliable. Furthermore, it can provide some information about fluency, fatigue, and mistakes.  \nOther reading tests are valuable for other indications: The MN Read 1 \u2013 3 and Radner 4 , 5 texts use short and simple (second and third grade material) single sentences in different print sizes to assess reading acuity, critical print size, and magnification need. 18 Radner compared reading speeds of German-speaking readers of the single-sentence Radner texts (third grade sentences) 5 to those of paragraphs from the Zuercher Reading Test 19 (ZRT, standardized for fourth to sixth grade children) and found a high correlation between the short sentences and the ZRT, but a mean difference of 40 words/min\u2014the single sentences being read much faster. This can be attributed at least partially to the higher linguistic difficulty of the ZRT, and no conclusion can be drawn in regard to the difference between one sentence and paragraph reading.  \nRandom word reading tests can be helpful to find typical mistakes to get an idea about the location of a scotoma (Nair UK, et al. IOVS 2006:ARVO E-Abstract 3481). The Colenbrander Mixed Contrast Reading Cards 20 assess the influence of contrast sensitivity on reading of unrelated simple sentences at fourth grade level. The Pepper Test was reported to be useful in designing training programs in different low vision conditions. 21 , 22 Therefore, all these different text charts have their specific indication and supplement each other regarding specific aspects of reading, and cannot be compared directly.  \nAnother new feature of IReST is the fact that they provide a set of 10 texts in each language for repeated measurements. Since equivalent sets of 10 texts are available in 17 languages, they provide a new tool for international multi-language studies. Due to the language characteristics, the counts of words, syllables, and characters differ considerably among the languages ( Table 1 ). The text, as a unit of the same content in all languages, is processed in a quite comparable time between the languages. Therefore, in normal subjects, the differences in spatial length of the texts (number of lines) between languages do not seem to have a major role, because the perceptual span, 23 that is the number of letters perceived during one fixation, is sufficient in normal subjects. This span can be expected reasonably to be slightly different between different languages due to the varying average word length. Further analysis showed different reading speeds for the different units, which depends on the language structure ( Fig. 2 ). In our previous reports, we used characters (with spaces and punctuation marks) to measure reading speed because the texts were nearly identical with respect to this variable. 6 , 7 For scientific use, especially from a speech articulation point of view, syllables per minute can be more conclusive, whereas words per minute are not suitable because of differences in word lengths between languages. However, for everyday life, words per minute is more common and more intuitive, and can be considered as a reasonable compromise, if all languages are considered. All these measures are displayed in the IReST charts to provide the user with the units needed for a specific question ( Fig. 1 ).  \nMean reading time per character for 14 languages (except Arabic, Chinese, Japanese) is 68 ms (95% CI 65\u201371 ms) determined by the slope of the line in Figure 3 . This explains why Spanish is the fastest and Slovenian the slowest among the alphabetic languages (excluding Arabic and Hebrew) if reading speed is based on syllables per minute, because in Slovenian a syllable has approximately twice the number of characters.  \nThe texts account for only 11.5% of the total variability ( Table 3 ), that is they are well designed with a high equivalence between each other in each language. The individual reading speed, which depends on reading habits and skills, is the main reason for the variability. It also becomes clear that within one individual reader the variability is relatively low with a high correlation coefficient when reading all 10 texts ( r = 0.772\u20130.934).  \nAge also has a role for reading speed. In this study, age 18 to 35 years was an inclusion criterion. We showed in a previous study using IReST 6 (first edition) that older subjects (aged 60\u201385 years) read more slowly by a factor of approximately 20% (English and German readers).  \nBased on the results and considering the different language structures, the following instructions for use are recommended: For use within one language direct comparison is possible for texts without a clinically relevant reading speed difference of \u226410 words/min (Table A1). Dependent on the question, any unit can be used (see above and Fig. 2 ). Between languages, in clinical studies normally only the relative difference of reading speed is needed, for example before and after interventions. Therefore, a direct comparison of absolute reading speeds between languages normally is not necessary, but possible using a correction factor.  \nFields of Applications \nThe IReST charts are suitable for a wide field of applications, for assessment of reading speed for:  \n \n      \n    Diagnostics. \n      \n    Course monitoring. \n      \n    Effect documentation after interventions. \n      \n    Low vision patients. \n      \n    Effect of multifocal intraocular lens implantation. \n      \n    Neurological reading disorders (e.g., hemianopia, alexia). \n      \n    Developmental dyslexia. \n      \n    Studies with normal subjects. \n \nTwo patient groups will be discussed in more detail.  \nLow Vision Patients \nThe aim of low vision rehabilitation is reading ability of common print (1 M). Patients with central scotoma regain reading ability by the use of an eccentric retinal locus and magnifying visual aids to compensate for the lower resolution. 24 \u2013 30 Therefore, low vision patients must read the IReST with their magnifying aids\u2014a requirement of everyday reading situations. In patients, the mistakes are, of course, much more frequent than in normal subjects and must be taken into account by subtracting the omitted or incorrectly read words from the entire number of words of the text when calculating the reading speed (60 \u00d7 number of correctly read words/reading time in seconds).  \nThe reading speeds in English and German native-speaking patients with age-related macular degeneration (AMD) were assessed in two studies using the IReST (first edition), when patients read the texts with appropriate magnification. A total of 40 English AMD patients had a mean reading speed of 0.44 \u00b1 0.39 log char/sec, that is 38 words/min, viewed at their critical print size. 31 In 530 German AMD patients, the mean reading speed was 20 \u00b1 33 words/min before and 72 \u00b1 35 words/min after adaptation of optimal magnifying visual aids. 32   \nDevelopmental Dyslexia \nIn this diagnosis, not only the speed, but also the kind of errors is important. For example, when measuring reading performance in developmental dyslexia, standardized texts previously were available only for young children and not for higher grades or adults. In a recent study in German teenagers (mean age 18 \u00b1 3.3 years) using IReST (first edition), we found a mean reading speed of 184 words/min, 33 which does not differ from the cohort examined here of German young adults (18\u201335 years) with 179 words/min, which confirms that reading aloud does not increase after the age of 15 to 18 years (due to speech rate ceiling). 34 Therefore, these texts also close a gap for testing teenagers and young adults, especially in patients with developmental dyslexia. In developmental dyslexia research, it is of special interest to compare reading performance in different languages, because reading strategies depend very much on the orthographic regularity of a language. 35 \u2013 37 The texts can open a new opportunity to perform multi-language studies in the research on developmental dyslexia.  \nTo summarize, the IReST provide a reproducible outcome measure that will facilitate cross-language comparisons, and are suitable for monitoring the impact of eye disease and success of interventions. The second edition of the IReST charts with 17 languages will be available in 2012. For further information, see www.amd-read.net .  \nAcknowledgments \nManfred MacKeben, PhD, The Smith Kettlewell Eye Research Institute, San Francisco, California, provided valuable comments and discussions. Malin Safioti supplied an earlier version of the Swedish texts. Sam Featherston performed statistical support in an early stage of the study. Alexander N. Sokolov, PhD, provided data documentation and statistics in earlier stages of the project. Vera Orth, University Library Tuebingen, contributed first analysis of syllables of the Arabic texts, and Ellen Skodlar, Centre for Ophthalmology, University of Tuebingen performed secretarial work and assistance in coordination. Three anonymous referees supplied detailed comments that led to many clarifications in the paper.  \nReferences \n1. \nAhn SJ Legge GE Luebker A. Printed cards for measuring low-vision reading speed. Vis Res . 1995;35:1939\u20131944. [CrossRef] [PubMed] \n2. \nLegge GE Ross JA Luebker A LaMay JM. Psychophysics of reading VIII. The Minnesota Low-Vision Reading Test. Optom Vis Sci . 1989;66:843\u2013853. [CrossRef] [PubMed] \n3. \nMansfield JS Ahn SJ Legge GE Luebker A. A new reading acuity chart for normal and low vision. Ophthalmic Vis Opt . 1993;3:232\u2013235. \n4. \nRadner W Willinger U Obermayer W Mudrich C Velikay-Parel M Eisenwort B. A new reading chart for simultaneous determination of reading vision and reading speed. Klin Monatsbl Augenheilkd . 1998;213:174\u2013181. [CrossRef] [PubMed] \n5. \nRadner W Obermayer W Richter-Mueksch S Willinger U Velikay-Parel M Eisenwort B. The validity and reliability of short German sentences for measuring reading speed. Graefes Arch Clin Exp Ophthalmol . 2002;240:461\u2013467. [CrossRef] [PubMed] \n6. \nHahn G Penka D Gehrlich C New standardised texts for assessing reading performance in four European Languages. Br J Ophthalmol . 2006;90:480\u2013484. [CrossRef] [PubMed] \n7. \nMessias A Velasco e Cruz A Schallenm\u00fcller S Trauzettel-Klosinski S. New standardized texts in Brazilian Portuguese to assess reading speed\u2014comparison with four European languages. Arq Bras Oftalmol . 2008;71:553\u2013558. [CrossRef] [PubMed] \n8. \nGibson E. Linguistic complexity: locality of syntactic dependencies. Cognition . 1998;68:1\u201376. [CrossRef] [PubMed] \n9. \nGibson E. The dependency locality theory: a distance-based theory of linguistic complexity. In: Miyashita Y Marantz A O'Neil W eds. Image, Language, Brain . Cambridge, MA: MIT Press; 2000:95\u2013126. \n10. \nContemporary Chinese Dictionary, 5th ed. Beijing, China: The Commercial Press; 2005. \n11. \nWang T Ruan Z Chang X Chinese Idioms Dictionary, 1st ed. Shanghai, China: Shanghai Lexicographical Publishing House; 1987. \n12. \nLi D Cheng M. A Practical Chinese Grammar for Foreigners, 2nd ed. Beijing, China: Sinolingua; 1988. \n13. \nLiu X. New Practical Chinese Reader . Beijing, China: Beijing Language and Culture University Press; 2002. \n14. \nShono L. Arabic texts for assessing reading performance . Tuebingen, Germany: Aalen University, Dept. for Optometry, and University of Tuebingen, Germany. 2008. Master thesis. \n15. \nSloan LL. New test charts for the measurement of visual acuity at far and near distances. Am J Ophth . 1959;48:807\u2013813. [CrossRef] [PubMed] \n16. \nDeMarco LM Massof RW. Distributions of print sizes in U.S. newspapers. J Vis Impair Blindness . 1997;91:9\u201315. \n17. \nHsu JC. Multiple Comparisons: Theory and Methods, 1st ed. Boca Raton, FL: Chapman & Hall; 1996. \n18. \nChung ST Mansfield JS Legge GE. Psychophysics of reading. XVIII. The effect of print size on reading speed in normal peripheral vision. Vis Res . 1998;38:2949\u20132962. [CrossRef] [PubMed] \n19. \nLindner M Grissemann H. Zuercher Lesetest . Berne, Switzerland: Huber; 1968. \n20. \nColenbrander A Fletcher DC. The mixed contrast reading card, a new screening test for contrast sensitivity. In: Jones S Rubin G Hamlin D eds. Vision 2005 . London, United Kingdom: Elsevier International Congress Series; 2006:492\u2013497. \n21. \nStelmack J Stelmack TR Fraim M Warrington J. Clinical use of the Pepper Visual Skills for Reading Test in low vision rehabilitation. Am J Optom Physiol Opt . 1987;64:829\u2013831. [CrossRef] [PubMed] \n22. \nWatson G Baldasare J Whittaker S. The validity and clinical uses of the Pepper Visual Skills for Reading Test. J Vis Impair Blindness . 1990;84:119\u2013123. \n23. \nMcConkie GW Rayner K. Asymmetry of the perceptual span in reading. Bull Psychonomic Soc . 1976;8:365\u2013368. [CrossRef] \n24. \nBullimore MA Bailey IL. Reading and eye movements in age-related maculopathy. Optom Vis Sci . 1995;72:125\u2013138. [CrossRef] [PubMed] \n25. \nCrossland MD Culham LE Rubin GS. Predicting reading frequency in patients with macular disease. Optom Vis Sci . 2005;82:11\u201317. [CrossRef] [PubMed] \n26. \nFalkenberg HK Rubin GS Bex PJ. Acuity, crowding, reading and fixation stability. Vision Res . 2007;47:126\u2013135. [CrossRef] [PubMed] \n27. \nLovie-Kitchin JE Bowers AR Woods RL. Oral and silent reading performance with macular degeneration. Ophthalmic Physiol Opt . 2000;20:360\u2013370. [CrossRef] [PubMed] \n28. \nTimberlake GT Peli E Essock EA Augliere RA. Reading with a macular scotoma. II. Retinal locus for scanning text. Invest Ophthalmol Vis Sci . 1987;28:1268\u20131274. [PubMed] \n29. \nTrauzettel-Klosinski S Tornow RP. Fixation behavior and reading ability in macular scotoma. Neuro-Ophthalmol . 1996;16:241\u2013253. [CrossRef] \n30. \nTrauzettel-Klosinski S. Rehabilitation for visual disorders. J Neuro-Ophthalmol . 2010;30:73\u201384. [CrossRef] \n31. \nRubin GS Feely M. The role of eye movements during reading in patients with age-related macular degeneration (AMD). Neuro-Ophthalmol . 2009;33:120\u2013126. [CrossRef] \n32. \nNguyen NX Weismann M Trauzettel-Klosinski S. Improvement of reading speed after providing of low vision aids in patients with age-related macular degeneration. Acta Ophthalmol . 2009;87:849\u2013853. [CrossRef] [PubMed] \n33. \nHofmann C. Studies on different processing of script and pictograms in dyslexics and normal readers [in German]. Tuebingen, Germany: Medical Faculty, University of Tuebingen. 2011. Thesis. \n34. \nTressoldi P Stella G Faggella M. The development of reading speed in Italians with dyslexia: a longitudinal study. J Learn Disabil . 2001;34:414\u2013417. [CrossRef] [PubMed] \n35. \nD\u00fcrrw\u00e4chter U Sokolov AN Reinhard J Klosinski G Trauzettel-Klosinski S. Word length and word frequency affect eye movements in dyslexic children reading in a regular (German) orthography. Ann Dyslexia . 2010;60:86\u2013101. [CrossRef] [PubMed] \n36. \nStella G Savelli E Scorza MS Morlini I. La dislessia evolutiva lungo l'arco della scolarit\u00e0 obbligatoria. In: Vicari S Caselli MC eds. Neuropsicologia Dello Sviluppo . Bologna, Italy: Il Mulino Editore; 2010:161\u2013178. \n37. \nTrauzettel-Klosinski S Koitzsch M D\u00fcrrw\u00e4chter U Sokolov AN Reinhard J Klosinski G. Eye movements in German-speaking children with and without dyslexia when reading aloud. Acta Ophthalmol . 2010;88:681\u2013691. [CrossRef] [PubMed] \nFootnotes \n  Supported by the European Commission in the AMD Read Project (QLK6-CT-2002-00214, the first study for 4 languages), the German Academic Exchange Office (for Portuguese/Brazilian), and Alcon Laboratories, Inc., Fort Worth, Texas (development and evaluation of 11 more languages). \nFootnotes \n  Disclosure: S. Trauzettel-Klosinski , None; K. Dietz , None \nAppendix \nThe IReST Study Group \nPrincipal Investigators. Arabic: Lama Shono, University of New South Wales, Australia/Saudi-Arabia.  \nChinese: Lin Wang, Research and Outcomes Unit Care Management Administration, Johns Hopkins HealthCare LLC, Johns Hopkins Medicine, Baltimore, MD.  \nDutch: Ger van Rens, Department of Ophthalmology, VU University Medical Centre Amsterdam, The Netherlands.  \nEnglish: Gary S. Rubin, Department of Visual Neuroscience, UCL Institute of Ophthalmology, London UK.  \nFinnish: Lea Hyv\u00e4rinen, University of Helsinki, and Markku Leinonen, Department of Ophthalmology, Turku University Hospital, Finland.  \nFrench: Fran\u00e7ois Vital-Durand, SBRI Unit 846 Inserm, Bron, UCB-Lyon1, France.  \nGerman: Susanne Trauzettel-Klosinski (co-ordinator), 1 Klaus Dietz, 2 Raphael Niebler, 3 Gesa A. Hahn, 1 from the 1 Centre for Ophthalmology, 2 Department of Medical Biometry, and 3 Department for Psychiatry, University of Tuebingen, Tuebingen, Germany.  \nHebrew: Anat Kesler and Uri Soiberman, Neuro-Ophthalmology Unit, Department of Ophthalmology, Tel Aviv Medical Center, Israel.  \nItalian: Giacomo Stella, Department of Education and Human Sciences, University of Modena and Reggio Emilia, Italy.  \nJapanese: Satoshi Kashii, Faculty of Health and Medical Sciences, Aichi Shukutoku University, Nagakute-City, Japan.  \nPolish: Slawomir Teper, Department of Ophthalmology, Okregowy Szpital Kolejowy in Katowice, Poland.  \nPortuguese/Br: Andre Messias, Department of Ophthalmology, USP, Ribeir\u00e3o Preto, Brazil.  \nRussian: Natalia Eliseeva, Burdenko Neurosurgery Institute, Moscow, Russia.  \nSlovenian: Marku Hawlina and Polona Jaki Mekjavi\u0107, Department of Ophthalmology, Faculty of Medicine, University of Ljubljana, Slovenia.  \nSwedish: Jorgen Gustafsson, Section of Optometry and Vision Sciences, Linnaeus University, Kalmar, Sweden.  \nSpanish: Jorge Arruga, Hospital Universitario de Bellvitge (HUB), Barcelona, Spain.  \nTurkish: Do\u011fan Ceyhan, Y\u00fcz\u00fcnc\u00fc Y\u0131l University Department of Ophthalmology, Van, Turkey.  \nLinguists. Arabic: Haris Mohammad, Al-Azhar University, German Department, Cairo, Egypt.  \nChinese: Jianguo Lu, Department of Psychology, Chengdu Medical College, Chengdu, China.  \nDutch: Evy G. Visch-Brink, Department of Neurology, Erasmus MC, University Medical Center, Rotterdam, The Netherlands.  \nGerman: Doris Penka, Ocuserv, Department of Linguistics, University of Tuebingen, Germany/Department of Linguistics, University of Konstanz, Germany.  \nItalian: Gabriele Pallotti, University of Modena and Reggio Emilia, Department of Education, Italy.  \nJapanese: Susanne Miyata, Faculty of Health and Medical Sciences, Aichi Shukutoku University, Nagakute-City, Japan.  \nPolish: Ewa Baglajewska-Miglus, Europe University Viadrina, Language Centre, Frankfurt (Oder), Germany.  \nPortuguese/Br: Sonia Jecov Schallenm\u00fcller, Instituto de Ensino Brasil-Alemanha, Ribeir\u00e3o Preto, Brazil.  \nRussian: Valeri Belianine, Institute of Psycholinguistic Psychotherapy, Toronto, Canada.  \nSlovenian: Uro\u0161 Mozeti\u010d, University of Ljubljana, Faculty of Arts, Department of English, Slovenia.  \nSpanish: Ant\u00f2nia Mart\u00ed, CLiC-Centre de Llenguatge i Computaci\u00f3, University of Barcelona, Spain.  \nSwedish: Gustaf \u00d6qvist Seimyr, The Bernadotte Laboratories, St. Erik Eye Hospital, Stockholm, Sweden.  \nTurkish: Dilek Fidan, Kocaeli University, Faculty of Education, Department of Turkish Language Teaching, Kocaeli, Turkey.  \n(see Table A1)  \nTable A1.\u2003 \n  \nView Table \n  \nReading Speeds for Each Text in Each Language, Ranked in Order of Decreasing Speed. Texts with the Same Letter in Performance Category A to F Differ not More than 10 Words per Minute \nCopyright \u00a9 Association for Research in Vision and Ophthalmology \n40,842 Views \n157 Web of Science \nArticle has an altmetric score of 108 \nView Metrics \nRelated Articles \nSlow Reading in Glaucoma: Is it due to the Shrinking Visual Span in Central Vision? \nBaseline MNREAD Measures for Normally Sighted Subjects From Childhood to Old Age \nClinical and Microperimetric Predictors of Reading Speed in Low Vision Patients: A Structural Equation Modeling Approach \nThe Influence of Manual Dexterity on Reading Speed with a Hand-held Magnifier \nPsychophysics of reading. Clinical predictors of low-vision reading speed. \nFrom Other Journals \nBayesian adaptive assessment of the reading function for vision: The qReading method \nThe dynamic effect of reading direction habit on spatial asymmetry of image perception \nEffects of visual span on reading speed and parafoveal processing in eye movements during sentence reading \nSensory and cognitive influences on the training-related improvement of reading speed in peripheral vision \nComparing reading speed for horizontal and vertical English text \nRelated Topics \n \n    Low Vision \n    Eye Anatomy and Disorders \n \nAdvertisement \n \n    IOVS Home \n    Issues \n    Topics \n    For Authors \n    About \n        Editorial Board \n \nOnline ISSN: 1552-5783 \n \n    Investigative Ophthalmology & Visual Science \n    Journal of Vision \n    Translational Vision Science & Technology \n \n    JOURNALS HOME \n    TOPICS \n    ABOUT ARVO JOURNALS \n        Rights & Permissions \n        Privacy Statement \n        Advertising \n        Submit a Manuscript \n        Disclaimer \n        Contact Us \n        ARVO.org \n \nCopyright \u00a9 2015 Association for Research in Vision and Ophthalmology. \nThis site uses cookies. By continuing to use our website, you are agreeing to our privacy policy. Accept \n",
        "hash_id": "b459c48cc6ef82fa88f5b6b80b180bae"
    },
    {
        "key": "I46N8RYW",
        "version": 56,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/I46N8RYW",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/I46N8RYW",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/MCNCUB84",
                "type": "application/json",
                "attachmentType": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Trauzettel-Klosinski et al.",
            "parsedDate": "2012-08-13",
            "numChildren": 1
        },
        "citation": "<span>Susanne Trauzettel-Klosinski, Klaus Dietz &#38; the IReST Study Group, <i>Standardized Assessment of Reading Performance: The New International Reading Speed Texts IReST</i>, 53 <span style=\"font-variant:small-caps;\">Investigative Ophthalmology &#38; Visual Science</span> 5452\u20135461 (2012), https://doi.org/10.1167/iovs.11-8284 (last visited Feb 1, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "6QTZND3N",
        "version": 55,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/6QTZND3N",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/6QTZND3N",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Sabatini",
            "parsedDate": "2002-07-01",
            "numChildren": 0
        },
        "citation": "<span>John P Sabatini, <i>Efficiency in Word Reading of Adults: Ability Group Comparisons</i>, 6 <span style=\"font-variant:small-caps;\">Scientific Studies of Reading</span> 267\u2013298 (2002), https://doi.org/10.1207/S1532799XSSR0603_4 (last visited Feb 1, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "INQFS9PE",
        "version": 53,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/INQFS9PE",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/INQFS9PE",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/4KJ2DHMY",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/INQFS9PE/file/view",
                "title": "Fran\u00e7ois et al. - 2020 - AMesure A Web Platform to Assist the Clear Writin.pdf",
                "length": 593576
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://aclanthology.org/2020.aacl-demo.1.pdf (last visited Feb 1, 2023).</span>",
        "fulltext": "AMesure: a web platform to assist the clear writing of administrative texts\n\nThomas Franc\u00b8ois\n\nAdeline Mu\u00a8 ller\n\nIL&C, CENTAL/TeAMM IL&C, CENTAL\n\nUCLouvain\n\nUCLouvain\n\nEva Rolin IL&C, CENTAL\nUCLouvain\n\nMagali Norre\u00b4 IL&C, CENTAL\nUCLouvain\n\nAbstract\nThis article presents the AMesure platform, which aims to assist writers of French administrative texts in simplifying their writing. This platform includes a readability formula specialized for administrative texts and it also uses various natural language processing (NLP) tools to analyze texts and highlight a number of linguistic phenomena considered dif\ufb01cult to read. Finally, based on the dif\ufb01culties identi\ufb01ed, it offers pieces of advice coming from of\ufb01cial plain language guides to users. This paper describes the different components of the system and reports an evaluation of these components.\n1 Introduction\nIn our current society, written documents play a central role as an information channel, especially in the context of communication between institutions and their target audiences (Madinier, 2009). Unfortunately, although efforts to raise the education level of the population worldwide have increased in recent decades, reports (OECD, 2016) point out that a signi\ufb01cant proportion of citizens still have general reading dif\ufb01culties. As regards administrative texts, various reading issues have been reported. For instance, Kimble (1992) reported that, in a survey carried out in the US, 58% of the respondents admitted to dropping out of an administrative process due to the reading dif\ufb01culty.\nAdministrations have been aware of this issue for decades and have launched various initiatives to address it, the most prominent of which is the Plain Language movement. Plain language aims to increase the accessibility of legal documents for a general audience and has been shown to both reduce costs and please readers (Kimble, 1996). It has not only been promoted through various campaigns (e.g. Plain English Campaign in the UK) and writing guides (Gouvernement du Que\u00b4bec,\n\n2006; Ministe`re de la Communaute\u00b4 franc\u00b8aise de Belgique, 2010; European Union, 2011; Plain Language Action and Information Network, 2011; Cutts, 2020), but also incorporated in some legal principles. However, its widespread application is still undermined due to, for example, the efforts required to train writers (Desbiens, 2008), or the necessity to persuade writers \u2013 especially legal ones \u2013 to abandon their \ufb02owery style, which is seen as a determinant of the image of expertise they project in the reader\u2019s mind (Adler, 2012). This second reason, however, falls beyond the scope of the current study, which aims to address the \ufb01rst reason, i.e. writers\u2019 training.\nRecent research by Nord (2018) revealed that although several plain language guides are available to assist writers of administrative texts in their work, the guidelines provided in these guides are not always followed by writers, mainly because they are too vague and too numerous. To relieve writers from the need to keep all these guidelines in mind, we have designed a web platform, AMesure1, aimed at automatically identifying clear writing issues in administrative texts and providing simple writing advice that is contextually relevant. In its current state, the platform offers the three following functionalities: (1) providing an overall readability score based on a formula specialized to administrative texts; (2) identifying, in a text, linguistic phenomena that are assumed to have a negative effect on the comprehension of the text; (3) for the phenomena detected in step 2, proposing simpli\ufb01cation advice found in plain language guides.\nIn the following sections, we \ufb01rst refer to some related work (Section 2), before describing the NLP analyses carried out to operate the system (Section 3.1). Then, we introduce the system and the way suggestions are provided (Section 3.2). The paper\n1The platform is freely available online at https:// cental.uclouvain.be/amesure/.\n\n1\nProceedings of the 1st Conference of the Asia-Paci\ufb01c Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 1\u20137\nDecember 4 - 7, 2020. c 2020 Association for Computational Linguistics\n\nconcludes with a report about the system performance (Section 4).\n2 Related work\nThis work stands at the intersection between two very different \ufb01elds: writing studies \u2013 \u201cthe interdisciplinary science that studies all the processes and knowledge involved in the production of professional writings and their appropriateness for the addressees\u201d (Labasse, 2001) \u2013 and automatic text simpli\ufb01cation (ATS), a branch of NLP that aims to automatically adapt dif\ufb01cult linguistic structures while preserving the meaning to enhance text accessibility.\nRelevant facts from writing studies have already been covered in the introduction. As regards ATS, the last few years have witnessed the publication of numerous interesting studies, reviewed by Shardlow (2014), Siddharthan (2014), and Saggion (2017). In brief, the \ufb01eld has mainly focused on developing algorithms to automatically simplify complex words (lexical simpli\ufb01cation) and/or complex syntactic structures (syntactic simpli\ufb01cation). It has \ufb01rst relied on rule-based approaches (Chandrasekar et al., 1996; Siddharthan, 2011) in which a text is automatically parsed before being applied simpli\ufb01cation rules de\ufb01ned by experts. Later, ATS has been assimilated to a translation task (the original version is translated into a simpli\ufb01ed version) and addressed with statistical translation systems (Specia, 2010; Zhu et al., 2010). As neural machine translation has emerged under the impulse of deep learning, the Seq2Seq model has become prevalent for ATS too (Nisioi et al., 2017; Zhang and Lapata, 2017).\nSome work has speci\ufb01cally focused on the issue of lexical simpli\ufb01cation, which involves different techniques. Lexical simpli\ufb01cation is generally operated in four steps, the \ufb01rst one being the identi\ufb01cation of complex words. Some systems choose to consider all words as candidates for substitution (Bott et al., 2012); others use a list of complex words or machine learning techniques for classi\ufb01cation of complex words (Alarcon et al., 2019). Once complex words have been identi\ufb01ed, the next step is the generation of simpler synonyms for substitution, either by relying on lexical resources (De Belder and Moens, 2010; Billami et al., 2018), getting candidates from corpora (Coster and Kauchak, 2011), producing them with embeddings (Glavas\u02c7 and S\u02c7 tajner, 2015; Paetzold\n\nand Specia, 2016) or, more recently, with BERT (Qiang et al., 2020). In a next step, the candidates are semantically \ufb01ltered to \ufb01t the context and are ranked according to their dif\ufb01culty by classi\ufb01ers using various word characteristics (e.g. frequency, embedding, morphemes, syllabic structures, etc.) (Paetzold and Specia, 2017; Billami et al., 2018; Qiang et al., 2020).\nAlthough numerous ATS systems are described in publications, we have found only four of them that made their way through a web platform tailored to writers\u2019 needs. Scarton et al. (2010) developed a simpli\ufb01cation web platform for Portuguese, in which the user is able to either accept or reject simpli\ufb01cations done by the system. Similarly, Lee et al. (2016)\u2019s system performs lexical and syntactic simpli\ufb01cations for English and supports human post-editing. More recently, Falkenjack et al. (2017) introduced TeCST, which is able to perform simpli\ufb01cation at different levels, depending on the user. Finally, Yimam and Biemann (2018) implemented a semantic writing aid tool able to suggest context-aware lexical paraphrases to writers. None of these tools, however, have focused on writers of administrative texts, nor on French.\nAMesure could also be related to the family of writing assistants, such as Word or LibreOf\ufb01ce. However, only a few of them provides writing advice based on speci\ufb01c criteria or plain language guides. There are some examples of these tools available for the general public in French: (1) Plainly2; (2) Lirec3 which relies on the FALC guidelines, an equivalent of the Easy-to-Read language in French, tailored to people with a cognitive disability; or (3) Antidote4, which offers various writing advice to be clearer and includes \ufb01ve readability indexes. These are however commercial tools, whose scienti\ufb01c foundations are dif\ufb01cult to know and to compare to.\n3 The platform\nAMesure aims to help writers to produce clear and simple administrative texts for a general audience5.\nFor this purpose, it offers various diagnoses about the reading dif\ufb01culty of a text as well as\n2https://www.labrador-company.fr/ outil-langage-clair/\n3http://sioux.univ-paris8.fr/lirec/ 4https://www.antidote.info/fr 5People with low reading levels require even more simpli\ufb01ed texts (with shorter sentences, no subordinated clauses, etc.). This \u201doversimpli\ufb01cation\u201d falls under the scope of the Easy Language domain.\n\n2\n\nadvice on simpler ways of writing. Before moving to the description of the platform in Section 3.2, we \ufb01rst introduce the various NLP processes used to analyse the text and annotate dif\ufb01culties in Section 3.1.\n3.1 The analysis of the text\nAs soon as a text is uploaded on the platform, it is processed through various NLP tools to get a rich representation of the text, on which further rulebased processes are then applied. In a \ufb01rst step, the text is split into sentences and POS-tagged with MElt (Denis and Sagot, 2012), before being syntactically parsed with the Berkeley parser adapted for French (Candito et al., 2010). As a result, each sentence is represented as a dependency tree, on which we apply a set of handcrafted rules expressed in the form of regular expressions using the Tregex (Levy and Andrew, 2006) syntax. The rules currently implemented (Franc\u00b8ois et al., 2018) are able to identify four classes of complex syntactic structures: passive clauses, relative clauses, object clauses, and adverbial clauses. Identifying these four classes is motivated by the characteristics of administrative texts. Passive clauses and in\ufb01nitive verbs are often used in administrative texts to conceal the presence of the writer (Cusin-Berche, 2003), while other types of clause are used to provide the reader with as many detailed information as possible (Catherine, 1968). Parentheticals are also identi\ufb01ed, as they are prone to hinder the reading process.\nIn a second step, the tagged text is further processed to carry out lexical analyses of the text. During this step, three types of lexical dif\ufb01culties are identi\ufb01ed. Firstly, rare words are detected relying on frequencies from Lexique3 (New et al., 2007), based on a threshold set empirically.\nSecondly, technical terms are detected with some heuristics able to detect both simple terms and multi-word terms \u2013 a task that remains a challenge for current fully automatic approaches (da Silva Conrado et al., 2014) \u2013 that are included in a database. The database has been compiled from three different sources: (1) the of\ufb01cial lists from the Belgian administration; (2) a list of terms extracted from a corpus of 115 administrative texts following the automatic extraction approach of Chung (2003) and then manually validated; and (3) a book describing various characteristics of the administrative style and listing administrative terms (Catherine, 1968). At the end of the collection phase, we\n\nobtained 3,382 terms, some of which could, however, not be considered as dif\ufb01cult (e.g. academy, degree, jury, trainee, etc.). We therefore \ufb01ltered the resource by excluding words found in the list of the 8000 simplest words in French (Gougenheim et al., 1964). As result, the \ufb01nal term database amounts to 2,481 entries.\nThirdly, abbreviations are automatically detected as they are known to produce reading errors, especially when they are used by specialized writers to communicate to non-specialized readers. For instance, Sinha et al. (2011) report that the Joint Commission on Accreditation of Healthcare Organizations estimated that 5% of medical errors are due to abbreviations. In our system, abbreviations are detected based on an abbreviation database, collected from Belgian public authorities. The database relate the extended version(s) of abbreviations (e.g. communaute\u00b4 franc\u00b8aise, Institutions publiques de protection de la jeunesse) with the corresponding abbreviated forms (e.g. comm. fr.; IPPJ and I.P.P.J. respectively). The list provided by public authorities was supplemented via a semiautomatic extraction process applied to our corpus of 115 administrative texts. This extraction process was based on manual rules maximizing the recall, in order to extract all forms prone to be abbreviations. Then, we \ufb01ltered out all forms already in our list and manually checked the remaining ones, obtaining a \ufb01nal database with 2,022 entries.\n3.2 Description of the platform\nLeveraging the NLP analysis described above, the AMesure platform provides four types of diagnoses about texts to its users, as illustrated in Figure 1. The \ufb01rst diagnosis (marked by the letter A in the Figure 1) is a global readability score for the text. It is computed by a readability formula, specialized for administrative texts, that we previously developed (Franc\u00b8ois et al., 2014). The output score ranges from 1 (for very easy texts) to 5 (for very complex texts) and is yielded by a support vector machine classi\ufb01er combining 10 linguistic features of the text (e.g. word frequency, proportion of complex words, type-token ratio, mean length of sentence, ratio of past participle forms, etc.).\nThe second type of diagnosis (letter B in Figure 1) is more detailed and includes 11 readability yardsticks, each corresponding to one linguistic characteristic of the text known to affect reading. The psycholinguistic rationales for the choice of\n\n3\n\nthese yardsticks have been discussed in length in Franc\u00b8ois (2011), who has de\ufb01ned a set of 344 variables. Among this set, we have retained 11 yardsticks based on a correlational analysis (Franc\u00b8ois et al., 2014). In the interface, the yardsticks are organised according to three linguistic dimensions of texts: lexicon, syntax, and textual aspects. The \ufb01ve lexical yardsticks capture (1) the percentage of dif\ufb01cult words, based on the list of 8000 simplest words in French (Gougenheim et al., 1964); (2) the number of rare words (see Section 3.1); (3) the density of abbreviations (see Section 3.1); (4) the proportion of unexplained abbreviations; and (5) the number of technical words (see Section 3.1). The four syntactic yardsticks include (1) the dif\ufb01culty of the syntactic structures estimated roughly as the ratio of conjunctions and pronouns; (2) the mean number of words per sentence; (3) the ratio of structures considered as complex by plain language guides among all syntactic structures detected (see Section 3.1); and (4) the total number of sentences. As regards the two textual yardsticks, they include (1) a score corresponding to the level of personalization of the texts (text using pronouns at the \ufb01rst or at the second person are considered to be more readable (Daoust et al., 1996)); and (2) a score corresponding to the average intersentential coherence of the text. It is measured as the average cosine score between all adjacent sentences of the text, each of them being represented as a vector in a latent space (Foltz et al., 1998).\nTo render all these yardsticks more visual and more understandable, we project each of them on a \ufb01ve-degree scale, represented by colored feathers. The more feathers a yardstick gets, the more complex this linguistic dimension is supposed to be for reading. To transform the yardstick values into a \ufb01ve-degree scale, we applied the following method. Our corpus of 115 administrative texts has been annotated by experts on a \ufb01ve-degree dif\ufb01culty scale (Franc\u00b8ois et al., 2014). For each of our 11 yardsticks, we then estimated its Gaussian distribution (mean and standard deviation) on the corpus for each of the \ufb01ve levels. At running time, we simply compute the probability of the yardstick score for a given text to be generated by each of these \ufb01ve Gaussians and assign it the level corresponding to the higher probability.\nThe third type of diagnosis allows to directly visualize the text in which all complex phenomena annotated during the analysis step (see Section 3.1)\n\nFigure 1: Result of a text analysis in AMesure.\nare underlined, namely the three types of subordinated clauses, passives, parentheticals, rare words, abbreviations, and technical terms. For each of these categories, AMesure allows the user to select a tab showing only the respective phenomenon. It also offers a global view of the text in which complex sentences are highlighted in various shades of yellow (see letter C in Figure 1): the darker the yellow, the more dif\ufb01cult the sentence is to read.\nFinally, the last functionality offers writing advice related to the complex phenomena detected (letter D in Figure 1). Two forms of advice are provided. On the one hand, we apply a list of 7 rules to \ufb01lter out syntactic structures detected during the NLP analysis that should not be considered as complex. For instance, in\ufb01nitive, participial, or even object clauses can be very short (e.g. quand on de\u00b4cide d\u2019avoir un be\u00b4be\u00b4 or le logement qu\u2019il occupe) and are therefore not at all a burden for reading. The \ufb01ltering rules were de\ufb01ned based on writing guidelines from three plain language guides for French (Gouvernement du Que\u00b4bec, 2006; Ministe`re de la Communaute\u00b4 franc\u00b8aise de Belgique, 2010; European Union, 2011). We also extracted\n\n4\n\nfrom these guides some pieces of advice that are shown to users of the platform when a dif\ufb01cult syntactic phenomenon is detected. Examples of advice are: \u201cThis sentence has 50 words. Please avoid sentences longer than 15 words\u201d or \u201cThis sentence has three subordinate clauses. Please avoid having so many subordinate clauses in a sentence\u201d. On the other hand, we also offer simpler synonyms for words detected as rare words or technical words. The synonyms are taken from ReSyf (Billami et al., 2018), a lexical resource in which synonyms are ranked by dif\ufb01culty. For now, we show the three simpler synonyms found in ReSyf for a given dif\ufb01cult word, letting the user to pick the best one. More advanced methods based on embeddings are, however, considered at the moment to improve the automatic selection.\n4 Evaluation of the system\nTo assess the performance of the various extraction algorithms included in our platform, three linguists manually annotated, in 24 administrative texts, the following \ufb01ve phenomena: passive structures, relative clauses, object clauses, adverbial clauses, and abbreviations6. The work of annotators was supported by guidelines focusing on dif\ufb01cult cases7. At the end of the annotation process, the expert agreement was evaluated using Fleiss\u2019 kappa (see Table 1). The agreement was high for the rather easy tasks of annotating abbreviations and passive clauses. Detecting subordinate clauses is, however, a much more complex task, if only because it is also necessary to identify the type of structures. A common reference version was then built through consensus-building.\nThis gold-standard version of the annotation was manually compared to the output of AMesure for the 24 texts in the test set. Table 1 reports the results of this evaluation in terms of recall, precision, and F1-score for the different types of structures. Performance for the detection of passive clauses, relative clauses, adverbial clauses and abbreviations are satisfactory (F1 is above .8). By comparison, Zilio et al. (2017), who detect syntactic structures in English, obtained a precision of 0.88\n6The detection of rare words and complex technical terms could not be assessed according to the same protocol as what matters is the psychological relevance of their identi\ufb01cation for a given audience of readers. Further experiments with subjects are required to assess these two dimensions.\n7For instance, in\ufb01nitive clauses led by a semi-modal auxiliary such as devoir (ought to) or pouvoir (can) were discussed, as contradictory points of view can be found in grammars.\n\nPhenomena\n\nR P F1 \u03ba\n\nPassive clauses\n\n0.92 0.92 0.92 0.92\n\nSubordinated clauses (all) 0.84 0.87 0.85 0.47\n\nRelative clauses\n\n0.83 0.88 0.86 /\n\nObject clauses\n\n0.56 0.42 0.48 /\n\nAdverbial clauses\n\n0.78 0.83 0.8 /\n\nAbbreviations\n\n0.73 0.9 0.8 0.97\n\nTotal (macro-average)\n\n0.83 0.9 0.86 0.79\n\nTable 1: Recall (R), precision (P), F1, percentage of agreement and Fleiss\u2019 \u03ba scores for the \ufb01ve phenomena detected in the platform.\n\nand a recall of 0.62 for the relative clauses and a recall of 0.66 and a precision of 0.94 for in\ufb01nitive clauses introduced by the particle \u201dTO\u201d. Chinkina and Meurers (2016) reached a recall of 0.83 and a precision of 0.71 for relative clauses. However, our system has trouble detecting object clauses, which have a F1-score of only 0.48. Investigation of the confusion matrix reveals that 77% of object clauses (37 out of 48) are correctly detected by AMesure, but 17 out of 37 are wrongly classi\ufb01ed as adverbial clauses. This is a limited issue, as advice can still be provided even if the system gets the type of clause wrong.\n5 Conclusion\nWe have presented the AMesure system, which automatically analyzes the readability of French administrative texts based on classic readability metrics, but also on guidelines from plain language books. The system is freely available through a web platform and is aimed to help writers of administrative texts to produce more accessible documents and forms. To that purpose, it offers a global readability score for the texts, 11 readability yardsticks, a detailed diagnosis in which dif\ufb01cult linguistic words and syntactic structures are highlighted, and some plain language advice. We also carried out a manual evaluation of the system based on 24 administrative texts annotated by linguists. Performance is satisfactory, except as regards the identi\ufb01cation of object clauses. More work is needed on this category, especially to distinguish it from adverbial clauses. We also plan to improve the system providing simpler synonyms by adding a semantic \ufb01lter based on embedding models. Finally, we plan to conduct a study with real writers of administrative texts to measure the perceived usefulness of AMesure as a whole, but also the usefulness of each functionality.\n\n5\n\nAcknowledgments\nWe would like to thank the \u201dDirection de la language franc\u00b8aise\u201d from the Federation WalloniaBrussels for its continued support to the AMesure project since 2014. We also want to acknowledge the wonderful work of Romain Pattyn, Gae\u00a8tan Ansotte, Baptiste Degryse on the interface and the great visuals of Brian Delme\u00b4e.\nReferences\nM. Adler. 2012. The plain language movement. In The Oxford handbook of language and law.\nR. Alarcon, L. Moreno, I. Segura-Bedmar, and P. Martinez. 2019. Lexical simpli\ufb01cation approach using easy-to-read resources. Procesamiento de Lenguaje Natural, 63:95\u2013102.\nM. Billami, T. Franc\u00b8ois, and N. Gala. 2018. ReSyf: A French lexicon with ranked synonyms. In Proceedings of COLING 2018.\nS. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012. Can Spanish Be Simpler? LexSiS: Lexical Simpli\ufb01cation for Spanish. pages 357\u2013374.\nM. Candito, J. Nivre, P. Denis, and E. H. Anguiano. 2010. Benchmarking of statistical dependency parsers for French. In Proceedings of COLING 2010, pages 108\u2013116.\nR. Catherine. 1968. Le style administratif. A. Michel.\nR. Chandrasekar, C. Doran, and B. Srinivas. 1996. Motivations and methods for text simpli\ufb01cation. In Proceedings of the 16th conference on Computational Linguistics, volume 2, pages 1041\u20131044.\nM. Chinkina and D. Meurers. 2016. Linguistically aware information retrieval: Providing input enrichment for second language learners. In Proceedings of BEA 2016, pages 188\u2013198.\nT.M. Chung. 2003. A corpus comparison approach for terminology extraction. Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication, 9(2):221\u2013246.\nW. Coster and D. Kauchak. 2011. Simple English Wikipedia: A new text simpli\ufb01cation task. In Proceedings of ACL 2011, pages 665\u2013669. Association for Computational Linguistics.\nF. Cusin-Berche. 2003. Les mots et leurs contextes. Presses Sorbonne nouvelle.\n\nF. Daoust, L. Laroche, and L. Ouellet. 1996. SATOCALIBRAGE: Pre\u00b4sentation d\u2019un outil d\u2019assistance au choix et a` la re\u00b4daction de textes pour l\u2019enseignement. Revue que\u00b4be\u00b4coise de linguistique, 25(1):205\u2013234.\nJ. De Belder and M.-F. Moens. 2010. Text simpli\ufb01cation for children. In Proceedings of the SIGIR workshop on accessible search systems, pages 19\u201326.\nP. Denis and B. Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art POS tagging. Language resources and evaluation, 46(4):721\u2013736.\nK. Desbiens. 2008. Les obstacles a` la simpli\ufb01cation: le cas des membres du centre d\u2019expertise des grands organismes. Langue, me\u00b4diation et ef\ufb01cacite\u00b4 communicationnelle. Que\u00b4bec.\nEuropean Union. 2011. How to write clearly . Publications Of\ufb01ce of the EU, Luxembourg.\nJ. Falkenjack, E. Rennes, D. Fahlborg, V. Johansson, and A. Jo\u00a8nsson. 2017. Services for text simpli\ufb01cation and analysis. In Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 309\u2013313.\nP.W. Foltz, W. Kintsch, and T.K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse processes, 25(2):285\u2013307.\nT. Franc\u00b8ois. 2011. Les apports du traitement automatique du langage a` la lisibilite\u00b4 du franc\u00b8ais langue e\u00b4trange`re. Ph.D. thesis, Universite\u00b4 Catholique de Louvain. Thesis Supervisors : Ce\u00b4drick Fairon and Anne Catherine Simon.\nT. Franc\u00b8ois, L. Brouwers, H. Naets, and C. Fairon. 2014. AMesure: une formule de lisibilite\u00b4 pour les textes administratifs. In Proceedings of TALN 2014.\nT. Franc\u00b8ois, A. Mu\u00a8ller, B. Degryse, and C. Fairon. 2018. Amesure : une plateforme web d\u2019assistance a` la re\u00b4daction simple de textes administratifs. Repe`res DoRiF, 16(1).\nG. Glavas\u02c7 and S. S\u02c7 tajner. 2015. Simplifying lexical simpli\ufb01cation: Do we need simpli\ufb01ed corpora? In Proceedings of ACL-IJCNLP 2015, pages 63\u201368.\nG. Gougenheim, R. Miche\u00b4a, P. Rivenc, and A. Sauvageot. 1964. L\u2019e\u00b4laboration du franc\u00b8ais fondamental (1er degre\u00b4). Didier, Paris.\nGouvernement du Que\u00b4bec. 2006. Re\u00b4diger simplement - Principes et recommandations pour une langue administrative de qualite\u00b4 . Bibliothe`ques et archives nationales du Que\u00b4bec, Que\u00b4bec.\nJ. Kimble. 1992. Plain english: A charter for clear writing. TM Cooley L. Rev., 9:1.\n\nM. Cutts. 2020. Oxford guide to plain English. Oxford J. Kimble. 1996. Writing for dollars, writing to please.\n\nUniversity Press, USA.\n\nScribes J. Leg. Writing, 6:1.\n\n6\n\nB. Labasse. 2001. L\u2019institution contre l\u2019auteur : pertinence et contraintes en re\u00b4daction professionnelle. In Congre`s annuel de l\u2019ACPRST/CATTW, Universite\u00b4 Laval, Que\u00b4bec.\nJ. Lee, W. Zhao, and W. Xie. 2016. A customizable editor for text simpli\ufb01cation. In Proceedings of COLING 2016: System Demonstrations, pages 93\u201397.\nR. Levy and G. Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of LREC 2006, pages 2231\u2013 2234.\nB. Madinier. 2009. Langage administratif et modernisation de l\u2019e\u00b4tat : pour une communication re\u00b4ussie. In La communication avec le citoyen : ef\ufb01cace et accessible ? Actes du colloque de Lie`ge 2009, pages 55\u201366.\nMiniste`re de la Communaute\u00b4 franc\u00b8aise de Belgique. 2010. E\u00b4crire pour e\u02c6tre lu - Comment re\u00b4diger des textes administratifs faciles a` comprendre ? Ingber, Damar, Bruxelles.\nB. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007. The use of \ufb01lm subtitles to estimate word frequencies. Applied Psycholinguistics, 28(04):661\u2013677.\nS. Nisioi, S. S\u02c7 tajner, S. P. Ponzetto, and L.P. Dinu. 2017. Exploring neural text simpli\ufb01cation models. In Proceedings of ACL2017: Short Papers), pages 85\u201391.\nA. Nord. 2018. Plain language and professional writing : A research overview. Technical report, Language Council of Sweden.\nOECD. 2016. Skills Matter : Further Results from the Survey of Adult Skills. OECD Publishing, Paris.\nG. Paetzold and L. Specia. 2016. Unsupervised lexical simpli\ufb01cation for non-native speakers. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence.\nG. Paetzold and L. Specia. 2017. Lexical simpli\ufb01cation with neural ranking. In Proceedings of EACL2017: Short Papers, pages 34\u201340.\n\nM. Shardlow. 2014. A survey of automated text simpli\ufb01cation. International Journal of Advanced Computer Science and Applications, 4(1):58\u201370.\nA. Siddharthan. 2011. Text simpli\ufb01cation using typed dependencies: A comparison of the robustness of different generation strategies. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 2\u201311.\nA. Siddharthan. 2014. A survey of research on text simpli\ufb01cation. ITL-International Journal of Applied Linguistics, 165(2):259\u2013298.\nM. da Silva Conrado, A. Di Felippo, T.A.S. Pardo, and S.O. Rezende. 2014. A survey of automatic term extraction for brazilian portuguese. Journal of the Brazilian Computer Society, 20(1):12.\nS. Sinha, F. McDermott, G. Srinivas, and P.W.J. Houghton. 2011. Use of abbreviations by healthcare professionals: what is the way forward? Postgraduate medical journal, 87(1029):450\u2013452.\nL. Specia. 2010. Translating from Complex to Simpli\ufb01ed Sentences. In Proceedings of Propor 2010., pages 30\u201339.\nS.M. Yimam and C. Biemann. 2018. Demonstrating par4sem-a semantic writing aid with adaptive paraphrasing. In Proceedings of EMNLP 2018: System Demonstrations, pages 48\u201353.\nX. Zhang and M. Lapata. 2017. Sentence simpli\ufb01cation with deep reinforcement learning. In Proceedings of EMNLP 2017, pages 584\u2013594.\nZ. Zhu, D. Bernhard, and I. Gurevych. 2010. A monolingual tree-based translation model for sentence simpli\ufb01cation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1353\u20131361. Association for Computational Linguistics.\nL. Zilio, R. Wilkens, and C. Fairon. 2017. Using nlp for enhancing second language acquisition. In Proceedings of RANLP 2017, pages 839\u2013846.\n\nPlain Language Action and Information Network. 2011. Federal plain language guidelines.\n\nJ. Qiang, Y. Li, Y. Zhu, Y. Yuan, and X. Wu. 2020. Lexical simpli\ufb01cation with pretrained encoders. In Proceedings of AAAI 2020, pages 8649\u20138656.\n\nH. Saggion. 2017. Automatic text simpli\ufb01cation. Synthesis Lectures on Human Language Technologies, 10(1):1\u2013137.\n\nC. Scarton, M. Oliveira, A. Candido Jr, C. Gasperin, and S. Alu\u00b4\u0131sio. 2010. Simpli\ufb01ca: a tool for authoring simpli\ufb01ed texts in brazilian portuguese guided by readability assessments. In Proceedings of the NAACL HLT 2010: Demonstration Session, pages 41\u201344.\n\n7\n\n",
        "hash_id": "7a436c340a130c951bf0b19a0b7ed49a"
    },
    {
        "key": "4KJ2DHMY",
        "version": 52,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/4KJ2DHMY",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/4KJ2DHMY",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/INQFS9PE",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 593576
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Fran\u00e7ois et al.",
            "parsedDate": "2020-12",
            "numChildren": 1
        },
        "citation": "<span>Thomas Fran\u00e7ois et al., <i>AMesure: A Web Platform to Assist the Clear Writing of Administrative Texts</i>, <i>in</i> <span style=\"font-variant:small-caps;\">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</span> 1\u20137 (2020), https://aclanthology.org/2020.aacl-demo.1 (last visited Feb 1, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "AJG9KHRK",
        "version": 50,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/AJG9KHRK",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/AJG9KHRK",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/8995VEUW",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/AJG9KHRK/file/view",
                "title": "Benjamin - 2012 - Reconstructing Readability Recent Developments an.pdf",
                "length": 313033
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://link.springer.com/content/pdf/10.1007%2Fs10648-011-9181-8.pdf (last visited Jan 20, 2023).</span>",
        "fulltext": "Educ Psychol Rev (2012) 24:63\u201388 DOI 10.1007/s10648-011-9181-8 REVIEW\nReconstructing Readability: Recent Developments and Recommendations in the Analysis of Text Difficulty\nRebekah George Benjamin\nPublished online: 4 October 2011 # Springer Science+Business Media, LLC 2011\nAbstract Largely due to technological advances, methods for analyzing readability have increased significantly in recent years. While past researchers designed hundreds of formulas to estimate the difficulty of texts for readers, controversy has surrounded their use for decades, with criticism stemming largely from their application in creating new texts as well as their utilization of surface-level indicators as proxies for complex cognitive processes that take place when reading a text. This review focuses on examining developments in the field of readability during the past two decades with the goal of informing both current and future research and providing recommendations for present use. The fields of education, linguistics, cognitive science, psychology, discourse processing, and computer science have all made recent strides in developing new methods for predicting the difficulty of texts for various populations. However, there is a need for further development of these methods if they are to become widely available.\nKeywords Readability . Text difficulty . Reading . Text analysis\nA century of reading research paralleled a century of research into what makes one text more or less difficult to read and comprehend than another. Some estimate that by the 1980s, over 200 readability formulas had already been developed (DuBay 2004), and since the 1980s, the area has exploded in fields like discourse processing and computer science. The question that remains in the minds of educators and researchers alike is \u201cwhat is the best way of determining the difficulty of a particular text?\u201d Several reviews and summaries are available of the older, more traditional methods of assessing readability (e.g., Bormuth 1966; DuBay 2004; Klare 1974), and most researchers in the field discuss these classic methods by way of introduction to their own research. Controversy, however, has surrounded these older formulas, and new methods are constantly being developed and tested. The purposes of this review were to (a) examine recent developments in the field of\nR. G. Benjamin (*) Department of Educational Psychology, University of Georgia, Athens, GA 30602, USA e-mail: bben81@gmail.com\n\n64\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nreadability, (b) provide direction for future research, and (c) recommend appropriate readability analysis tools for educational and research settings.\nThe standard test of a text difficulty analysis method is how well its prediction of readability matches up with the actual reading comprehension scores of readers using existing texts. This very practical criterion has a simple and logical face value validity to it. However, the common problem with classic readability formulas is that once a formula is determined to be a good general predictor of a text\u2019s difficulty level, educational publishers and writers may then use the formula as a tool in the writing process in order to \u201cscientifically\u201d create texts at specific levels. The classic study by Davison and Kantor (1982) clearly demonstrates the shortcomings of this practice when they conducted a case study of four texts. Attempting to modify the texts via readability formulas was unsuccessful, and when the team made successful changes to make the texts easier to read, these changes actually ran counter to what the formulas would suggest. As Bormuth (1966) suggested, then, readability formulas are useful for measuring the factors reflecting a text\u2019s readability. When adapting texts, it was more important that adaptors were paying attention to the discourse structures of sentences, paragraphs, and the whole text rather than simply trying to shorten words and sentences.\nOther critics have also complained that the practice of \u201cwriting to the formulas\u201d does more harm than good (Schriver 2000) and that the assumptions underlying traditional readability formulas are far too simplistic to account for the varied linguistic and textual factors that can differ widely, especially in adult texts (Bailin and Grafstein 2001; Schriver 2000). Bailin and Grafstein (2001) argue that there is no single method for determining the readability of a text, and factors like style, vocabulary, background knowledge, grammar, textual coherence, and how well a reader can \u201cfix\u201d errors in a text likely interact with one another in making a text more or less difficult for readers.\n\nThe Need for Description and Evaluation of Text Difficulty Methods\nRegardless of the problems with and criticisms surrounding traditional readability formulas, they are still popular today and continue to be used in reading research. However, in the past two decades, major technological advances have made way for the streamlining and automation of traditional readability formulas and have also sparked the development of more complex methods for measuring text difficulty. Researchers, administrators, and policy decision makers in the field of education, however, may need guidance on which methods might be most useful in research studies and in classrooms. Because of the rapid influx of newly designed procedures for analyzing text difficulty and matching readers and texts, educational researchers may either feel overwhelmed with the numerous options or may simply be tempted to stick with the classic methods they and their predecessors have used for decades. However, selecting appropriate texts for a population of readers requires some understanding of both the reader and the text, and different methods may be more or less appropriate for different types of texts and different populations of readers.\nAs research in the field of text difficulty analysis expands, researchers, administrators, publishers, and policy makers in education and other fields are increasingly responsible for defending their use or support of a particular readability tool. Likewise, so many developments have been made in the past two decades across disciplines that researchers in the text difficulty field need to guide their research with a wide variety of previous research in mind. A need, then, has arisen for both a description and evaluation of methods developed in the more classic style as well as emerging methods of assessing text difficulty and matching readers to texts. This need for exposure to and awareness of the rapidly expanding field of text difficulty analysis serves as the\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n65\n\nmotivation for this review. There are several goals for this paper. The first goal is to describe the development and evaluate the effectiveness of readability analysis methods which have emerged in the past two decades. This goal is met by describing the basic unifying elements within \u201ctypes\u201d of methods, pointing out their key differences, and identifying their strengths and weaknesses. The second goal builds upon the first: provide recommendations for practical use by evaluating each of these methods based on the following criteria:\n& Is the method intended for/tested on a particular population? & Is the method intended for/tested with particular texts? & Has the method demonstrated predictive validity\u2014especially in predicting readers\u2019\ncomprehension of texts? & How widely available for immediate use is the method? & How much special training is required to use the method?\n\nThe third and final goal is to consider these two decades of development in the field and make recommendations about what research needs to be done to advance the field of text difficulty analysis. For example, even though certain recent methods may not be ready now for wide-scale use, do the theoretical foundation for the method and preliminary tests of the method point to promising future applicability?\nThe following sections discuss the types of methods that have been developed in the past two decades and conclude by providing recommendations for using these methods in educational practice and research. The bulk of this paper describes and evaluates currently available tools and methods in readability research. These tools and methods are divided into three genres or types: (1) traditional methods, (2) methods inspired by cognitive science, and (3) methods based on the use of statistical language modeling tools. All methods discussed in this review are quantitative for the purpose of simplifying the comparison and evaluation of methods. The discussion of each readability genre concludes with some recommendations for future research directions. Following the discussion of methods developed in the past two decades is a section providing recommendations for current wide-scale use of existing readability methods. This final section is mainly intended for educators, administrators, and educational researchers.\n\nDevelopments in the past two decades\nFormulas based on traditional readability features\nAlthough they have been on the receiving end of much criticism (e.g., Davison and Kantor 1982), several traditional readability formulas have remained popular and largely valid over the 90 years that they have existed as measures of text difficulty (Klare 1974). In the past two decades, technological advances have allowed researchers to automate the application of formulas and to test new variables for developing new formulas. Online corpora have automated word frequency analyses, and even the most basic word processing software can instantly report sentence length and word length statistics. In fact, readability formulas that utilize traditional variables like sentence length, percentage of familiar words, and word length are still being developed and have remained popular over the past two decades.\nIn their essential components, all traditional methods for computing readability are similar. They tend to incorporate some combination of easily measured units like sentence length, word length, and word frequency. Passages that contain shorter sentences, shorter\n\n66\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nwords, and more frequent words would be considered more readable or less difficult than passages with longer sentences, longer words, and rare words. The validity of these formulas for various readers is typically established by correlating reading comprehension scores with the formula\u2019s predicted readability of the texts. Of course, this technique can only result in a rough estimation of difficulty, and its weakness is that the formula might judge even a nonsense passage as quite readable if the text\u2019s jumbled words are frequent, short, and organized into brief sentences (see Davison and Kantor 1982 for a criticism). However, because of their wide use, simplicity, popularity, and some modern creative adaptations of traditional features like word frequency, several of the more recently developed methods are discussed below. These methods include the New Dale\u2013Chall Readability Formula (Chall and Dale 1995), the Lexile framework (Smith et al. 1989), the Advantage-TASA Open Standard for Readability (ATOS) formula (School Renaissance Inst., Inc. 2000). A lesser known and less studied method is also briefly discussed to demonstrate the direction in which these traditional methods may be heading: the Read-X tool (Miltsakaki and Troutt 2007, 2008).\nRecent traditional-style methods\nThe new Dale\u2013Chall readability formula After publishing and observing the use of their popular Dale\u2013Chall Readability Formula (Dale and Chall 1948), Edgar Dale and Jeanne Chall began to work on revising their formula in the 1970s. During these years of development, controversy regarding the use of traditional readability formulas came to a head, and several researchers began focusing on measures of assessing text difficulty that were based on theories in cognitive science (e.g., Kintsch and van Dijk 1978). Cognizant of the need to provide solid evidence of the validity of the new formula, when Chall published the revised formula, she included some methods for users to assess cognitive\u2013structural elements of texts to better match readers to texts (Chall and Dale 1995). The new formula took sentence length and word familiarity into account, as the older formula did, but Dale and Chall improved upon the older version by updating and expanding their corpus of familiar words and validating the scale using cloze scores from the 32 passages used in Bormuth\u2019s (1971) extensive study of readability. Both the new and old formulas correlated highly with Bormuth\u2019s cloze mean scores (r=0.92 for both), and correlations with other criterion cloze scores were 0.85 or above (Caylor et al. 1973; Miller and Coleman 1967). Finally, the scale was also successfully validated by comparing predicted difficulty levels with various standardized reading tests. An additional improvement included expanding the range of grade levels that can be analyzed using the formula. While the old formula was recommended only for reading levels at grades 4 and above, the new formula can be used for assessing readability at grade 1 through college level.\nThe New Dale\u2013Chall Readability Formula provides two readability measures for a text: a cloze score (the lower the score the more difficult the text) and a grade level. The authors recommend using the cloze score for research purposes as it allows for finer distinctions between texts. While this formula is subject to criticism for its failure to account for more complex structural relations within a text (e.g., lexical overlap, coherence, etc.), it has also received praise for potentially being the most valid of the popular traditional readability formulas (DuBay 2004). Indeed, interpretations of readability based on this formula may be at least moderately valid in a psycholinguistic sense as word frequency is an established indicator of the comprehensibility of texts (Just and Carpenter 1980).\nThe Lexile framework While the New Dale\u2013Chall (1995) formula has gained appeal thanks to its user-friendly simplicity, another popular text-leveling method\u2014developed in the late 1980s\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n67\n\nbut still flourishing in schools\u2014is more complex in design: the Lexile scale (Smith et al. 1989). Creators of the Lexile scale attempted to design a scale that matched their construct definition of reading comprehension, thereby creating a measure with a high degree of construct validity in addition to the predictive validity sought by many traditional scale developers. Determining that reading comprehension depends upon \u201c\u2026the familiarity of the semantic units and\u2026the complexity of the syntactic structures used in constructing the message\u201d (Smith et al. 1989, p. 6), the creators devised a scale that included measures of word frequency (the semantic variable) and sentence length (a proxy for syntactic complexity). At this level, the scale is quite similar to the New Dale\u2013Chall Readability Formula (Chall and Dale 1995). But the actual measures taken and the application of the Lexile scale differ from prior methods and cannot easily be computed manually as the Dale\u2013Chall method can. The Lexile word frequency measure is the mean log word frequency from a 5 million word corpus (Carroll et al. 1971), and their sentence length measure is the log of the mean sentence length in the text.\nCreators found that the Lexile scale is limited to use with continuous prose (as are most formulas), but correlated highly with item difficulties of numerous reading comprehension tests (r=0.84, 0.93 when corrected for range restriction and measurement error) [Smith et al. 1989]. The scale ratings also correlated highly with rank order difficulties of texts from 11 different basal series for grades 1\u20138 (r=0.83, 0.99 when adjusted for range restriction and measurement error) [Smith et al. 1989]. When compared with nine other readability formulas, however, the Lexile scale did not differ significantly in performance. The appeal of the scale for wide use seems to be largely based on its application: a person receives a Lexile score based on his or her ability to answer comprehension questions correctly; a text also receives a Lexile score. If the person and the text are matched, then the person has a 75% chance of answering a comprehension item correctly for that text. Teachers, then, can look at the Lexile score for a text and determine whether or not that text would be appropriate for a student based on the student\u2019s Lexile score.\nIn recent years, researchers\u2014often the original developers of the scale\u2014have written numerous defenses of the scale in addition to instructions for its use (Blackburn 2000; Stenner 1996, 1999; Stenner and Burdick 1997; Wright and Stenner 1998, 2000; Smith 2000a, b). The Lexile Framework for Reading is a commercial venture owned by Metametrics, Inc. Because the formula is complex and is performed on entire texts rather than samples, individual teachers and researchers are not able to perform analyses on particular texts without the assistance of the company. The regression equation and detailed explanations of the logit system and Lexile difficulty scale equation are available in the original NIH report (Smith et al. 1989) as well as a more recent MetaMetrics, Inc. publication (Stenner et al. 2007). A large body of literature, test items, and educational materials has been analyzed for Lexile scores, and the company is continually updating its library.\nAdvantage-TASA open standard for readability Like Metametrics, Inc., two large companies sought to develop a readability formula that would be widely applicable and used in schools nationwide. In 1998, researchers began work developing the ATOS formula (School Renaissance Inst., Inc. 2000). Renaissance Learning, Inc. (formerly Advantage Learning Systems, Inc., developers of the Accelerated Reader software) and Touchstone Applied Science Associates, Inc. (TASA, developers of the Degrees of Reading Power program) used their massive book and reading assessment databases to create two formulas: the ATOS for Text Readability Formula and the ATOS for Books Readability Formula. Both formulas are based on the same traditional variables\u2014word length, sentence length, and grade level of words\u2014though the formula for books also takes book length into account, a factor that was found to significantly influence book difficulty.\n\n68\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nThe development of the ATOS method involved comparing the predictive performance of numerous variables with the reading achievement scores of over 3 million students using the Accelerated Reader and STAR Reading programs\u2014programs developed by Renaissance Learning, Inc. Final development and refinement of the formula involved examining the predictive value of numerous text variables when compared with the Rasch difficulty values of several hundred reading comprehension items. The final ATOS for Text Readability Formula incorporated three variables: average characters per word, average words per complete sentence, and the average grade level of the words which could be found in the ATOS graded vocabulary list (excluding the 100 most frequent words in the corpora). Validation of the formula was conducted using several hundred test items as well as authentic texts that had been used to validate the New Dale\u2013Chall formula (Chall and Dale 1995). To convert the formula values to grade-level equivalents, researchers examined the performance and reading lists of thousands of student who had participated in the Accelerated Reader program and used these data to develop a grade equivalency conversion formula. An additional formula was also developed for use with books as ATOS researchers found that when other factors were controlled, students scored lower on Accelerated Reader quizzes for longer books than for shorter books. Development of the ATOS for Books Readability Formula simply involved adding a weighted book length variable that is adjusted based on general book length categories, e.g., whether the book has fewer than 500 words, more than 500 words, more than 5,000 words, more than 50,000 words, etc.\nWhile the variables in the ATOS for Text formula individually explained over 80% of the variance in text difficulty during the development phase of the formula (Milone 2009; School Renaissance Inst., Inc. 2000), the study reports do not provide evidence of the formula\u2019s performance as a whole. This, of course, makes it difficult to determine the predictive validity of the formula\u2014a significant weakness in this method. Because of the unknown or publicly inaccessible predictive value of the formula as a whole, independent studies are still needed to test the effectiveness of the ATOS formulas. The method\u2019s strengths lie in its extensive development phase and the large databank of student reading performance that the company was able to use in its development. The comprehensive research conducted in the development of ATOS, in addition to its wide applicability\u2014the formulas are appropriate for grades K\u201312, adjustments are available for non-fiction texts, and conversion scales have been developed for Reading Recovery levels, Degrees of Reading Power, and Lexiles\u2014lends to its large-scale use in educational settings.\nRead-X A promising new text\u2013user matching tool called Read-X is in development (Miltsakaki and Troutt 2007, 2008). This software uses some traditional readability variables\u2014number of sentences, number of words, number of \u201clong words,\u201d and number of letters in the text\u2014to analyze the readability of texts on the Web in real time so that a person can perform a web search and filter results by reading level. The uniqueness of this program lies specifically in its ability to categorize search results by theme (e.g., science, music, history, etc.), and future versions of Read-X should be able to take a user\u2019s existing topical knowledge into account and customize reading level filtering based on a reader\u2019s level of content knowledge about a particular topic (e.g., long words in a particular domain are not necessarily difficult for a reader with a lot of background knowledge in that domain). This customization is possible because the authors developed theme-based corpora and gathered word frequencies based on theme (e.g., atom and cell both occur frequently in science texts, but are not frequent in general). Thus, the program is similar to the methods discussed above in its use of formulas to determine the readability of texts, but it differs in that it customizes the formulas for various domains of information.\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n69\n\nAdditionally, its future versions should do much more to take user knowledge into account than a more basic traditional method could ever do.\nThis program is promising for its potential use in school computer labs and libraries, where students often need to find information about a particular topic but may be overwhelmed by the difficulty of texts that result from their search. Future empirical research will determine the predictive validity of Read-X and its usability in educational settings. Meanwhile, developers are planning to examine whether psycholinguistic and discourse processing factors like syntactic complexity, propositional density, and rhetorical structure may improve the program\u2019s readability analysis (Miltsakaki and Troutt 2008).\nEvaluation and directions for research\nThe four methods discussed in this section are similar in their use of traditional text difficulty analysis features like word frequency and length of sentences, words, and paragraphs. While available data regarding the performance of these methods can only be found for the New Dale\u2013 Chall and Lexile methods, it is likely that ATOS performs similarly since similar variables and methods of development were used for all three methods. These three methods have been designed to determine the readability of books and articles. The widespread use of readability formulas in business, research, and education\u2014in addition to the large-scale commercial endeavors\u2014demonstrates the continued relevance of these methods for typical use in analyzing general connected text. These methods are simple to use and have been around long enough that there are many user-friendly software programs available to analyze texts\u2019 readability. However, the original criticisms of these methods still hold true: (1) a jumbled passage would be judged just as readable as a sensible passage containing the same words and sentence lengths and (2) the temptation to \u201cwrite to the formula\u201d in creating leveled texts is likely too strong for most publishers to resist. Thus, these methods can still be used, but must be used only with appropriate texts (usually defined as authentic books or articles containing at least 300 words). Their use, then, is limited.\nThe Read-X method addresses the increasing need for analyzing the readability of web texts. While it is not yet sophisticated enough to take web images, captions, and nonstandard text (words or phrases that can be used as \u201cInternet shorthand\u201d), it does move in the direction of automating user- and text-specific text difficulty analysis. Due to the increasingly widespread use of social media, Internet news sites, and interactive educational web sites, methods that can instantly determine the readability of web pages in a way that takes the user\u2019s knowledge as well as the page\u2019s non-traditional content into account will likely be highly useful. However, while traditional variables might be used to accomplish this, it is doubtful that a single traditional \u201cformula\u201d would be successful since variables like word frequency would have to draw from numerous domain-based corpora.\nMethods inspired by advances in cognitive theory\nAs connectionist (McClelland and Rumelhart 1981; Rumelhart and McClelland 1982), schema (e.g., Anderson and Pichert 1978), prototype (Rosch et al. 1976), and spreading activation (Anderson 1983) theories emerged to explain how humans store and retrieve information in long-term memory, some researchers who studied text processing began to hypothesize that text difficulty and readability were more related to coherence and the relationships between elements in a text rather than simply the sum or averages of individual surface features (Britton and G\u00fclg\u00f6z 1991; Kintsch 1988; McNamara and Kintsch 1996). Walter Kintsch\u2019s work with proposition density (Kintsch and Keenan 1973),\n\n70\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nas well as his construction\u2013integration and computational models of comprehension (Kintsch 1988; Kintsch and van Dijk 1978), inspired much of the recent work in text difficulty and matching readers to appropriate texts. This is especially the case since computer software like Coh-Metrix (Graesser et al. 2004) and WordNet (Fellbaum 1998) have made such analyses easily accessible to researchers.\nSince many researchers sought to analyze text difficulty based on cognitive theories, several new methods and variables were developed. Because of the complexity involved in trying to represent cognitive processes involved in reading text, these methods often require automation. Before discussing the empirical work conducted to test these cognitively inspired tools, a brief introduction is provided to define some terms and describe some of the tools used in this work.\nVariables and tools used in cognitively inspired readability methods\nPropositions and inferences Kintsch and van Dijk (1978) set the stage for incorporating analysis of propositions and inferences into text difficulty analysis through their theoretical framework for text comprehension. Simply put, sentences can be broken down into propositions, or brief meaningful units that do not take into account information like tense, voice, or aspect (Graesser et al. 1997). Propositions are units comprised a predicate and at least one argument, and propositions can also include other propositions. An argument serves a functional purpose within a proposition, indicating the relationships between meaningful words in the sentence. For example, in the sentence The nurse placed the scalpel on the table and grabbed the sponge, the propositional breakdown is as follows:\n\n1. Place (AGENT = nurse; OBJECT = scalpel; LOCATION = on table) 2. Grab (AGENT = nurse; OBJECT = sponge) 3. And (PROP 1; PROP 2)\nIn order to carry on coherent discourse or write a coherent text, there must be some propositional or at least argument overlap among successive sentences. Likewise, at the macro level, there must be some propositional connections across the larger text or throughout a conversation if the text or conversation is supposed to address a particular topic.\nWhen there are few or no gaps in overlap across sentences, then a text is seamlessly moving from one point of information to another while giving the reader all the help he or she needs to build new knowledge. This type of text is a highly cohesive text: inferences are explicit and the reader does not have to fill many gaps using his or her own knowledge about the topic. However, in a text where less propositional overlap exists, the reader will be required to fill gaps of information with his or her own knowledge. This type of text has low cohesion: inferences are implicit and require more work on the part of a novice reader. Kintsch and van Dijk (1978) describe these latter texts as more difficult texts because a novice reader may not have the schema in place to make the necessary inferences to comprehend the text. Thus, in cognitively oriented text difficulty analysis, propositions and inferences play an important role, and the analysis and manipulation of these variables can yield significant differences in reader comprehension (Britton and G\u00fclg\u00f6z 1991; Britton et al. 1993).\nLatent semantic analysis Latent semantic analysis (LSA; defined and described in Landauer et al. 1998) is an automated tool that represents text content (e.g., an individual word and all the contexts in which it appears, for example) as a vector in semantic space. LSA has been used for many purposes including analyzing interview data (Dam and Kaufmann 2008), assessing\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n71\n\nreading comprehension (see Millis et al. 2007), and scoring essays (Miller 2003). LSA analyzes the semantic relatedness either between texts or among segments of text in a more expanded way than simple measures of word overlap. The researchers can train the system on a large corpus of topical text so that it begins to develop a \u201cknowledge\u201d of which words tend to appear in particular contexts. For example, the word music probably appears frequently in the same contexts as guitar. Thus, music and guitar would have a strong semantic relationship and music would be considered to be an important word in texts containing the word guitar. This is a simplification of the numerous connections that LSA makes between words and contexts, but it describes the general principle.\nNot only are these direct relationships analyzed, but LSA examines the indirect relationships among words in contexts as well. For example, in a text about trumpets where the word music exists but guitar does not exist, the word guitar will still have a positive semantic relationship to the word trumpet even though they may have never appeared in the same context. This happens because through guitar\u2019s relationship to music and other words, the system understands that guitar is indirectly semantically related to trumpet. Mathematically, this is accomplished by using matrices to capture a word and its relatedness to all the other words in a text, paragraph, or sentence. Each word is a vector in multidimensional semantic space with rows in the vector being the contexts (lines, paragraphs, etc.) in which the word appears. These vectors are linked by words appearing in proximity to other words. The cosine between two vectors provides a numerical value of the relationship between two vectors.\nComplete texts can be represented as a vector as well, with the text vector being the average of the vectors of the words within the text. The system, then, can get an idea of which words are most likely to appear in which contexts, and it can determine the importance of particular words in particular contexts based on the strength of the relationships among words via the training corpus. Because an entire text, sentence, or paragraph can be represented as a vector as well, the relatedness of these various units can be compared. A more technical explanation of how this all works is too involved for this review, but LSA is discussed in greater detail by Landauer et al. (1998) and Foltz et al. (1998).\nAs a readability tool, LSA has been used to match readers to appropriately difficult texts (Wolfe et al. 1998) as well as to gauge the cohesiveness of a given text (Foltz et al. 1998). To match readers to texts, LSA can be used to compare the semantic relatedness of several texts about global warming, for example, with a student\u2019s composition about global warming. The belief is that the student is most likely to learn more from a text that more closely relates to the student\u2019s prior knowledge. LSA provides a measure of this relationship by creating vectors for each text, allowing researchers to compare cosine between text A and the student composition with the cosine between text B and the student composition. The higher the cosine, the more closely related the texts.\nTo estimate the difficulty of a text, LSA can report on the cohesiveness of the text: LSA is used to compare the semantic relatedness of adjoining sentences, for example, to determine how closely they connect. Texts in which there is a high degree of cohesion tend to be easier for non-expert readers to read than texts in which more connections have to be made by the reader (McNamara and Kintsch 1996; McNamara et al. 1996).\nEmpirical studies of cognitively based text difficulty analysis\nThe following studies illustrate the use of cognitively based variables and tools for text difficulty analysis. They are discussed in a roughly historical sequence, beginning with studies examining propositions, inferences, and text cohesion. Following these studies\n\n72\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nwhich are based largely on early work by Kintsch and colleagues, studies using a program called Coh-Metrix (Graesser et al. 2004) are examined. Coh-Metrix can conduct LSA as well as analyze text cohesion and dozens of other variables. As an alternative to methods based largely on Kintsch\u2019s text comprehension model, some researchers have recently used prototype theory (Rosch et al. 1976) as the basis for a study incorporating semantic networks. Finally, a new software program called DeLite has been developed by German researchers and serves as a kind of bridge to the statistical language modeling methods that are discussed later in this paper. This section concludes with some guidance for future research in cognitively based text difficulty analysis.\nRevising texts for better comprehension using inference analysis Using principles developed based on Kintsch\u2019s model of text comprehension (Kintsch and van Dijk 1978), Britton and Gulgoz (1991) developed principles for revising texts at a local level to improve comprehension. They took a text that was used to train Air Force recruits and used Kintsch\u2019s computer program (Miller and Kintsch 1980) to find places in the text where inferences were lacking. Having devised modification principles based on Kintsch\u2019s theory, they modified the text by linking each sentence to the previous sentence via overlapping propositions and arguments using only one term for each concept that appeared in the text, arranging sentences so that old information precedes new information, and making important implicit inferences explicit for the reader. The authors found that participants performed better on free recall tasks and multiple-choice inference questions when given the revised version rather than the original version of the text even though traditional readability statistics (e.g., Flesch\u2013Kincaid, Coleman\u2013Liau, and Automated Readability Index) between the passages were the same. Additionally, when the novices\u2019 ratings of relationships among terms were compared to experts\u2019 ratings (experts had read only the original text), ratings of novices who had read the revised text correlated much higher with the experts than ratings of novices who had read the original text.\nBritton et al. (1993) later found similar results when they conducted a review of studies in which textbooks had been revised according to similar principles, providing a promising contrast to studies in which revisions made according to readability formulas had little effect (e.g., Coleman 1962; Klare 1963). These studies by Britton and colleagues demonstrate that even if readability formulas are not able to discern differences between texts, analyses of explicit inferences within a text can show that one text is more comprehensible\u2014at least for novices\u2014than another.\nText cohesion and reader knowledge The important distinction between high-knowledge and low-knowledge (or novice) readers is highlighted in two studies (McNamara and Kintsch 1996; McNamara et al. 1996) in which low-knowledge and high-knowledge participants were given either a low-cohesion or high-cohesion text to read about a given informational topic. The difference between the two types of texts lies largely in the number and type of inferences a reader has to make in order to form a coherent mental representation of the content. A low-cohesion text requires a reader to make more inferences, while a high-cohesion text tends to provide more information explicitly for the reader. Both studies found that in general, low-knowledge readers benefited from highcohesion texts while high-knowledge readers benefited from low-cohesion texts.\nThe explanation for this result is based on Kintsch\u2019s construction\u2013integration model of reading comprehension (Kintsch 1988); high-knowledge readers need some obstacles placed in their path to promote deeper-level processing. Low cohesion requires these knowledgeable readers to make inferences which activate and strengthen semantic\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n73\n\nnetworks. However, low-knowledge readers need the obstacles removed because they do not have the prior knowledge necessary to construct appropriate models of text content, i.e., if inferences are not made for them, these readers will often fail to make the inferences necessary to fully comprehend a text. This high/low-cohesion distinction is similar to recent findings in cognitive load theory, demonstrating that certain efforts to decrease cognitive load for learners can be beneficial for novices but detrimental for experts (e.g., Leahy et al. 2003; Kalyuga et al. 2003, 2000, 2001a, b).\nUse of Coh-Metrix in text difficulty analysis As the studies above have contributed to the understanding of what factors might make texts differentially difficult for readers of a similar general reading skill, the development of various software programs have allowed researchers to develop new methods for determining the difficulty of texts based on traditional readability features as well as features of cohesion that indicate a text\u2019s level of coherence. Note that while coherence refers to how propositions are connected in a reader\u2019s mental representation (a psychological construct) and cannot be measured using computational surface indicators, cohesion refers to surface indicators of how sentences are related to one another in a text (a text construct). This is accomplished through examining propositional or argument overlap and can be measured using software like Coh-Metrix (Graesser et al. 2004), which reports over 50 indices of language, cohesion, and text difficulty.\nCoh-Metrix also conducts LSA. As the above-described studies (McNamara and Kintsch 1996; McNamara et al. 1996) point to the importance of gauging a reader\u2019s level of background knowledge when selecting texts for learning, Wolfe et al. (1998) attempted to use LSA to match students to texts of appropriate difficulty. They had undergraduates and medical students write brief essays about the human heart and then randomly assigned each of the participants to one of four texts about the human heart: one text designed for children, one general text for adults, one from an undergraduate textbook, and one for medical students. Participants\u2019 knowledge was tested after studying the assigned texts. After analyzing the results, the researchers predicted that had LSA been used to match readers to texts based on a comparison of the pre-essays to the texts, learning would have improved 53%. Unfortunately, though, they did not go on to test this prediction in an additional experiment.\nFoltz et al. (1998) used LSA to determine the coherence of original and revised texts from two studies (Britton and G\u00fclg\u00f6z 1991; McNamara et al. 1996). They found that LSA accurately predicted distinctions between texts and accurately predicted comprehension outcomes. Furthermore, LSA performed significantly better than simple measures of word overlap. LSA has been used for many other purposes (see volume 25 of Discourse Processes, issue 2/3\u2014dedicated to research regarding LSA) which fall outside the scope of this article, but it shows promise as a method for assessing the readability of texts especially in relationship with reader background knowledge.\nOther linguistic features utilized by Coh-Metrix have also been used to develop measures of text difficulty for predicting comprehension for both L1 and L2 (discussed below) English speakers. Three Coh-Metrix variables\u2014number of words per sentence, argument overlap, and CELEX word frequency scores\u2014were combined to develop a readability index (Crossley et al. 2007). Using Bormuth\u2019s (1971) 32 academic texts as their text corpus and Bormuth\u2019s mean cloze scores as the dependent variable, the three variables predicted cloze scores with an adjusted R2 of 0.90. However, employing Bormuth\u2019s (1969) formula which utilizes number of letters per word, number of Dale\u2013Chall words per total words (authors used the updated list), and number of words per sentence, the authors achieved an adjusted R2 of 0.92. While the more sophisticated Coh-Metrix variables did not perform any better than the traditional variables, it is critical to note that Bormuth\u2019s corpus\n\n74\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n(1971) is not necessarily representative of all texts. Because Coh-Metrix measures variables that are more in line with theories about how humans process text, further studies with a greater variety of texts will likely favor the newer methods. Crossley et al. (2007) point out that if a determination about superior methods is to be made, then studies using larger corpora are necessary.\nMore recent studies using Coh-Metrix variables focus on using the tool for making writing quality distinctions (McNamara et al. 2010) and distinguishing between the relative cohesion of texts (McNamara et al. 2010). While these functions are important and seem to be areas in which Coh-Metrix stands out as a useful tool, these functions are not necessarily precise measures of readability, and so in-depth discussion of these studies is beyond the scope of this review.\nSemantic networks While the studies above examine text difficulty largely in light propositional networks, Lin et al. (2009) believed that text difficulty would be a function of semantic networks\u2014lexical relationships such as those described in WordNet software (Fellbaum 1998)\u2014in accordance with prototype theory (Rosch et al. 1976). In light of the role that word length often plays as a measure of lexical complexity for determining text difficulty, Lin and colleagues tested the hypothesis that relative word length is what really matters (i.e., the basic level of a noun is typically the shortest version of that noun, e.g., red). Hypernymns (words that subsume the basic level noun in a semantic network, e.g., color) tend to be longer, while hyponyms (words that are subsumed by the basic level noun, e.g., crimson) are virtually always longer than their basic-level noun. Basic-level words also tend to be less morphologically complex. Thus, the authors propose that absolute word length is not really what matters when determining lexical complexity in a text; relative word length (the length of a noun as compared with the length of its basic noun form) is a more precise determiner. However, Lin et al. (2009) also found that sometimes hyponyms can also function cognitively as basic-level words (e.g., card is a hyponym of paper, but is often used to form compounds and tends to behave as a basic level noun).\nLin et al. (2009) determined to find basic-level words in a text via two filter conditions: compound ratios and word length differences between the target word and all hyponyms. Using the ratio of basic-level nouns in texts as their index, the authors compared their text difficulty levels to other readability measures\u2014using online graded readings as the texts\u2014 and found that their measure ordinally matched the grade-level progression more accurately than the traditional readability formulas. The readability formulas used included the New Dale\u2013Chall formula, Spache, Powers\u2013Sumner\u2013Kearl, Flesch\u2013Kincaid, FOG, SMOG, and FORECAST. These formulas tended to indicate a dip in difficulty for the grade 7\u20138 text, meaning that the grade 4\u20136 text was described by the formulas as more difficult than the grade 7\u20138 text. The method of Lin et al. (2009) did not result in such an error.\nA weakness in this study lies in its use of online graded texts rather than texts matched with students\u2019 comprehension scores; the authors never describe the methods used to grade these texts, so it is impossible to know whether they were \u201cwritten to a formula\u201d or authentic. Also, even though this method can be fully automated and, therefore, simple to use, it needs to be tested on a larger corpus of texts and text types for a more complete validation, something which the authors are, no doubt, considering. Because this method relies on psychological theory-based variables, it is likely that studies using larger corpora will further validate this technique.\nDeLite software Bridging the gap between these cognitive theory-inspired methods and the statistical language modeling methods of computer scientists lies the DeLite/\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n75\n\nEnLite program (vor der Br\u00fcck et al. 2008). This program was originally designed for German, but a prototype English version has been created as well. Published testing and evaluation, however, was conducted with the German version. The appeal of this program lies in its combination of what the authors call surface and deep indicators of both syntactic and semantic complexity\u2014and it is the inclusion of these deep indicators that the authors believe makes this system more psychologically valid. However, like the statistical language modeling methods that follow, this program conducts its final layer of text analysis by comparing the text in question with what it has \u201clearned\u201d about leveled texts based on prior training data. That is, program designers had over 3,000 ratings (seven-point Likert-type scale) of 500 texts by 300 participants from an online readability study. Some of these texts and readability ratings were used to train the program to recognize certain combinations of syntactic and semantic features at particular levels of readability. The rest of the texts were used for testing and cross-validation. Some of the syntactic indicators that the program examined were depth of embedded clauses (e.g., He left the house where the woman he loved lived immediately causes difficulty for the reader because the reader has to hold the main clause in memory while reading the subordinate clauses) and number of words per noun phrase. Some of the semantic indicators included semantic network quality (i.e., are semantic connections complete and clear), number of propositions per sentence, and length of causal and concessive chains (i.e., are there too many ideas in too few words).\nInterestingly, testing and cross-validation of this system showed that the indicator with the greatest weight was the basic surface-level indicator of sentence length (vor der Br\u00fcck et al. 2008). The use of both surface and deep indicators together, however, resulted in the best performance. DeLite\u2019s readability predictions correlated more highly with participants\u2019 difficulty ratings than did Flesch\u2013Kincaid predictions (r=0.53 vs. 0.43, respectively). Additionally, the authors used local government texts in this study rather than newspapers or general texts, and the Flesch\u2013Kincaid formula might not perform well with texts containing a lot of specialized language, numbers, and symbols.\nWhile the DeLite program improved upon the simple Flesch\u2013Kincaid formula for this corpus of texts, the program\u2019s predictions only accounted for 28% of the variance among ratings\u2014a much lower percentage than is typically expected among text difficulty measures\u2014 and the method was not tested by comparing text difficulty predictions with actual comprehension scores. Further research using more traditional texts may result in improved performance, but the software also should be tested against more recent traditional measures like the New Dale\u2013Chall, ATOS, and Lexile.\nEvaluation and directions for research\nText difficulty analysis via methods inspired by developments in cognitive science is a field still in development. Much of this work has moved beyond traditional readability methods by explaining what might make texts difficult for different readers (McNamara and Kintsch 1996; McNamara et al. 1996; Wolfe et al. 1998), how principled revisions of texts can improve readability (Britton and G\u00fclg\u00f6z 1991; Britton et al. 1993), and how complex cognitive indicators can be objectively measured through automated language processing software (Graesser et al. 2004; Lin et al. 2009; vor der Br\u00fcck et al. 2008). While these studies have helped researchers and educators better explain text difficulty, some traditional readability indicators seem to perform as well as the newer more complex methods (e.g., in Crossley et al. 2007, sentence length and word frequency were the most powerful indicators).\n\n76\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nHowever, these methods are still relatively new, and multiple opportunities exist to demonstrate the strengths of methods based on advances in cognitive science. One of the greatest weaknesses of solely relying on variables like sentence or word length and word frequency is that a passage in which the sentences are in random order can be judged to be just as difficult as a passage that actually makes sense. Additionally, a poorly written text with short sentences and common words might fool a traditional readability formula. Propositional analysis, however, could reveal the lack of cohesion in such texts. Combining traditional variables with more sophisticated ones could presumably provide a safety net for users of automated text difficulty analysis tools.\nThese cognitively based methods can be improved and validated through additional testing on large, diverse corpora. Empirical research should determine at what age group or reading level these methods can begin to be reliably used. A text designed for very young children, for instance, may not be complex enough for LSA or propositional analysis to work properly. Additionally, designing user-friendly tools and programs utilizing these methods is a critical step if these techniques are to be widely used. WordNet and Coh-Metrix are powerful programs, but the learning curve may be steep for many of the individuals who could benefit from their use. Likewise, their complexity could result in misuse. Some refinement of these programs in upcoming years to a more user-friendly interface could expand their use dramatically and result in fewer researchers exclusively using traditional readability formulas simply because of their simplicity. The DeLite software described above seems to reflect strides taken in this direction.\nFindings in statistical language modeling\nA common feature of several recent studies is their focus on improving readability analyses for Web pages (which did not exist when most traditional readability methods were developed) as well as more conventional informational texts. Traditional formulas have often performed poorly when analyzing Web documents (Collins-Thompson and Callan 2004; Schwarm and Ostendorf 2005; Si and Callan 2001), a phenomenon which may be attributed to the significant amount of \u201cnoise\u201d found in web documents (i.e., punctuation errors, sidebar menus, photograph captions) as well as the large number of web pages containing fewer than 100 words. Advances in computer science through statistical language models (SLMs) and support vector machines (SVMs) have made new types of studies possible. Both methods of analyzing texts function as classifiers based on training data, and both methods are briefly introduced below prior to discussing the developmental and empirical research that has been conducted using these methods.\nTools used in text difficulty research\nStatistical language modeling The type of SLM technique used in these studies is based on the probability that a particular word or words were generated by a language model of a particular grade level (e.g., a language model for grade 5) without regard to the surrounding context. For example, statistical analysis of a large text corpus reveals that the word red is more likely to appear in texts designed for the primary grades than the middle school grades, regardless of the topic or context. This method is well known among many educational and psychological researchers.\nHere is a concrete example of how this works: to build the appropriate models, the program is given a corpus of texts which have been labeled by their grade level. This allows the program to build a language model for each grade level which basically analyzes the\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n77\n\nprobability of particular words appearing in texts at particular grade levels. Once the program has been trained, the researcher can give it a new text. Based on the statistical analysis performed on the training data, the program can analyze the text\u2019s words and/or combinations of words to assign the text to the model most likely to generate that text. So if the new text has words that are more likely to be generated by a third grade model than any other grade, the program classifies the text as a third grade text based on the texts it was trained on. While the simplest SLMs only look at the likelihood of individual words being generated by a model, free of any surrounding context, SLMs can also be designed to look at strings of words. These more complex models, however, require a greater amount of training data (Si and Callan 2001).\nSupport vector machines SVMs allow a researcher to take a variety of input and classify it based on previous training data. SVMs are helpful when attempting to determine the types of grammatical features and patterns that might be more common in third grade texts, for example, as opposed to sixth grade texts. The SVM, like the SLM, develops grade-level text models by learning the characteristics of texts and then determining the likelihood that a new text was generated by one of the grade-level models. While SVMs are trained and tested similarly to ordinary SLMs, the benefit of using SVMs is that they can take the information from SLMs and incorporate it into a more complex classifier that can also include traditional readability features as well as more complex grammatical parsing features. This results in very powerful grade-level text classifiers that can be customized according to the needs of the researcher. SVMs may or may not use information from SLMs; the features incorporated into an SVM classifier simply depends upon the desire of the researcher.\nText difficulty research using statistical language modeling\nIn the following studies, researchers have used SLMs and/or SVMs to classify texts into grade-level categories. While the studies are presented in a roughly historical manner, they are intentionally grouped categorically rather than chronologically. Basic text difficulty research using SLMs is followed by refinements to the initial SLM techniques. Studies follow which add grammatical features to SLMs in attempts to improve performance. Finally, studies which incorporate numerous features into SVM classifiers suggest considerable potential for these tools as a means of analyzing text readability.\nSimple SLMs in text difficulty analysis: Si and Callan (2001) In the \u201cclassic\u201d study by Si and Callan (2001), it was hypothesized that readability estimates could be valid on a wider variety of document types if the estimates were based on the actual content of the text rather than surface features like sentence length, syllable counts, etc. Si and Callan focused their attention on Web documents, which can be problematic due to their often non-traditional layout, formatting errors, and sentence fragments. The authors trained their three models on an admittedly small corpus of 30 science web pages that had been written for students at particular grades levels (K\u20132, 3\u20135, and 6\u20138), ten pages for each model. Thus, the K\u20132 model, for example, was able to learn which words were likely to be generated by K\u20132 texts. After training, the authors tested the model on the remaining 61 Web pages they had collected and found that the best performance occurred when they combined their models with sentence length, for a total of 75.4% accuracy in predicting the grade categories of documents from the test set. For comparison, they found that the Flesch\u2013Kincaid formula only performed at 21.3% accuracy.\n\n78\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nSubsequent studies have expanded Si and Callan\u2019s (2001) findings and contributed to the field of readability for Web content in several ways: increasing training and test corpora as well as the specificity of grade-level assignments (Collins-Thompson and Callan 2004, 2005), including grammatical features in text analyses (Heilman et al. 2007, 2008; Schwarm and Ostendorf 2005), examining the application of machine methods for L2 learners (discussed in the following section; Heilman et al. 2007; Peterson and Ostendorf 2009), and exploring the potential for online readability decisions based on user search engine queries (Liu et al. 2004).\nRefining the technique: Collins-Thompson and Callan (2004, 2005) Since Si and Callan (2001) used a small corpus of Web texts in their study and categorized texts into only three grade categories, Collins-Thompson and Callan (2004, 2005) expanded the 2001 research by building a substantially larger corpus, classifying texts into 12 grade-level designations, using cross-validation to test reliability, and applying a \u201csmoothing\u201d process which removes words that only occur once or twice in the corpus and could skew the analysis. While authors found that traditional readability indices performed equal to or better than the new model set on controlled, expert-labeled test sets, traditional indices\u2019 performance dropped significantly when tested on the Web corpus, while the new model set remained stable (correlating 0.64\u20130.79 with the grade levels specified on the web sites). Especially significant is the finding that the authors\u2019 model set performed significantly better than a traditionally high-performing index (percent of unknown tokens) when analyzing short texts (fewer than 100 words) at the primary grade levels. The current commonly used shorttext readability index\u2014the Fry Short Passage Readability Formula (Fry 1990)\u2014has only been recommended for fourth grade and above, while the method developed by CollinsThompson and Callan was tested on grades 1 through 12.\nStudies incorporating grammatical features and error analysis By enlarging prior models to include syntactic features (Heilman et al. 2007, 2008; Schwarm and Ostendorf 2005) and changing the evaluation approach from examining correlations to also examining error levels (thus looking at the magnitude of a judgment error rather than simply whether or not a text was classified correctly; for example, classifying a fourth grade text as an eighth grade text is a greater error than classifying it as a fifth grade text), some of the most recent studies have moved toward potentially developing high-quality alternatives to traditional formulas for classifying both Web (Heilman et al. 2007, 2008) and traditional text (Schwarm and Ostendorf 2005). For Web texts, the use of grammatical feature sets have had mixed results: Heilman et al. (2007) found that the grammatical features could moderately improve performance when added to SLMs, but Heilman et al. (2008) found that by expanding the set of grammatical features using a context-free grammar parser, the grammatical features alone could perform well as readability predictors for Web texts. However, Heilman et al. (2007) did not compare their method with any traditional readability formulas, and the comparison of Heilman et al. (2008) with the Flesch\u2013Kincaid formula and a Lexile-like measure showed that both measures performed almost as well as their own. The benefit of the methods developed by Heilman and colleagues is their applicability for automatically analyzing documents on the Web, but based on the good performance of the Lexile-like measure, some of the newer commercial formulas like Lexile and ATOS might perform adequately with some minor adjustments.\nSchwarm and Ostendorf (2005) compiled a corpus of Weekly Reader, Encyclopedia Britannica, Britannica Elementary, CNN, and CNN abridged texts to train their SLM classifiers and their grammatical parser. They then trained, developed, and tested their SVM\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n79\n\nclassifiers on a Weekly Reader corpus (for grades 2\u20135), dividing it into three smaller corpora, one for each stage of the process. Their SVM classifiers included indices like sentence length, syllables per word, Flesch\u2013Kincaid score, the syntactic parse features, and the analyses of the SLMs. Once in the testing phase, the authors used the percentage of articles that were misclassified by more than one grade level as the error criteria. The authors\u2019 classifier performed with error rates ranging from 5.5% to 21%, Flesh\u2013Kincaid error rates ranged from 59% to 78%, and Lexile error rates ranged from 24% to 33%. Clearly, the authors\u2019 classifier outperformed the others on this test set. The weakness of this research is simply that the SVM classifier should be developed and tested for use with a much larger and more diverse corpus in order to demonstrate its superiority over the more traditional formulas.\nUtilizing Internet search engines: Liu et al. (2004) Machine methods may prove useful not only for analyzing the difficulty of texts but also for determining the reading-level category of the user. Liu et al. (2004) developed a method for analyzing search queries in online search engines, determining the reading-level category of the user based on the query, and returning texts that match the grade-level category of the user. Of course, search engines have improved dramatically since the 2004 study, and the authors\u2019 grade categories were very broad (K\u20136, 7\u20139, 10\u201312, undergraduate, graduate). However, the SVM classifier they developed using both SLMs as well as numerous syntactic features performed with 66\u201396% accuracy when given two-category combinations.\nIt is impressive that the SVM method was able to work so well given the paucity of input in a typical search engine query. Though the two-category combinations result in a very broad classification of texts, limited search engine results to those classified as appropriate for grades K\u20139 could be quite helpful for an elementary school student searching for information on the earth\u2019s crust, for example. However, until this type of approach is refined, it is probably not helpful for readers who can only comprehend texts within a very limited difficulty range. Of course, it would be highly convenient to be able to use search engine queries to determine a user\u2019s reading level and supply him or her with appropriate texts. Search engines, though, have developed significantly since the 2004 study, and it remains to be seen whether this method would be helpful with the current capabilities of search tools like Google or Bing.\nSummary and evaluation\nLanguage modeling provides a powerful means for expanding simple word frequency indices by analyzing the probability of a particular model (e.g., a model for fifth grade texts) generating a word or combination of words. Adding other features to the SLMs, though, seems to improve performance when classifying Web texts (Heilman et al. 2007, 2008) as well as informational documents (Schwarm and Ostendorf 2005). The major weakness common to all these studies is that their standard for determining performance accuracy is often a grade-level label that a text author gives to the text. It is impossible to fully compare these machine learning methods to methods like ATOS, Lexile, Read-X, and others without more information on how well readers actually comprehend these texts. Researchers developing these methods are largely computer scientists rather than reading researchers, and while machine-learning methods certainly show some promise, it is important that scientists from the fields of computer science, reading, and psychometrics combine their efforts to make substantial gains in developing more universal methods of determining readability.\n\n80\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nOf course, the SLM/SVM methods require the classifiers to be trained on texts that have already been deemed appropriate for particular grade levels or groupings. Thus, they do not stand alone as tools for measuring text difficulty. Rather, once other methods have determined the difficulty of the texts in the training corpus, these tools could be used to quickly and automatically categorize numerous texts, including Web texts, using sophisticated statistical techniques. For now, these studies simply demonstrate the beginnings of potential for using SLMs and SVMs as graded text classifiers.\n\nGeneral Recommendations for Educational Use\nIn the past two decades, readability research has expanded far beyond simple regression equations. Readers no longer simply read published text on a page; rather, much of what modern readers consume is on the Internet. Different methods of assessing text readability have begun to be better tailored for these media differences as well as distinctions in domain and variation in human beings. Anyone recommending particular methods of readability, then, must appropriately judge these methods based on various criteria.\nThe recommendations here are based on several factors which have been summarized for simplicity (see Table 1):\n& Is the method intended for/tested on a particular population? & Is the method intended for/tested with particular texts? & Has the method demonstrated predictive validity\u2014especially in predicting readers\u2019\ncomprehension of texts? & How widely available for immediate use is the method? & How much special training is required to use the method?\nEasily available methods that make valid predictions of readability on a wide range of texts for a diverse population will receive the highest recommendations. The reading populace can be divided into at least three easily definable groups; recommendations will be made for each group: adults, student readers, young emerging readers.\nRecommendations for use with adults\nMuch of the quantitative readability work in the past two decades has focused on analyzing text difficulty for skilled readers or at least readers who have acquired fundamental reading skills. This work also varies by purpose\u2014whether the metric or method focuses on general texts, specialized documents, Web pages, domain-specific texts, or specific populations. Though many adults still enjoy the novels and essays that are often required reading for school children, adults also have to navigate their way through news, technical writing, and informational texts intended for members of special fields as well as the general public. Not many of the methods utilized in these areas, though, are available for general use (for research in specialty texts and special populations, see Kirsch and Mosenthal 1990; Meyer et al. 1993; Kim et al. 2007; Feng 2009; Green et al. 2010; Heilman et al. 2007; Peterson and Ostendorf 2009).\nGenerally, developments in text difficulty assessment in the past two decades demonstrate that some of the more traditional readability variables still work well for typical books and texts. For general reading of connected text, the New Dale\u2013Chall Readability Formula (Chall and Dale 1995), Lexile (Smith et al. 1989), and ATOS (School Renaissance Inst., Inc. 2000) have been widely tested and proven to be largely reliable and valid for predicting comprehension of texts among readers of wide-ranging ages and abilities. The Coh-Metrix formula (Crossley\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nTable 1 Summary of general-purpose text analysis methods\n\nName or author of method\n\nTested population\n\nTexts\n\nEvidence of validity\n\nWidely available\n\nEasy to use\n\nNew Dale\u2013Chall Readability Grades 1 through\n\nFormula\n\nadults\n\nContinuous prose\n\nLexile scale\n\nGrades 1 through adults Continuous prose - books\n\nATOS\n\nK\u201312\n\nContinuous prose\n\nYes. R2=0.85 with cloze scores R2=0.91 with Bormuth\u2019s (1971)\ncloze scores Yes. R2=0.71 with test\nitem difficulties R2=0.69 with rank order of basal texts Yes. R2\u22650.80 with comp scores\n\nRead-X\nLSA for matching texts to readers (Wolfe et al. 1998)\n\nNone Formula is in\ndevelopment College students\n\nWeb texts Academic texts\n\nNone N/A\n\nLSA for predicting text comprehension based on coherence (Foltz et al. 1998)\nCoh-Metrix formula (Crossley et al. 2007)\nWordNet relations (Lin et al. 2009)\nDeLite/EnLite\nSLM by Si and Callan (2001)\nSLM by Collins-Thompson and Callan (2004, 2005)\n\nCollege students\nNone\u2014tested against Bormuth\u2019s corpus\nNone\u2014tested against online graded texts\nAdults\nNone\u2014K-8 texts\nNone\u2014grades 1\u201312 texts\n\nAcademic texts\nBormuth\u2019s corpus (school texts)\nOnline graded texts\nMunicipal texts\nScience Web pages labeled by grade level\nWeb pages labeled by grade level\n\nN/A\nYes. R2=0.90 with Bormuth\u2019s (1971) mean cloze scores\nLittle. No computed statistics, only graphs.\nYes. R2=0.28 with participant difficulty ratings\nYes. 75% accuracy in predicting grade-level category (K\u20132, 3\u20135, 6\u20138)\nYes. R2=0.41\u20130.62 with designated grade levels of texts\n\nYes\n\nYes\n\nManual and automated\n\nYes\u2014commercial Yes\n\nYes\u2014commercial for schools\nNo\n\nAutomated Yes Automated Yes\n\nYes\n\nNo\u2014must collect prior\n\nknowledge samples\n\nfrom readers\n\nYes\n\nYes\u2014with access to\n\nCoh-Metrix\n\nNo\u2014no formula was designed\nNo\n\nNo\u2014requires conducting a regression analysis\nNo\n\nNo\n\nNo\u2014not developed yet\n\nNo\n\nNo\n\nNo\n\nNo\n\n81\n\nTable 1 (continued)\n\nName or author of method\n\nTested population\n\nSLM by Heilman et al. (2008)\nSLM by Schwarm and Ostendorf (2005)\nLiu et al. (2004)\n\nNone\u2014grades 1\u201312 web texts\nNone\u2014grade 2\u20135 texts\nNone\u2014K-6, 7\u20139, and 10\u201312 texts\n\nTexts\nWeb pages labeled by grade level\nWeb pages labeled by grade level\nWeb pages labeled by grade\n\nEvidence of validity\n\nWidely available\n\nYes. R2=0.58 with labeled grade levels No\n\nYes. Error rates ranged from 5.5% to No 21%, much lower than FK and Lexile\nYes, but limited. 66\u201396% accuracy in No predicting which grade-level \u2018bin\u2019 a text belonged in when given two-category combinations\n\nEasy to use\nNo No No\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n82\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n83\n\net al. 2007) is promising, but still underdeveloped; however, because of its inclusion of deeper cognitive features, it presumably will be applicable to a wider variety of texts than traditional formulas. Further testing would need to be done to continue to validate this method on corpora other than Bormuth\u2019s (1971) text corpus. Also, the Coh-Metrix formula has not yet been tuned to determine the particular difficulty level or grade level of a text; it has only been used to judge the relative difficulty of texts. Coh-Metrix itself, however, could and should be used for analyzing texts for literate adult readers. A text\u2019s cohesion affects a reader\u2019s ability to comprehend a text, as illustrated by the studies in cognitively based text difficulty analysis. Thus, LSA and variables like argument and propositional overlap can actually provide means of measuring the difficulty as well as the quality of texts that may be too sophisticated for typical readability formulas (which have nearly always been developed with school children in mind). Furthermore, analysis using LSA and propositions can be used to revise poorly written texts, a task at which traditional readability formulas have performed notoriously poorly.\nRecommendations for use with school children\nPrior to the late 1980s, many readability scales for children were only validated for grades 4 and above. Many methods discussed in this review, however, are recommended for use with texts as simple as those designed for children at emergent reading levels. The category of \u201cschoolchildren\u201d here, though, is defined as those students who have reached the stage of \u201creading to learn\u201d rather than simply \u201clearning to read.\u201d Thus, recommendations in this section consider students largely in grades 4\u201312. The appropriateness of readability methods for emergent readers will be discussed in a following section.\nIf a school wants to use a single method for determining the readability of texts for readers of all ages and skill levels, the New Dale\u2013Chall Readability Formula (Chall and Dale 1995), Lexile (Smith et al. 1989), and ATOS (School Renaissance Inst., Inc. 2000) methods seem to be specifically directed toward this population and have been studied and researched extensively. At times, however, difficulty in determining readability can arise when examining content area textbooks. For this, Chall et al. (1996) Qualitative Readability Scales can be helpful in resolving ambiguities. Teachers, reading experts, and students are likely able to detect readability issues that automated scales cannot. Since the criterion passages of Chall et al. were tested against user ratings for appropriate leveling, these scales can provide additional support for decision making when student texts are selected. For a more automated approach, ATOS researchers found non-fiction books to be 0.42 grade levels more difficult, on average, than fiction books. Thus, educators and parents could make that simple adjustment when choosing books for children.\nFinally, though they are not yet ready for widespread use, the most promising tools for analyzing the difficulty of Web texts for school children appear to be variations on SLMs and SVMs. This research is so new that these methods are all still in development, but with continued research testing and refining these tools with large text corpora and validating them with actual reader comprehension data, these methods should allow for considerable flexibility when analyzing already existing texts.\nRecommendations for use with emergent readers\nAssessing text difficulty for emergent and developing readers provides a particular challenge for text difficulty researchers. While many of the currently available formulas and automated systems are advertised as being appropriate for use with children as young as first grade, some\n\n84\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nresearchers would argue that there are relevant features in texts for young children that simply cannot be measured via an automated or generalized system, and multiple criteria are necessary for determining appropriate texts for developing readers (e.g., Hiebert 1999).\nQualitative methods of assessing readability have seemed especially appropriate and have been popular when used for emerging readers (Hiebert and Pearson 2010). While readers are learning to read, factors like phonological regularity, number of words on a page, quality and specificity of images, size of print, number of syllables, and other factors can play a role in whether or not the child will have a successful reading experience while developing his or her reading skills (Hiebert 1999; Rog and Burton 2002). For this reason, the quantitative methods discussed in this review may not be the most appropriate for very young beginning readers. Teachers in the primary grades might consider using qualitative leveling methods (e.g., Fountas and Pinnell 1999, 2001; Peterson 1991; Rog and Burton 2002) when selecting books for their students. If, in a particular school district, teachers are not the primary persons responsible for choosing and leveling particular books in their classrooms, then teachers and administrators might request that their publishers begin to provide text levels based on some of these qualitative leveling methods.\nPopular newer readability methods like Coh-Metrix variables and Lexiles have not been previously tested as tools for discriminating between multiple levels of texts within the early grades. However, a recent study compared the effectiveness of these two tools in discriminating texts for beginning readers (Hiebert and Pearson 2010) in grades K\u20132. Hiebert and Pearson (2010) tested Lexiles and Coh-Metrix variables against other common readability formulas for detecting differences among leveled texts. Lexiles performed better than other indices in distinguishing between the seven levels of texts used, but neither the Lexiles nor the Coh-Metrix variables were consistently sensitive enough. Thus, with care, a tool like the Lexile system can be used as an initial indicator of appropriate texts for young readers, but teacher judgment and perhaps more qualitative methods are necessary for making final determinations.\n\nConclusion\nWhile controversy has surrounded the development and use of readability formulas for decades, researchers continue to develop methods to overcome past weaknesses. It seems quite possible that advances in natural language processing and other computerized language systems will reach a point when not only can text difficulty be accurately assessed for an individual but automated adaptation can also make informational texts more accessible to a particular reader. No doubt, when the time comes, controversy will surround such developments as well. In the meantime, educators and researchers alike can begin to see how the readability research field applies to the many facets of public and private life in a literate society. Traditional readability formulas have served their purpose in leveling typical books for school children, but more advanced and psychologically valid methods have been developed which are modeled after cognitive processing theories. These cognitively motivated approaches seem particularly appropriate for analyzing more complex informational texts for adolescents, college students, and adults in general. Finally, the most recent advances in machine-learning approaches point to a promising future in which non-traditional texts like those found on many web sites can be categorized for greater accessibility. With all these developments taking place in the past couple decades, the coming decade will hopefully present us with automated user-friendly readability tools for use in both research and practice.\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n85\n\nReferences\n\nAnderson, J. R. (1983). A spreading activation theory of memory. Journal of Verbal Learning and Verbal Behavior, 22, 261\u2013295.\nAnderson, R., & Pichert, J. (1978). Recall of previously unrecallable information following a shift in perspective. Journal of Verbal Learning & Verbal Behavior, 17(1), 1\u201312. doi:10.1016/S0022-5371(78) 90485-1.\nBailin, A., & Grafstein, A. (2001). The linguistic assumptions underlying readability formulae: A critique. Language & Communication, 21(3), 285\u2013301.\nBlackburn, B. (2000). Best practices for using Lexiles. Popular Measurement, 3(1), 22\u201324. Bormuth, J. (1966). Readability: A new approach. Reading Research Quarterly, 1, 79\u2013132. Bormuth, J.R. (1969). Development of readability analyses. Final Report, Project No. 7-0052, Contract No.\n1, OEC-3-7-070052-0326. Washington, DC: U.S. Office of Education. Bormuth, J. R. (1971). Development of standards of readability: Toward a rational criterion of passage\nperformance. Final report, U.S. Office of Education, Project No. 9-0237. Chicago: University of Chicago. Britton, B., & G\u00fclg\u00f6z, S. (1991). Using Kintsch\u2019s computational model to improve instructional text: Effects of repairing inference calls on recall and cognitive structures. Journal of Educational Psychology, 83(3), 329\u2013345. doi:10.1037/0022-0663.83.3.329. Britton, B., G\u00fclg\u00f6z, S., Glynn, S. (1993). Impact of good and poor writing on learners: Research and theory. Learning from textbooks: Theory and practice (pp. 1\u201346). Hillsdale, NJ: Lawrence Erlbaum. Carroll, J. B., Davies, P., & Richman, B. (Eds.). (1971). Word frequency book. New York: Houghton Mifflin. Caylor, J.S., Sticht, T.G., Fox, L.C., Ford, J.P. 1973. Methodologies for determining reading requirements of military occupational specialties: Technical report No. 73-5. Alexandria, VA: Human Resources Research Organization. Chall, J. S., & Dale, E. (1995). Readability revisited: The new Dale\u2013Chall readability formula. Cambridge: Brookline Books. Chall, J. S., Bissex, G. L., Conrad, S. S., & Harris-Sharples, S. (1996). Qualitative assessment of text difficulty: A practical guide for teachers and writers. Cambridge: Brookline Books. Coleman, E. (1962). Improving comprehensibility by shortening sentences. Journal of Applied Psychology, 46(2), 131\u2013134. doi:10.1037/h0039740. Collins-Thompson, K., & Callan, J. (2004). A language modeling approach to predicting reading difficulty. In S. Dumais, D. Marcu, & S. Roukos (Eds.), HLT-NAACL 2004: Main proceedings (pp. 193\u2013200). Morristown: Association for Computational Linguistics. Collins-Thompson, K., & Callan, J. (2005). Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science & Technology, 56(13), 1448\u20131462. doi:10.1002/asi.20243. Crossley, S. A., Dufty, D. F., McCarthy, P. M., & McNamara, D. S. (2007). Toward a new readability: A mixed model approach. In D. S. McNamara & G. Trafton (Eds.), Proceedings of the 29th Annual Conference of the Cognitive Science Society (pp. 197\u2013202). Austin: Cognitive Science Society. Dale, E., & Chall, J. S. (1948). A formula for predicting readability. Educational Research Bulletin, 27(1), 11\u201320\u201328. Dam, G., & Kaufmann, S. (2008). Computer assessment of interview data using latent semantic analysis. Behavior Research Methods, 40(1), 8\u201320. doi:10.3758/BRM.40.1.8. Davison, A., & Kantor, R. (1982). On the failure of readability formulas to define readable texts: A case study from adaptations. Reading Research Quarterly, 17(2), 187\u2013209. DuBay, W.H. 2004. The principles of readability. Retrieved 30 August 2010 from http://www.impactinformation.com/impactinfo/readability02.pdf. Fellbaum, C. (Ed.). (1998). WordNet: An electronic lexical database. Cambridge: MIT. Feng, L. (2009). Automatic readability assessment for people with intellectual disabilities. ACM SIGACCESS Accessibility and Computing, 93, 84\u201391. doi:10.1145/1531930.1531940. Foltz, P., Kintsch, W., & Landauer, T. (1998). The measurement of textual coherence with latent semantic analysis. Discourse Processes, 25(2\u20133), 285\u2013307. doi:10.1080/01638539809545029. Fountas, I. C., & Pinnell, G. S. (1999). Matching books to readers: Using leveled books in guided reading, K-3. Portsmouth: Heinemann. Fountas, I. C., & Pinnell, G. S. (2001). Guiding readers and writers: Grades 3\u20136. Portsmouth: Heinemann. Fry, E. (1990). A readability formula for short passages. Journal of Reading, 33(8), 594\u201397. Graesser, A. C., Gernsbacher, M. A., & Goldman, S. R. (1997). Cognition. In T. A. van Dijk & T. A. van Dijk (Eds.), Discourse as structure and process: Discourse studies: A multidisciplinary introduction, vol. 1 (pp. 292\u2013319). Thousand Oaks: Sage.\n\n86\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nGraesser, A., McNamara, D., Louwerse, M., & Cai, Z. (2004). Coh-Metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments & Computers, 36(2), 193\u2013202.\nGreen, A., \u00dcnaldi, A., & Weir, C. (2010). Empiricism versus connoisseurship: Establishing the appropriacy of texts in tests of academic reading. Language Testing, 27(2), 191\u2013211. doi:10.1177/0265532209349471.\nHeilman, M., Collins-Thompson, K., Callan, J., Eskenazi, M. (2007). Combining lexical and grammatical features to improve readability measures for first and second language texts. In Proceedings of the NAACL Human Language Technology Conference (pp. 460\u2013467). Morristown, NJ: Association for Computational Linguistics.\nHeilman, M., Collins-Thompson, K., Eskenazi, M. (2008). An analysis of statistical models and features for reading difficulty prediction. In EANL '08 Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications, June 19\u201319 (pp.71\u201379). Morristown, NJ: Association for Computational Linguistics.\nHiebert, E. (1999). Text matters in learning to read. Reading Teacher, 52(6), 552\u201366. Hiebert, E. H., Pearson, P. D. (2010). An examination of current text difficulty indices with early reading texts\n(Reading Research Report No. 10-01). Santa Cruz, CA: TextProject, Inc. Just, M., & Carpenter, P. (1980). A theory of reading: From eye fixations to comprehension. Psychological\nReview, 87(4), 329\u2013354. doi:10.1037/0033-295X.87.4.329. Kalyuga, S., Chandler, P., Sweller, J. (2000). Incorporating learner experience into the design of multimedia\ninstruction. Journal of Educational Psychology, 92, 126\u2013136.2000-03003-01110.1037/00220663.92.1.126. doi:10.1037/0022-0663.92.1.126. Kalyuga, S., Chandler, P., & Sweller, J. (2001). Learner experience and efficiency of instructional guidance. Educational Psychology, 21, 5\u201323. doi:10.1080/0144341012468110.1080/01443410124681200116707-001.10.1080/01443410124681. Kalyuga, S., Chandler, P., Tuovinen, J., & Sweller, J. (2001). When problem solving is superior to studying worked examples. Journal of Educational Psychology, 93, 579\u2013588. doi:10.1037/00220663.93.3.57910.1037/0022-0663.93.3.5792001-18059-013.10.1037/0022-0663.93.3.579. Kalyuga, S., Ayres, P., Chandler, P., Sweller, J. (2003). The expertise reversal effect. Educational Psychologist, 38, 23\u201331. doi:10.1207/S15326985EP3801. 10.1207/S15326985EP3801. Kim, H., Goryachev, S., Rosemblat, G., Browne, A., Keselman, A., & Zeng-Treitler, Q. (2007). Beyond surface characteristics: A new health text-specific readability measurement. American Medical Informatics (AMIA) Annual Symposium (pp. 418\u2013422). Washington, DC. Kintsch, W. (1988). The role of knowledge in discourse comprehension: A construction\u2013integration model. Psychological Review, 95(2), 163\u2013182. doi:10.1037/0033-295X.95.2.163. Kintsch, W., & Keenan, J. (1973). Reading rate and retention as a function of the number of propositions in the base structure of sentences. Cognitive Psychology, 5(3), 257\u2013274. doi:10.1016/0010-0285(73)90036-4. Kintsch, W., & van Dijk, T. (1978). Toward a model of text comprehension and production. Psychological Review, 85(5), 363\u2013394. doi:10.1037/0033-295X.85.5.363. Kirsch, I. S., & Mosenthal, T. B. (1990). Exploring document literacy: Variables underlying the performance of young adults. Reading Research Quarterly, 25, 5\u201330. Klare, G. R. (1963). The measurement of readability. Ames: Iowa State University Press. Klare, G. (1974). Assessing readability. Reading Research Quarterly, 10, 62\u2013102. Landauer, T., Foltz, P., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse Processes, 25(2\u20133), 259\u2013284. doi:10.1080/01638539809545028. Leahy, W., Chandler, P., Sweller, J. (2003). When auditory presentations should and should not be a component of multimedia instruction. Applied Cognitive Psychology, 17, 401\u2013418.2003-0069000410.1002/acp.877. doi:10.1002/acp.877. Lin, S., Su, C., Lai, Y., Yang, L., & Hsieh, S. (2009). Assessing text readability using hierarchical lexical relations retrieved from WordNet. Computational Linguistics and Chinese Language Processing, 14(1), 45\u201384. Liu, X., Croft, W.B., Oh, P., Hart, D. (2004). Automatic recognition of reading levels from user queries. SIGIR '04 Proceedings of the 27th annual international ACM SIGIR conference on research and development in information retrieval (pp. 548\u2013549). New York, NY: ACM. McClelland, J., & Rumelhart, D. (1981). An interactive activation model of context effects in letter perception: I. An account of basic findings. Psychological Review, 88(5), 375\u2013407. doi:10.1037/0033295X.88.5.375. McNamara, D., & Kintsch, W. (1996). Learning from texts: Effects of prior knowledge and text coherence. Discourse Processes, 22(3), 247\u2013288. doi:10.1080/01638539609544975. McNamara, D., Kintsch, E., Songer, N., & Kintsch, W. (1996). Are good texts always better? Interactions of text coherence, background knowledge, and levels of understanding in learning from text. Cognition and Instruction, 14(1), 1\u201343. doi:10.1207/s1532690xci1401_1.\n\nEduc Psychol Rev (2012) 24:63\u201388\n\n87\n\nMcNamara, D., Crossley, S., & McCarthy, P. (2010). Linguistic features of writing quality. Written Communication, 27(1), 57\u201386. doi:10.1177/0741088309351547.\nMcNamara, D., Louwerse, M., McCarthy, P., & Graesser, A. (2010). Coh-Metrix: Capturing linguistic features of cohesion. Discourse Processes, 47(4), 292\u2013330. doi:10.1080/01638530902959943.\nMeyer, B., Marsiske, M., & Willis, S. (1993). Text processing variables predict the readability of everyday documents read by older adults. Reading Research Quarterly, 28(3), 234\u2013249. doi:10.2307/747996.\nMiller, T. (2003). Essay assessment with latent semantic analysis. Journal of Educational Computing Research, 29(4), 495\u2013512. doi:10.2190/W5AR-DYPW-40KX-FL99.\nMiller, G. R., & Coleman, E. B. (1967). A set of thirty-six prose passages calibrated for complexity. Journal of Verbal Learning and Verbal Behavior, 6(6), 851\u2013854.\nMiller, J., & Kintsch, W. (1980). Readability and recall of short prose passages: A theoretical analysis. Journal of Experimental Psychology: Human Learning and Memory, 6(4), 335\u2013354. doi:10.1037/02787393.6.4.335.\nMillis, K., Magliano, J., Wiemer-Hastings, K., Todaro, S., McNamara, D. (2007). Assessing and improving comprehension with latent semantic analysis. Handbook of latent semantic analysis (pp. 207\u2013225). Mahwah, NJ: Lawrence Erlbaum.\nMilone, M. (2009). The development of ATOS: The renaissance readability formula. Wisconsin Rapids: Renaissance Learning.\nMiltsakaki, E., Troutt, A. (2007). Read-X: Automatic evaluation of reading difficulty of web text. Proceedings of E-Learn 2007, sponsored by the Association for the Advancement of Computing in Education. Quebec, Canada.\nMiltsakaki, E., & Troutt, A. (2008). Real-time web text classification and analysis of reading difficulty. In J. Tetreault, J. Burstein, & R. De Felice (Eds.), EANL '08 Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications (pp. 89\u201397). Morristown: Association for Computational Linguistics.\nPeterson, B. (1991). Selecting books for beginning readers: Children\u2019s literature suitable for young readers. In D. E. DeFord, C. A. Lyons, & G. S. Pinnell (Eds.), Bridges to literacy: Learning from reading recovery (pp. 119\u2013147). Portsmouth: Heinemann.\nPeterson, S., & Ostendorf, M. (2009). A machine learning approach to reading level assessment. Computer Speech and Language, 23(1), 89\u2013106.\nRog, L., & Burton, W. (2002). Matching texts and readers: Leveling early reading materials for assessment and instruction. Reading Teacher, 55(4), 348\u201356.\nRosch, E., Mervis, C. B., Gray, W., Johnson, D., & Boyes-Braem, P. (1976). Basic objects in natural categories. Cognitive Psychology, 8(3), 382\u2013439.\nRumelhart, D., & McClelland, J. (1982). An interactive activation model of context effects in letter perception: II. The contextual enhancement effect and some tests and extensions of the model. Psychological Review, 89(1), 60\u201394. doi:10.1037/0033-295X.89.1.60.\nSchool Renaissance Inst., Inc. (2000). The ATOS[TM] readability formula for books and how it compares to other formulas. Madison, WI: School Renaissance Inst., Inc. (ERIC Document Reproduction Service No. ED449468).\nSchriver, K. A. (2000). Readability formulas in the new millennium: What\u2019s the use? ACM Journal of Computer Documentation, 24(3), 105\u2013106.\nSchwarm, S.E., Ostendorf, M. 2005. Reading level assessment using support vector machines and statistical language models. Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pp. 523\u2013530, Ann Arbor, MI.\nSi, L., Callan, J. 2001. A statistical model for scientific readability. CIKM\u201901: Proceedings of the Tenth International Conference on Information and Knowledge Management, pp. 574\u2013576.\nSmith, R. (2000a). How the Lexile framework operates. Popular Measurement, 3(1), 18\u201319. Smith, R. (2000b). The Lexile community: From science to practice. Popular Measurement, 3(1), 20\u201321. Smith, D., Stenner, A.J., Horabin, I., Smith, M. (1989). The Lexile scale in theory and practice: Final report.\nWashington, DC: MetaMetrics (ERIC Document Reproduction Service No. ED307577). Stenner, A.J. (1996). Measuring reading comprehension with the Lexile framework. Paper presented at the\n4th North American Conference on Adolescent/Adult Literacy, Washington, DC. Stenner, A.J. (1999). Instructional uses of the Lexile framework. Durham, NC: MetaMetrics, Inc. (ERIC\nDocument Reproduction Service No. ED435976). Stenner, A., Burdick, D. (1997). The objective measurement of reading comprehension: In response to\ntechnical questions raised by the California Department of Education Technical Study Group. Durham, NC: MetaMetrics, Inc. (ERIC Document Reproduction Service No. ED435978). Stenner, A.J., Burdick, H., Sanford, E.E., Burdick, D.S. (2007). The Lexile framework for reading technical report. MetaMetrics, Inc.\n\n88\n\nEduc Psychol Rev (2012) 24:63\u201388\n\nvor der Br\u00fcck, T., Hartrumpf, S., & Helbig, H. (2008). A readability checker with supervised learning using deep indicators. Informatica, 32(4), 429\u2013435.\nWolfe, M., Schreiner, M., Rehder, B., Laham, D., Foltz, P., Kintsch, W., et al. (1998). Learning from text: Matching readers and texts by latent semantic analysis. Discourse Processes, 25(2\u20133), 309\u2013336. doi:10.1080/01638539809545030.\nWright, B., Stenner, A. (1998). Readability and reading ability. Paper presented to the Australian Council on Education Research (ACER) (ERIC Document Reproduction Service No. ED435979).\nWright, B., & Stenner, A. (2000). Lexile perspectives. Popular Measurement, 3(1), 16\u201317.\n\n",
        "hash_id": "78f9a7f2fabe5bb20d50358878143af5"
    },
    {
        "key": "8995VEUW",
        "version": 49,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/8995VEUW",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/8995VEUW",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/AJG9KHRK",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 313033
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Benjamin",
            "parsedDate": "2012-03-01",
            "numChildren": 1
        },
        "citation": "<span>Rebekah George Benjamin, <i>Reconstructing Readability: Recent Developments and Recommendations in the Analysis of Text Difficulty</i>, 24 <span style=\"font-variant:small-caps;\">Educ Psychol Rev</span> 63\u201388 (2012), https://doi.org/10.1007/s10648-011-9181-8 (last visited Jan 20, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "KMZHGKIW",
        "version": 47,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/KMZHGKIW",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/KMZHGKIW",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/U4PX8Z2N",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/KMZHGKIW/file/view",
                "title": "Davison and Kantor - 1982 - On the Failure of Readability Formulas to Define R.pdf",
                "length": 685091
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Davison and Kantor - 1982 - On the Failure of Readability Formulas to Define R.pdf.</span>",
        "fulltext": "On the Failure of Readability Formulas to Define Readable Texts: A Case Study from Adaptations Author(s): Alice Davison and Robert N. Kantor Source: Reading Research Quarterly , 1982, Vol. 17, No. 2 (1982), pp. 187-209 Published by: International Literacy Association and Wiley Stable URL: https://www.jstor.org/stable/747483 REFERENCES Linked references are available on JSTOR for this article: https://www.jstor.org/stable/747483?seq=1&cid=pdfreference#references_tab_contents You may need to log in to JSTOR to access the linked references.\nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at https://about.jstor.org/terms\nWiley and International Literacy Association are collaborating with JSTOR to digitize, preserve and extend access to Reading Research Quarterly\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n187\nOn the failure of readability formulas to define\nreadable texts: A case study from adaptations\nALICE DAVISON University of Illinois ROBERT N. KANTOR The Ohio State University\nIN THE PAST 30 OR 40 YEARS there has been much discussion of\nobjective formulas to measure the readability of texts. These formulas measure variables such as sentence length and familiarity of vocabulary, but do not define the actual features of texts which make them easy or hard to read. In this study, we compared two versions of four texts, the original versions intended mainly for adult readers, and the adapted versions intended for less skilled readers. We discuss the specific changes made to make the texts easier to read, with their apparent motivations. Some changes, such as splitting complex sentences into component clauses, changing vocabulary items, etc., may have been made to make the text conform to a certain level of readability defined by formulas. But these changes are not always the most successful, and some actually make the text harder to understand. Other changes could not have been made solely because of the effect they would have on readability measurement. They were influenced by factors such as definition of discourse topic, logical ordering of ideas, background knowledge assumed in the reader, and choices of syntactic structure which do not affect length. Adaptations were found to be most successful when the adaptor functioned as a conscientious writer rather than someone trying to make a text fit a level of readability defined by a formula. We argue strongly against the implicit use of readability formulas as guides to writing graded texts and urge experimental research to define the real factors constituting readability.\nEl fracaso de formulas de lecturabilidad para definir la lecturabilidad de textos: Un estudio de adaptaciones\nEN LOS ULTIMOS 30 a 40 afios ha habido mucho dialogo sobre f6rmulas objetivas para medir la lecturabilidad de textos. Estas f6rmulas miden variables como la longitud de oraciones y el previo conocimiento de vocabulario, pero no definen las caracteristicas presentes en los textos que permiten hacerlos mais ficiles o mais dificiles de leer. En este estudio comparamos dos versiones de 4 textos, la versi6n original preparada principalmente para lectores adultos, y la versi6n adaptada destinada para lectores de menos\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n188 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\ndestreza. Discutimos los cambios concretos hechos para hacer l textos mais ficiles de leer, con sus aprarentes factores de motivaci Algunos cambios, como la fragmentaci6n de oraciones complejas en clausulas de componentes, cambios de vocabulario, etc., es posi que se hayan hecho para conformar el texto a cierto nivel de lecturabilidad definida por f6rmulas. Pero estos cambios no s siempre los mis acertados, y algunos hasta hacen el texto ma dificil de comprender. Otros cambios no se hubieran podido ha por el efecto que habrian tenido sobre medidas de lecturabilid Factores que los influenciaron fueron definici6n del tema, ord 16gico de ideas, previo asumido del lector, y selecci6n de estructur sintaictica que no afecta longitud. Las mejores adaptacio resultaron cuando el autor de la adaptaci6n actuaba como un escritor responsable en vez de tratar de encajar un texto en un nivel de lecturabilidad requerido por una f6rmula. Nos oponemos\nfirmemente a la utilizaci6n absoluta de f6rmulas de lecturabilidad\ncomo guia para escribir textos de diferentes niveles y urgimos investigacion experimental para definir los verdaderos factores que constituyen lecturabilidad.\nAu sujet de l'echec desformules de lisibilite dans le but de defin des textes lisibles. une itude de cas a partir d'adaptations.\nAU COURS DES 30 ou 40 dernieres ann6es, il y a eu de nombreuses discussions faites au sujet des formules objectives ayant pour but de mesurer la lisibilit6 des textes. Ces formules mesurent des variables telles que la longueur des phrases et la connaissance du vocabulaire, mais ne d6finissent pas les caracteres reels des textes qui rendent ces textes faciles ou difficiles a la lecture. Dans cette etude, nous avons compare deux versions de quatre textes, les versions originales d6sign6es principalement aux lecteurs adultes, et les versions adapt6es d6sign6es aux lecteurs moins exp6riment6s. Nous discutons les changements sp6cifiques faits afin de rendre les textes plus faciles a la lecture, avec leurs motivations apparentes. Quelques changements, tels que la division de phrases complexes en membres de phrases constituantes en changeant les l66ments de vocabulaire, etc..., peuvent avoir 6t6 faits afin de rendre le texte conforme a un certain niveau de lisibilit6 d6fini par les formules. Mais ces changements n'atteignent pas toujours le but recherch6, et certains en fait rendent la compr6hension du texte plus difficile. D'autres changements n'auraient pas pu 6tre faits uniquement 'a cause de l'effet qu'ils auraient sur la mesure de lisibilit6. Ils 6taient influenc6s par des facteurs, tels que la d6finition de matiere de discours, l'ordre logique des id6es, la connaissance acquise suppos6e chez le lecteur, et les choix de structure syntactique qui n'affectent pas la longueur. On a trouv6 que les adaptations atteignent leur but lorsque la personne qui adaptait, fonctionnait en tant que scripteur conscientieux\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 189\nplut6t que d'essayer de faire correspondre le texte au niveau de lisibilit6 defini par une formule. Nous r6futons fortement I'usage implicite de formules de lisibilit6, comme guides de r6daction de textes remanies, et nous encourageons la recherche exp6rimentale \"a definir les facteurs reels constituant !a lisibilit&.\nFor the past 30 or 40 years much attention has been paid to formulas that claim to measure readability. In this paper we would like to consider the implications of using these formulas not just for measuring readability but also as guides to producing texts. The creators of the formulas and others have always warned against using the formulas as guides to the production of text because the correlation of infrequent vocabulary and long sentences to difficulty in reading is, as everyone understands, not a causal relationship. However, there is an inescapable temptation to use these formulas as a guide to writing a text, a temptation that is especially strong if the writer of a text is under an obligation to produce materials at a specific readability level, a feature which is often attractive to potential buyers. And publishers of textbooks, for children as well as college students, have recently been obliged by law in some states to guarantee the reading levels of their products. Thus, there are strong pressures on publishers to ensure that texts do meet a specified readability level.\nObjections may be made to readability formulas in general. Reading difficulty may be affected by the purposes and background of the reader and the inherent difficulties of the subject matter; it is not just a function of measurable properties like length and vocabulary.\nAll other things being equal, a publisher might legitimately use as a guide the level of reading difficulty assigned by a formula to a text written for a particular purpose and audience and otherwise adequate for that purpose and audience. There is a more serious consequence of relying on readability formulas, which we have discovered from having looked at the texts available commercially and through talking with editors, textbook representatives, and authors. It appears that the exigencies of the marketplace as well as the political pressures imposed by state textbook commissions have forced publishers of textbooks for children and young adults to \"go to work\" on initial drafts of texts, altering the language and content so that target readability scores will\nresult.\nBut revising a text to meet a readability score involves a crucial assumption: that readability formulas define readability. It is our purpose here to show that this assumption is not only false, but further, to show\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n190 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nthat proceeding under this assumption to produce a text may do harm than good.\nThere is empirical evidence (Bormuth, 1966) that readabil formulas do measure factors-length and complexity of vocabul that may reflect readability. But it is clear that the formulas do not p to all the features of a text that actually contribute to readabilit comprehensibility. The use of an objective measurement like a ability formula presupposes that the text already exists; it assumes t a writer chose a topic, made decisions about how to order the i within the topic, and then decided how to express the ideas in w Mistakes and faults in the writing of the text might affect comprehe in ways which would not influence the score given by the reada formulas, as writers such as Freeman (1978) and Charrow (1979) pointed out.\nMany such critics of traditional readability formulas take a more subjective, relativistic view of readability. That is, they view factors such as sentence length, syntactic structure, vocabulary, etc. as contributing to complexity relative to other factors, such as the purpose of the author, the structure of the text, the definition of discourse topic, and so on (Gourlay & Catlin, 1978; Freeman, 1978; Charrow, 1979). Von Glaserfeld (1970-1971) proposes a schema of sentence complexity which refers to context and meaning as well as syntactic features like constituent structure. Botel and Granowsky (1972) and\nEndicott (1973) propose general procedures for categorizing and\nweighing the complexity of the constituents of a given sentence, based on the actual grammatical features and meaning of the parts of specific sentences. Dawkins (1975) presents a taxonomy of structures, noted as more or less complex, though not all these judgments are fully substantiated by psycholinguistic research.\nThis kind of view might appear to be the antithesis of the position which values the objectivity of readability formulas. It is. Yet a true measurement of readability, one that in the future might indeed guide the production of texts, is dependent not only on measurable lexical and syntactic factors but also on a number of subjective factors, namely those which constitute the skill or common sense of the writer who is presumed to have created a coherent, well-formed text to which objective measurement may eventually be applied. In effect, \"readability\" is a combination of whatever is measured by formulas plus a combination of unexplored and undefined factors, called in some circles a\n\"Black Box.\"\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 191\nIn this study, we will show how writers of graded text tried to tailor their sentences and use of words so that the formulas will yield the reading-level figure being aimed at. Of course writers and adaptors do not follow readability formulas slavishly; we have noted a good deal of conscientious and careful rewriting. But as the examples we will discuss presently show, adherence to vocabulary restrictions and constraints on sentence length (and passage length) are often given primary importance at the expense of other factors which no one would deny are related to readability. These include \"Black Box\" features such as the explicitness of connection between clauses, the extrasentential, pragmatic factors of discourse and sentence topic and focus, the inference load placed on a reader, the epistemological status of statements, and finally, the appropriateness of vocabulary for a particular audience reading with limited background knowledge.\nWe have found that the most successful changes in the text often run directly counter to what readability formulas would suggest, and that the most unsuccessful changes are those motivated by the strictures of the readability formulas.\nMethod\nWe looked at four texts from SRA Reading Laboratory IIIb (Parker, 1963) designed for students in eighth, ninth, or tenth grade w are reading at levels 5 to 6. We secured the original sources (Gluec 1962; Mattox, 1961; Melbo, 1941; Parsons, 1913; hereafter referred as MILK, LIGHT, TREES, and DAYTON, respectively) and did a sentence-by-sentence analysis to determine what information was common to the two versions and what had been changed. All the te were shortened, average sentence length was lowered, average numb of clauses per sentence was reduced, and scores on the Fry and Da Chall scales were lowered by zero to five grade levels. We prepare an exhaustive listing of all the changes made in the adaptations an derived a taxonomy of change types in adaptation. The taxonomy w a large number of examples is presented in Davison, Kantor, Hanna Hermon, Lutz, and Salzillo (1980).\nWe assumed that the intention behind each change was to increase ease in reading, whether or not we felt that the change succeed in doing so. The only exceptions to this would be deletions of mater simply to abridge the text so that it did not exceed a stipulated number of words overall. We noted the differences between what was conveyed\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n192 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nby the original text and the corresponding parts in the adaptation tried to recreate the line of reasoning which the adaptor followe making the change, particularly what kind of motivation there was the change in specific cases (e.g., elimination of ambiguity, clarificat of unfamiliar words, simplification of sentence structure, etc.).\nClause Connections\nWe will give some representative examples here of chan made in adaptation. The initial motivation seems to have been to sho overall sentence length and to delete expendable material to sho the overall text length. These changes, however, have conseque for how the reader gets information about the logical relations betw\nclauses and sentences.\nTo reduce sentence length, the adaptor often splits a complex clause from the original into its component parts, so that in th adaptation the component clauses stand alone as independent senten For example:\n1) TREES\n0 If given a chance before another fire comes, the tree will heal its own wounds by growing new bark over the burned part\nA If given a chance before another fire comes, the tree wi heal its own wounds. It will grow new bark over the burned part.\nHere, the subordinate gerund clause \"by growing new bark over burned part\" is turned into a separate sentence. The markers of subordinate gerund clause by ... -ing have been removed.\nBut what formerly marked subordination also contained semantic information about the meaning relation between the s ordinate clause in the original and the main sentence. The origin version expresses quite explicitly that \"growing new bark\" was means by which the tree healed its wounds. Since the means claus comes an independent sentence in the adaptation, the sequence of\nsentences without an overt connective does not in itself make clear to\nthe reader exactly what the meaning relation is between the two sentences. The reader must infer a means relation between them, which is not problematic if the reader has some background knowledge about trees. But it would be easy for a reader without such knowledge about trees to miss this connection and to infer just a sequential relation between the sentences. On this interpretation, which follows a very common pattern, the second sentence is merely understood as expressing what happens next after the tree has healed its wounds.\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 193\nTable 1 Comparisons of original and adapted texts\nTREES DAYTON MILK LIGHT\nNumber of Words: Original 3,000* 4725 1256 1450 Adaptation 775 827 861 900 % of Original Length 25 17 68 62\nNumber of Sentences: Original 250* 375* 58 75 Adaptation 62 90 63 61\nDale- Dale- Dale- Dale-\nFry Chall Fry Chall Fry Chall Fry Chall Reading Level:\nOriginal 5 5-6 4+ 4.9 10 11-12 11 11-12 Adaptation 6 5-6 2 4- 5 7-8 8 7-8\n*estimate\nTable 2 Average number of words per sentence\nMILK LIGHT DAYTON TREES\nOriginal 24 19 12.5 12.5 Adapted 13 14 9 12\n\nTable 3 Clause comp\n\nNumber of Sentences in:\n\nNumber of Clauses Per Sentence 1 2 3 4 5 6+\n\nLIGHT\nOriginal 21 22 23 7 3 Adapted 22 20 12\n\nMILK\nOriginal 20 16 12 Adapted 29 21 15 6\n\n2\n\n3\n\n6\n\nNote. In counting number of clauses, relative clauses, including reduced pos modifiers, nominalizations, and conjoin\nmore complex source which involved c break in surface structure. Though we\nconsistent.\n\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n194 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nIn another example, (2), 2) DAYTON\n0 \"I'm going down to the contract,\" said Jack, \"to see that everything is all right.\"2\nA \"I'm going down to the building project,\" Jack said, \"I have to see if everything is all right.\"\nthe infinitive purpose clause becomes an independent sentence which does not directly express purpose at all. It simply expresses obligation, which in the context of the story suggests that Jack has the purpose which is explicitly indicated in the original. The reader must infer from the adapted version that the first sentence expresses a precondition for the second, and that the second sentence spells out a purpose and hence an explanation for the first action. While it is not difficult to make the correct inference, it is by no means clear that linking two sentences by inference is any easier, cognitively, than processing a subordinate purpose clause. The motivation seems solely to change the original sentence (15 words) into shorter sentences (9 words each), despite the fact that the sentences together are longer than the original. Furthermore, explicit information in the original is lost in adaptation because the original two clause sentence has been split into two separate sentences, with the deletion of the subordinating conjunction.3 There is a body of evidence that the presence of a conjunction makes a positive difference in how well children comprehend sentences with reversible causal relationships (Irwin, 1980; Marshall & Glock, 1978-1979). Children also prefer explicit indication of causal connection (Pearson, 1974-1975).\nBy way of contrast, some changes made in adaptation remove the uncertainty about what logical relation is to be inferred between sentences. In (3), two sentences are combined with the addition of a\npurpose infinitive clause and a conjunction of means and purpose,\n\"so that....\" This meaning had to be inferred in the original version, and the task is even harder in the original because of the sentence order. The second sentence functions as an explanation for the first. While this is not an uncommon pattern, it is more common for causes to precede results in discourse (cf. Katz & Brent, 1968; Clark & Clark, 1968).\n3) DAYTON\n0 We had water to drink after that. We set out basins and\ncaught the raindrops. A We set out basins to catch the raindrops so that we would\nhave water to drink.\nThe adaptation thus represents an improvement over the original, but the net result is to lengthen the sentence, contrary to what would\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 195\nbe dictated by a readability formula used as a method of making part of the text easier to read. Additions of explanatory material examples (14) and (15), prevent the reader from making mis inferences, though they lengthen the sentence as well.\nSuch changes in adaptation have more global effects beyo the scope of single sentences. The splitting up of a complex sen into separate sentences can have effects on the organization of a paragraph. The adaptor may have to reorder the sequence of sent to avoid incoherence or topic shifts. For example, the original paragr (4) consists of three very long sentences:\n4) LIGHT 0 (a) Motor launches take visitors into such a lagoon (b) on the southern coast of Puerto Rico (c) where on dark nights there is a dramatic display of luminescence. (d) Curving lines of light fall from the bow (e) as the launch enters the lagoon, and (f) a trail of light is left in the boat's wake. In the lagoon, (g) which has one of the greatest concentrations of bioluminescence in the world, (h) it appears as though (i) a huge floodlight were burning under the launch (j) and the bow seems to be plowing into\na wall of fire.\nThe sentences of the original are indexed so as to make matching with the adapted version easier, and to allow schematic comparison in (6).\n5) A (b) On the southern coast of Puerto Rico is a lagoon (g) that has one of the greatest amounts of bioluminescence in the world. (c) On dark nights, it creates a very dramatic display. (a & e) As the motor launch takes visitors into the lagoon, (d) curving lines of light fall from the bow. (f) A trail of light is left in the boat's wake. (h) It appears as though (i) a huge floodlight were burning under the launch, and (j) the bow seems to be plowing into a wall of fire.\nNote that a primary result of sentence splitting is to shorten the length of the independent sentences in the paragraph as the number of clauses is reduced. Schematically, the clauses in the passage are combined in the following way:\n6)0 Sentence 1. (Clauses a,b,c) 26 words 2. (Clauses d,e,f) 24 Average: 30 words\n3. (Clauses I,h,i,j) 39 per sentence\nA Sentence 1. (Clauses b,g) 22' 2. (Clause c) 9\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n196 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\n3. (Clauses a/e,d) 18 Average: 17 words 4. (Clause f) 10 per sentence 5. (Clauses h,i,j) 25 One change which shortens the first long sentence is the separation of the relative clause (c), which becomes a main clause in the adaptation. The third sentence is shortened by moving the relative\nclause (g) out of that sentence and attaching it to clause (b), as a modifier. Both clauses, which are subordinate in the original, express background information. When they are made into main clauses, as (c) is in the adapted sequence, they express assertions of salient and not background information. But since the information expressed by these original subordinate clauses both occur in the first part of the reorganized paragraph, they still function as background information rhetorically in a way which is analogous to syntactic subordination. Thus the adaptor has compensated in a quite sensitive way for the potential disorganization and shift of topic which would result just from sentence splitting. The alert reader will note, however, that while the adaptor has better organized information about the lagopn, no antecedent has been provided\nfor \"the launch.\"\nNote that changes made in one part of the text require additional changes to be made elsewhere. This was true in (5) because splitting the original sentences of (4) would have produced a totally incoherent jumble of sentences, intermingling general and specific statements. Adaptors are often able to foresee the effects of change and compensate for them elsewhere in the text. We have termed th phenomenon the Domino Effect. If adaptors cope with the effect wel it is because they are paying attention to factors like text structure and content, not just sentence length.\nTopic and Focus\nOne way in which a writer can successfully direct attention of the reader to what organizes a succession of sentences is to summarize the common thread in a topic sentence. We have found in our study adaptations some completely contradictory processes in the treatmen of topic sentences. In (7), for example, the first sentence of the original which is the topic sentence for the next five paragraphs about the lif cycle of the sequoias, has been deleted in the adaptation.\n7) TREES 0 If the life story of a Big Tree could be told in full, it would read like a wild adventure tale. The Giant Sequoia\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC4:56 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 197\nproduces seeds every year. It blooms ... A The giant sequoia produces seeds every year. It blooms . Without the topic sentence, the reader will have to infer w these paragraphs are about, since the first sentence of the adap paragraph starts right into a discussion of the steps of the life Without the topic sentence, a reader might even form an inco hypothesis, that what is to come is specifically about seed productio the sequoia. Upon encountering information later in the paragr about the stages in the trees' growth, an inexperienced reader m conclude that there is no particular unity to the information contai in the paragraph other than facts about sequoia trees. But we also found an instance where the adaptor added formation at the beginning of a paragraph. 8) MILK 0 Milk, on the average, is composed of 87 percent wate\n4 percent fat, and 9 percent nonfat solids, of which A Just what do all these milk products give to the peo\nwho use them? Milk is about 87 percent water. But t solid part ... Here passage length was increased for the sake of clarity by the add of information which organized the reader's perceptions around a si theme. Kantor (1977) demonstrates that the preceding discourse a how well the reader is able to assign correct reference to pron Results of a reaction time experiment (Davison & Lutz, Note 1) that reading time for sentences is shortened if a closely relevant text precedes. The topic and focus of sentences may be informally defi as pragmatic rather than grammatical functions which readers assign to sentence constituents, using as cues both syntactic stru and information from the surrounding context of discourse. Exa (9) - (11) illustrate changes made in sentence structure which ha effect of defining sentence topic, what the sentence is predicated of sentence focus, what is asserted in a statement as opposed to w assumed as old information. The discourse topic of subsequent senten may be introduced as new information by being placed in focus posi in a sentence, generally the last large chunk in the sentence. To usually conveyed by the constituent in subject position in the senten Thus topic and focus are linked in some loose way to sentence struct and to change sentence structures while ignoring these functions of parts of the sentence is to allow opportunities for distortion o\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n198 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nsentence's meaning and import, or to destroy the connections be its parts.\nThe original three-clause sentence in example (9) is split into\ntwo sentences.\n9) LIGHT 0 This small sand-dwelling animal emerges at nig\nsecretes a luminous mucus as it moves about.\nA This small animal, which lives in the sand, comes out at night. As it moves about, it secretes a luminous substance.\nThis change is probably an example of the Domino Effect, made.as consequence of another change-the modifier \"sand-dwelling\" is turne into a full relative clause, \"which lives in the sand,\" in order to eliminate the relatively uncommon word \"dwelling.\" The resulting four clause are split into two shorter sentences, the first ending with the adverbial \"at night.\"\nSentence focus position is typically identified with the last major constituent in the sentence; in this case \"(comes out) at night\" would be perceived as the sentence focus. As such, it would receive distinctive intonation and therefore emphasis in the discourse. But since the whole paragraph is about things that glow in the dark, and so at night, it would be redundant and uninformative for the author to place emphasis on \"at night.\" As the adaptation stands, the fact that \"at night\" is in focus position makes the following sentence sound like a non-sequitur.\nYet under other circumstances, very similar changes may serve to make a text clearer. For example:\n10) LIGHT 0 In World War II, Japanese naval officers during blackout night action near an enemy moistened the powder in the palm of their hands and read their navigation charts in the dim light it produced. A During World War II, Japanese naval officers used this powder. When they were close to the enemy during blackout night action, they moistened the powder in the palm of their hands. The could read their navigation charts in the dim blue light it produced.\nBy splitting clauses into independent sentences with the loss of the conjunction \"and,\" the adaptor causes greater focus and prominence to be placed on the last constituent of the sentences, \"in the palm of their hands,\" and \"in the dim blue light it produced.\" Here the text expresses\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 199\nrather complex information, about which the reader would not be likely to have much background information. So it is not a bad move to place focus and discourse emphasis on separate points. The adapto has also expanded the phrase giving background information, \"when they were close .. .,\" which places the description of the powder in clear context. The addition of information explains why the powder needed to be used for the purpose described in the following sentences Since none of this information is redundant or unnecessary, the separation of clauses and lengthening of the sentences improves the text.\nOther changes made to make clearer the topic and focus of the sentence are related to active and passive form. An individual passive sentence without a context might require more effort in interpretation simply because the logical and grammatical subjects are different, all other things being equal. A discourse, however, may be easier to com prehend if subject position is consistently used to define what is the topi of the sentence and of the discourse (cf. Gourlay, 1978; Gourlay & Catlin, 1978), and so \"topic of SI\" may be generalized by the mature reader as \"topic of this sequence of sentences including S1,\" since sentences in paragraph may be mutually relevant.\n11) TREES 0 (Three paragraphs about fire damage to the sequoias.) Lightning causes most of the fires but long ago the Indians sometimes set fire to the grass and needles on the forest floor. (Three sentences about the hunting activities of' the Indians.) (Paragraph break.) All of the Big Trees have been struck by lightning at least once. (Three sentences about lightning damage.) A (Two paragraphs about fire damage to the sequoias.) Most of the fires are caused by lightning. All of the very old trees have been struck by lightning at least once. (Three sentences about lightning damage.)\nIn the original version, the active structure places \"lightning\" in topi position, although the actual topic of the three preceding paragraphs has been fire. The shift of the topic to \"lightning,\" in the next paragraph, is obscured by the continuation of fire as topic in the second half of the sentence quoted in (11), \"but long ago the Indians sometimes set fire..., with an explanation of why this occurred and what damage it caused the sequoias in the rest of the paragraph.\nBy changing the sentence to its passive form, the adaptor has made it clearer that the topic of the preceding sentence isfire, which\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n200 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nis placed in subject/ topic position. This position is consistent w previous mention and the description of how fire affects the trees now the agent phrase \"by lightning\" is in focus position, where it r some discourse prominence. The reader is therefore led to expe discussion of lightning, and this expectation is confirmed by follows. This example shows that careful choice of what is in position may facilitate a shift of discourse topic.\nInference\nWe saw in examples (1) and (2) that deletion of connectives\ncould result in a loss of information that could lead the reader to misinfer\nthe intended connection between prepositions. In this section we take up the topic of inference in more detail. We concentrate here on examples where content information (as opposed to more functional connective information) is deleted. The result of such deletions can lead the reader to misinfer the original author's intent. The second set of examples in this section are meant to illustrate how potential misinferences are clarified by the adaptor.\nMisinferences. In example (12) the adaptor has created a very real potential for misinference by changing the intransitive \"which dangle\" to the transitive \"which they dangle\" and by deleting the modifying phrase \"attracted to these lights\" in the second sentence.\n12) LIGHT 0 Angler fishes, among the most unusual of luminescent fishes, have fingerlike extensions which dangle in front of their large, gaping mouths. Fishes attracted to these lights in the darkness are easily caught and eaten. A Angler fishes have fingerlike lights which they dangle in front of their large, gaping mouths. Fishes P in the darkness are easily caught and eaten.\nThe change from intransitive to transitive allows the inference that the angler fishes have control over the lights, and the deletion of the modifying phrase allows the inference that the fishes may control the lights like a flashlight to search out the fishes in darkness or that the other fishes cannot see and get lost in the dark. In those cases where preservation of meaning is crucial, deletions and syntactic changes such as those in (12) can often seriously distort the message. In example (13), the entire \"if\"-clause plus the connective \"or\" has been deleted from a sentence that is one of a series of sentences describing the amount of wood contained in the General Sherman sequoia tree.\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 201\n13) TREES 0 It contains enough lumber to build a good sized village. Or, if the General Sherman Tree were sawed into inch boards, it would make a box large enough to hold the greatest ocean liner ever built. A It contains enough lumber to build a good-sized village It would make a box large enough to hold the greatest\nocean liner ever built. The deleted information tells the reader how it is that a box could be\nmade from the General Sherman Tree. The connective \"or\" conveys a disjunctive relationship to the preceding sentence; the tree contains enough wood either to build a village or to make a box, not both. The reader might make the misinference about the adapted version that the tree might be hollowed out to make a box big enough for an ocean liner. Even if this misinference is not made, the reader might still legitimately (though this is not the writer's intention) infer a conjunctive relation, that the tree contains enough lumber to construct a village and a large\nbox.\nIn (14), the adaptor has added the phrase \"when it froze,\" presumably to make explicit that we are comparing frozen skim milk to\nfrozen water.\n14) MILK 0 In Toronto, a suburban ice-skating rink was flooded with 250 surplus gallons of it (= skim milk). Skaters found 0 it chipped less easily than frozen water. A An ice skating rink was flooded with 0 it. Skaters found that when it froze it chipped less easily than frozen water.\nIt is an open question whether this inference would be made without the addition of the phrase \"when it froze,\" but the adaptor's inclusion of this phrase, especially at the expense of a longer sentence, suggests that he or she believed this to be a potential problem.\nThe notion that the skim milk was \"surplus\" has been deleted. This qualification provides the reader with the motivation for using the skim milk on the ice-skating rink, which appears to be an unusual or wasteful thing to do. We suspect that the motivation for the deletion of the qualifier was that the word \"surplus\" was a difficult vocabulary item for the intended audience. This problem could have been remedied in ways other than deletion, one possibility being a relative clause like \"milk that would have spoiled,\" \"that was not needed,\" etc.\nClarifications of time and the person involved have also been introduced in (10), where an adverbial phrase \"near an enemy\" has been\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n202 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nparaphrased by a full clause \"When they were close to the enemy Clarification is also achieved by reordering some of\nconstituents of a sentence, which eliminated a possible source of biguity, as in (15):\n15) TREES 0 When a Big Tree falls, its needle-like leaves do not wither for years. A When a big tree falls, it takes years for its needlelike\nleaves to wither.\nThe negative construction in the original form of this sentence might be misunderstood because it negates the expression \"for years,\" rather than the verb next to the negative, \"wither,\" as would be normally expected. The adaptor, alert to the possible confusion caused by the\nambiguity, changed the construction to an unambiguous positive\nexpression.\nPoint of View and Modality\nIn the texts studied, there were direct references to the reader in the original, and these were retained in the adapted version of the text. In some cases, the impersonal \"one\" is changed to \"you.\"\n16) TREES 0 Sometimes a tree is hollowed out all the way to the top. By standing at the bottom and looking up at it, it is possible to see a patch of blue sky much as if one were looking through a telescope. A Sometimes a burned tree is hollowed out all the way to the top. By standing at the bottom and looking up it is possible to see a patch of blue sky, as if you were looking through a telescope.\nThe writer seems to be talking directly to the reader, an impression which is supposed to involve the reader with the text and to increase the\nreader's interest in it. While the use of reference to the reader is not directly connected with the statistical measurement of formal properties\nof texts, for which readability formulas were designed, personal reference is believed by many writers to be another factor which enhances readability (Flesch, 1949). Adaptors often add references to the reader in adapting a text whose original form may be rather impersonal. One of the texts analyzed in our study was a newspaper feature article containing a lot of historical and scientific facts. The adaptor added several sentences whose function appears to be to get and focus the reader's\nattention on main ideas.\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 203\nNevertheless, while adaptors are sensitive to the value of involving the reader with the text by means of personal reference, they regularly delete information which also serves to involve the reader with the text, although in a different way. This information has to do with the basis for a statement: whether a proposition is a known fact or just a belief, what its source is, and to what extent the author shares the view expressed. Learning to distinguish opinions and beliefs presented by an author from statements of fact, or beliefs shared by the authors, is part of acquiring an adult competence in reading. The process of adaptation often encourages the deletion of the material which the reader would\notherwise make use of in order to judge how true a statement is likely to be and what supports it.\nSome of the deleted material consists of sentence adverbs like\n\"supposedly,\" \"apparently,\" etc. These deleted words do not greatly\naffect the length of the sentence, so it is not clear what the motivation for\ndeletion would be, aside from removing a parenthetical word which might make processing of the sentence a little more difficult. The adaptor might have wanted to remove words which cast some doubt on the statements, where the hypothetical truth of the statement was not of primary importance. Instead, the sentences in the adapted version appear\nas statements of absolute fact. Along with sentence adverbs indicating the speaker's attitudes,\nclauses indicating the source and reliability of a statement are deleted, simplifying and shortening the sentence in question, as in these examples.\n17) TREES 0 A railroad freight agent has figured that it would require at least 40 modern flat cars to haul just the trunk alone. A And at least forty freight cars would be needed to haul away just its trunk.\n18) MILK 0 Romans were said by Pliny to rub bread soaked in asses' milk on their faces to make them fairer and prevent the growth of beards.\nA The Romans rubbed bread soaked in asses' milk on their\nfaces. They thought that this would make their skin paler. They also thought it would keep their beards from growing! The deleted material gives the authority for a statement which the author lets pass without comment. But the inclusion of a source in (17) would increase its reliability, as a railroad freight agent would be the best judge of how many flat cars would be necessary. Likewise, a statement\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n204 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nabout the Romans which comes from a historian who was a contem-\nporary of the people described would have the validity of an account based on firsthand knowledge rather than indirect evidence. \"Pliny could have been paraphrased in a short description.\nIt is interesting that the adaptor adds information about a source when it is important to underscore the fact that an incorrect opinion is expressed. In (18), where the authority for a true proposition was omitted, the adaptor also wants to make clear that the growth o beards is not really prevented with milk nor does it lighten the skin. So clauses were added (\"they thought that .. .\") which attribute the belief to the Romans, as distinct from the author.\nThe adaptor is therefore aware of the importance of giving the source of a belief or fact, but, in the adaptation, the deletion o sources makes an artificial and misleading distinction which does not hold in general: It is not the case that all true statements emanate directly from the author without qualification, while incorrect beliefs are at tributed to someone else. It seems strange to us that inexperienced readers are given explicit exercises in distinguishing fact from opinion i classrooms, but are not given as much practice as possible in makin this kind of judgment in actual reading.\nVocabulary Nearly every readability formula contains some measure of\nvocabulary difficulty, either in the form of a word list or by the application of some sort of word length metric. The assumption is that longer words tend to be learned later. In our study we found numerous ex amples of easier words substituted for what the readability formula would predict are more difficult. In MILK, for example, \"insecticide is changed to \"insect poisons.\" In general, a lexical item may be a candidate for change under two major circumstances: if it is a proper nam technical, or specialized term with no obvious import to young readers; or if the item is vague in denotation or connotation, or has fallen from current usage.\nProper names, technical, or specialized vocabulary. Often proper names are replaced by descriptions. For example, \"Hippocrates in (19) is replaced by the phrase, \"one of the most famous Greek doctors\n19) MILK\n0 Hippocrates recommended milk to his patients as a curative beverage.\nA One of the most famous Greek doctors told his patients\nto drink milk to cure illness.\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 205\nNote also that \"recommend\" is paraphrased as \"tell NP to (drink),\" \"curative\" turns up to \"to cure,\" and \"beverage\" as the verb \"drink.\" In the adapted version, the vocabulary is simpler than in the original, bu the syntax has become more complex, though not necessarily more difficult, with the addition of two subordinate infinitive clauses.\nSometimes a lexical item is completely replaced by a para-\nphrase that is less specific in its reference, but retains the gist of its meaning, as in \"produce light\" for \"luminescence.\" If it is crucial to the development of the text, like \"bioluminescence\" in LIGHT, then it may be\nretained.\n20) A The production of light by living organisms is called\nbioluminescence.\nVague or obsolete vocabulary. Lexical items often have more than one meaning. While it is usually the case that contexts narrow down the possible meanings, some words may remain ambiguous, and the adaptor may make a change to an unambiguous expression. In the adaptation of DAYTON, for instance, the substitution is probably motivated by such considerations.\n21) DAYTON 0 But the funniest thing was the rats. They came sailing down on soap-boxes and things-sitting up just like squirrels, looking around for a place to jump off. A The strangest thing was the rats. They came sailing down on soapboxes, sitting up just like squirrels.\nWhile \"funniest\" may either mean humorous or queer, \"strangest\" only\nmeans queer.\nFinally, also in the DAYTON text, we find the word \"kettles\" changed to \"pans.\" In 1913, when the DAYTON text was written, \"k\ntles\" could denote larger open cooking vessels, but nowadays m speakers of American English, especially younger ones, understan \"kettles\" in the narrow sense of a closed smaller container for heat water. Similarly, \"building project\" replaces \"contract\" in (2). adaptor here has updated the reference appropriately to preserve\noriginal intent.\nConcluding Remarks\nIn this study we have been concerned with the differen between readability, as measured by averages calculated by formu and specific features of the language in a text. Clearly some of the chan in individual sentences are changes in features, like length of sentence or words, whose averages over the text are measured by readabil\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n206 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\nformulas. If these were the only relevant factors in adaptation, it w be possible, one imagines, to create adaptations by blindly and chanically changing features of the text to which readability for are sensitive, and nothing more.\nOf course, this possibility presupposes that sentence len always contributes to complexity, which is not the case; although tence length may be reduced by splitting up complex sentences components, the relationships between sentences may then have spelled out, requiring the addition of more words. Further, a dif vocabulary item may not have a more frequent synonym, so that it be paraphrased, adding a clause or phrase to the sentence. Which str is most appropriate and effective is obviously a matter more of the wr judgment than of something measurable by formula.\nIn any case, readability formulas do not measure unexpresse items, or what is to be inferred. Information which is not expli expressed is a source of serious potential difficulty in other ways as One can easily imagine a text being incomprehensible or given an en different interpretation from the intended one if the crucial inferenc not correctly made or if the information is not explicitly given.\nIn the case of very important inferences, one would expect adaptor to see the possible difficulty and to spell out the intended m ing more clearly, as in (15). But we are most concerned about m subtle cases where the actual loss of information is fairly small, might be recovered from context. Various aspects of this question h been discussed in connection with examples (1) -(3), where long sente with conjunctions are compared with sequences of sentences with conjunction.\nFor children, and possibly also for less skilled adult readers, the presence of the appropriate conjunction enhances comprehension,\nwhether or not the information is also recoverable from context. Irwin\n(1980) demonstrates that the presence of conjunctions- improves comprehension in fifth-grade students, while not affecting the comprehension of competent adult readers.\nAt the very least, the connection between clauses may just indicate relatedness, or mark some information as background and other information as more salient. Part of a text may then be subtly distorted or become \"disconnected\" if syntactic changes and syntactic splitting take away these indications of connectedness and relatedness of topic. The adaptor obviously has to balance the risk of disconnectedness and incoherence against the need to shorten sentences and perhaps to shorten the text as well. If deletion of topic sentences and splitting of\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 207\nlonger complex sentences into shorter component sentences is done without a view of the entire text as connected discourse, then the adapted version may have fewer cues than the original for relating sentences and clauses. It might actually be harder to understand correctly than the original. The loss of information in adaptation probably has a greater effect than adults realize because younger readers have less background knowledge than adults and less experience in making use\nof the information which is available in the text.\nThe effects of specific changes made in adaptation are very hard to test experimentally. It is known, however, that readers with adult competence in reading have a much harder time understanding and remembering a text if the overall topic is completely unspecified (Bransford & Johnson, 1972). Experimentation in whether different\nversions of the same text are harder or easier for children to read and understand should certainly be tried, nevertheless.\nWe have argued here that there are features of texts which contribute to readability and that these have not been given their due as factors entering into the question of readability. They are difficult to quantify, and in many cases are only recently beginning to be understood by linguists, cognitive psychologists, and others interested in the analysis of discourse. Yet features of topic, focus, inference load, and point of view play important roles in comprehension, which are all the more crucial to identify because their effects are subtle and therefore not always likely to be noticed, especially when changes are made in the language of a text that might cause unforeseen difficulties. Their effects are also tied to larger issues which must be confronted by children in learning to deal successfully with features of \"adult\" language, such as point of view, style, epistemological status, etc.\nIf we can reach a better understanding of what constitutes readability, it will be possible to break with the conventional wisdom reinforced by readability formulas, and thus to use a greater variety of language and structure than is now used, without contributing to difficulty. That is, if \"difficult\" language and content are used in a controlled way, it would not be necessary to make arbitrary changes in order that texts be \"homogenized\" throughout to some level of readability. But such adaptations obviously require a great deal of care and thought, and cannot be done mechanically.\nReadability formulas, in our opinion, fail to give any adequate characterization of readability, except in a purely statistical sense from which no particular valid conclusions can be drawn for creating readable texts. We believe that the specific examples we discuss here make a\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n208 READING RESEARCH QUARTERLY * Number 2, 1982 XVII/2\n\nstrong case against using readability formulas as guides to writing We urge writers to rely on their own judgments about language which i appropriate for the intended reader, and not on stereotyped notions or readability formulas.\n\nREFERENCES\n\nBORMUTH, J Readability: A new aGpOpUrRoLaAcYh,. J. w. This basal is easy to read-or Reading Research Quarterly, 1966,1,is7i9t-?1T3h2e. Reading Teacher, 1978,32, 174-182.\nBOTEL, M., & GRANOWSKY, A. A formula for GOURLAY, J. W., & CATLIN, J. Children's com-\n\nmeasuring syntactic complexity: A directional prehension of grammatical structures in\n\neffort. Elementary English, 1972, 49, 513-516. context. Journal of Psycholinguistic Re-\n\nBRANSFORD, J. D., & JOHNSON, M. K. Contextual\n\nsearch, 1978, 7, 419-434.\n\nprerequisites for understanding: Some in- IRWIN, J. W. The effects of explicitness and clause vestigations of comprehension and recall. order on the comprehensions of reversible\n\nJournal of Verbal Learning and Verbal Behavior, 1972, 11, 717-726.\n\ncausal sentences. Reading Research Quarterly, 1980, 15, 477-488.\n\nCHARROW, V. R. Let the rewriter beware. Wash- KANTOR, R. N. The management and comprehen-\n\nington, D.C.: American Institutes for Re- sion of discourse connection by pronouns\n\nsearch, 1979.\n\nin English. Unpublished dissertation, Ohio\n\nCLARK, H. H., & CLARK, E. V. Semantic distinctions\n\nState University, 1977.\n\nand memory for complex sentences. Quar- KATZ, E. W., & BRENT, S. B. Understanding con-\n\nterly Journal of Experimental Psychology, nectives. Journal of Verbal Learning and\n\n1968, 20, 129-138.\n\nVerbal Behavior, 1968, 7, 501-509.\n\nDAVISON, A., KANTOR, R.N., HANNAH, J., HERMON,\n\nMARSHALL, N., & GLOCK, M. D. Comprehension\n\nG., LUTZ, R., & SALZILLO, R. Limitations ofof connected discourse: A study into the\n\nreadability formulas in guiding adaptations relationships between the structure of text\n\nof texts (Tech. Rep. No. 162). Urbana: Uni- and information recalled. Reading Research\n\nversity of Illinois, Center for the Study of Quarterly, 1978-1979, 14(1), 10-56.\n\nReading, March 1980. (ERIC Document MATTOX, N. T. Living light on land and sea.\n\nED 184 090)\n\nScience Digest, April 1961. Adapted as\n\nDAWKINS, J. Syntax and readability. Newark, \"Light.\"\n\nDel.: International Reading Association,\n1975.\n\nMELBO, i. R. Sequioa National Park. In Our Country's National Parks. Bobbs-Merrill,\n\nENDICOTT, A. L. A proposed scale for syntactic 1960. Adapted as \"California's Giants.\"\n\ncomplexity. Research in the Teaching of PARKER, D. A. SRA reading laboratory (Second-\n\nEnglish, 1973, 7, 5-12.\n\nary series, 11 lb). Chicago: Science Research\n\nFLESCH, R. The art of readable writing. New Associates, 1963.\n\nYork: Harper, 1949.\n\nPARSONS, J. Prisoners of the flood. In McClure 's\n\nFREEMAN, C. Readability and text structure: A Magazine, July 1913, XLI(3), 39-48. Adapted\n\nview from linguistics. In P. Griffin & R. Shuy as \"Disaster in Dayton.\"\n\n(Eds.), Children's functional language and PEARSON, P. D. The effects of grammatical comeducation in the early years (Final report plexity on children's comprehension, recall to the Carnegie Corporation of New York). and conception of certain semantic relations.\n\nArlington, Va.: Center for Applied Linguistics, 1978.\n\nReading Research Quarterly, 1974-1975, 10,\n155-193.\n\nGLUECK, G. H. About: Grade A. New York Times VON GLASERFELD, E. The problem of syntactic\n\nMagazine, February 18, 1962. Adapted as\n\"Milk.\"\n\ncomplexity. Journal of Reading Behavior, 1970-1971, 3, 1-14.\n\nReference Note\n1. Davison, A., & Lutz, R. Measurement of syntactic complexity relative to discourse. Pap presented at the winter meeting of the Linguistic Society of America, 1980.\n\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\nA case study from adaptions DAVISON & KANTOR 209\nFootnotes This article is a discussion of research supported in part by National Institute of Education contract\nUS-NIE-C-400-76-0116, done by the present authors in collaboration with Jean Hannah, Gabrielle Hermon, Richard Lutz, and Robert Salzillo. A longer version of the study is available as Technica Report No. 162, Center for the Study of Reading, University of Illinois at Urbana-Champaign We would like to thank Jean Hannah, Richard Lutz, and Anne Roalef Kantor for invaluable editorial assistance in the preparation of the Technical Report. We are indebted to Richard C. Anderson, William Brewer, Bertram Bruce, Georgia Green, William Hall, Jean Osborn, and Mark Seidenberg for reading the longer version of this study and offering comments, criticism, and encouragement; we are particularly grateful for the latter but perhaps have derived more benefit from the other categories.\nMr. Glen Phillips, editor at Science Research Associates, was most helpful in answering questions we asked about how adaptations are currently done.\n'Underlining marks corresponding parts of the original and adaptation which have undergone some change. The symbol marks the position where something is deleted in the original or added in the adapted version.\n2See p. 194 for discussion of \"contract\" for \"building project.\" 31f it is absolutely necessary to preserve the content of a text while simplifying it so that the reading level is lower, then great care has to be taken in cases like these to indicate the information in the missing connectives in some other way. Often this involves adding a phrase of meaning which has been deleted in syntactic simplification. If the overall text has to be shorter than the original, then some content is inevitably lost. This issue has implications for the simplification of legal documents and for captioning television programs for the hearing-impaired.\nThis content downloaded from 69.43.75.70 on Fri, 20 Jan 2023 19:52:01 UTC\nAll use subject to https://about.jstor.org/terms\n\n",
        "hash_id": "82bbe3225f82f1ee52c5f2f3e4cda799"
    },
    {
        "key": "U4PX8Z2N",
        "version": 45,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/U4PX8Z2N",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/U4PX8Z2N",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/KMZHGKIW",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 685091
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Davison and Kantor",
            "parsedDate": "1982",
            "numChildren": 1
        },
        "citation": "<span>Alice Davison &#38; Robert N. Kantor, <i>On the Failure of Readability Formulas to Define Readable Texts: A Case Study from Adaptations</i>, 17 <span style=\"font-variant:small-caps;\">Reading Research Quarterly</span> 187\u2013209 (1982), https://www.jstor.org/stable/747483 (last visited Jan 20, 2023).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "CGDE8JH8",
        "version": 44,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/CGDE8JH8",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/CGDE8JH8",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/82ZMSLY2",
                "type": "application/json"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Extracted Annotations (1/20/2022, 11:52:55 AM)\"While general measures of usabil.</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "82ZMSLY2",
        "version": 44,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/82ZMSLY2",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/82ZMSLY2",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Lamm and Wolff",
            "parsedDate": "2021",
            "numChildren": 1
        },
        "citation": "<span>Lukas Lamm &#38; Christian Wolff, <i>GCS: A Quick and Dirty Guideline Compliance Scale</i>, 16 <span style=\"font-variant:small-caps;\">Journal of Usability Studies</span> 24 (2021).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "UVNALSN5",
        "version": 43,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/UVNALSN5",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/UVNALSN5",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Seckler et al.",
            "parsedDate": "2014",
            "numChildren": 0
        },
        "citation": "<span>Mirjam Seckler et al., <i>Designing Usable Web Forms \u2013 Empirical Evaluation of Web Form Improvement Guidelines</i>, 10 (2014).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "K98W3ER4",
        "version": 42,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/K98W3ER4",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/K98W3ER4",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Pieterse",
            "numChildren": 0
        },
        "citation": "<span>Hein Pieterse, <i>Towards Guidelines for Error Message Design in Digital Systems</i>.</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "D2S9DAR8",
        "version": 41,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/D2S9DAR8",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/D2S9DAR8",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/L445S6L8",
                "type": "application/json"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Extracted Annotations (12/17/2021, 12:18:37 AM)\"As we show in our evaluation, t.</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "L445S6L8",
        "version": 41,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/L445S6L8",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/L445S6L8",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Collins-Thompson and Callan",
            "parsedDate": "2004",
            "numChildren": 1
        },
        "citation": "<span>Kevyn Collins-Thompson &#38; Jamie Callan, <i>A Language Modeling Approach to Predicting Reading Difficulty</i>, 8 (2004).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "TZCZI5V6",
        "version": 39,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/TZCZI5V6",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/TZCZI5V6",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Survey Monkey",
            "parsedDate": "2008",
            "numChildren": 1
        },
        "citation": "<span><span style=\"font-variant:small-caps;\">Survey Monkey</span>, <i>Smart Survey Design</i>, 36 (2008), https://s3.amazonaws.com/SurveyMonkeyFiles/SmartSurvey.pdf (last visited Dec 7, 2021).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "U2NW448R",
        "version": 37,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/U2NW448R",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/U2NW448R",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/TZCZI5V6",
                "type": "application/json"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Extracted Annotations (1/18/2022, 1:28:04 PM)\"uestions like demographics or per.</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "NM4H7X9G",
        "version": 40,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/NM4H7X9G",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/NM4H7X9G",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Brosnan et al.",
            "numChildren": 1
        },
        "citation": "<span>Kylie Brosnan, Bettina Gr\u00fcn &#38; Sara Dolnicar, <i>Cognitive load reduction strategies in questionnaire design</i>, <span style=\"font-variant:small-caps;\">International Journal of Market Research</span> 9.</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "FHTFZJQ6",
        "version": 37,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/FHTFZJQ6",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/FHTFZJQ6",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/NM4H7X9G",
                "type": "application/json"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Extracted Annotations (1/20/2022, 11:51:21 AM)\"We find that this is not the cas.</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "IVAMVSLJ",
        "version": 36,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/IVAMVSLJ",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/IVAMVSLJ",
                "type": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 10636105,
                "username": "brycewsuffolk",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/brycewsuffolk",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Herd and Moynihan",
            "parsedDate": "2018",
            "numChildren": 0
        },
        "citation": "<span><span style=\"font-variant:small-caps;\">Pamela Herd &#38; Donald P. Moynihan</span>, <span style=\"font-variant:small-caps;\">Administrative burden: policymaking by other means</span> (2018).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "JRX664XE",
        "version": 29,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/JRX664XE",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/JRX664XE",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/Z6GMBPWF",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/JRX664XE/file/view",
                "title": "Conroy - Finding Case Law Leveraging Machine Learning Rese.pdf",
                "length": 1556835
            }
        },
        "meta": {
            "createdByUser": {
                "id": 27372,
                "username": "colarusso",
                "name": "David Colarusso",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/colarusso",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Conroy - Finding Case Law Leveraging Machine Learning Rese.pdf, https://ceur-ws.org/Vol-3257/paper6.pdf (last visited Nov 19, 2022).</span>",
        "fulltext": "Finding Case Law: Leveraging Machine Learning\nResearch to Enhance Public Access to UK Judgments\nAmy Conroy1,2,\u00a7, Editha Nemsic1,3,\u00a7, Daniel Hoadley1 and Imane Hafnaoui1,4\n1Mishcon de Reya LLP, London, WC2B 6AH, United Kingdom 2University of Bristol, Bristol, BS8 1TH, United Kingdom 3University College London, London, WC1E 6BT, United Kingdom 4Queen Mary University of London, London, E1 4NS, United Kingdom\nAbstract\nOnce ranked last in Europe for public access to judgment data, the United Kingdom has taken large strides in recent years to improve the accessibility of judgments. This paper discusses how the new platform from The National Archives, Find Case Law, was developed for the publication of UK judgments; in particular how we created the engine responsible for the enrichment of judgment text. We argue that the new system is necessary to address existing issues with the accessibility of judgment data, and if the platform were to leverage the abundance of research conducted in areas such as legal text classification, summarisation, and entity recognition, the UK could quickly become a world leader for public accessibility of judgments. We develop a proof of concept system, MyJudgments, that demonstrates a potential direction for development. Whilst it is early days, the launch of Find Case Law provides a unique opportunity to remind ourselves of the opportunities machine learning presents for broadening the accessibility of judgments to new users and expanding their utility for novel use-cases. To do this, we review existing research performed on UK judgment data and suggest how the various strands could practically be integrated into a case law publication system.\nKeywords\nCase law, machine learning, judgment publication\n\n1. Introduction\nThe United Kingdom has long fallen behind its European counterparts in regards to public access to judgment data, made clear by a 2018 report from the European Commission which ranked the UK last in comparison to other European countries [1]. Judgment dissemination in the UK has traditionally been carried out by commercial publishers who selectively published precedent-setting case law in the form of law reports on a paid-subscription basis. Since the early 2000s and until relatively recently, the de facto official online source of publicly accessible\n\n\u00a7These authors contributed equally to this work.\n\nJoint Proceedings of ISWC2022 Workshops: the International Workshop on Artificial Intelligence Technologies for Legal\n\nDocuments (AI4LEGAL) and the International Workshop on Knowledge Graph Summarization (KGSum) (2022) Envelope amy.conroy@mishcon.com (A. Conroy); editha.nemsic@mishcon.com (E. Nemsic);\n\ndaniel.hoadley@mishcon.com (D. Hoadley); imane.hafnaoui@mishcon.com (I. Hafnaoui) GLOBE https://www.amyconroy.co.uk (A. Conroy) Orcid 0000-0002-4030-0337 (A. Conroy)\n\n\u00a9 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\n\nCEUR Workshop Proceedings (CEUR-WS.org) CEUR\nWorkshop Proceedings\n\nhttp://ceur-ws.org ISSN 1613-0073\n\n51\n\njudgments in the UK was BAILII1. BAILII\u2019s case law coverage is not comprehensive. Recent work comparing BAILII\u2019s coverage of judicial review judgments with that provided by the commercial research platform vLex Justis identified a significant gap in public access throughout the period measured [2]. The authors of that study, building on earlier analyses [3, 4] conjectured that the gap in public access is attributable to the complicated court recording and transcription regime under which privately-owned transcription agencies convert oral judgments into written form for a fee. In contrast with its commercial counterparts, BAILII lacks the funds to obtain these transcripts at commercial rates, thereby rendering a substantial portion of judgments accessible only behind a paywall. This state of affairs is particularly problematic in common law jurisdictions like the UK where judgments constitute a primary source of law. To address this, in 2021 the Ministry of Justice announced that cases \u2019of legal significance\u2019 would be published on a new platform called Find Case Law, built and maintained by The National Archives (TNA) [5].\nThis paper discusses the development of the government-backed Find Case Law XML enrichment engine, including the introduction of open source annotation pipelines for case law citations, abbreviations, and legislative instruments. We provide an overview of where public access to judgments currently stands in the UK, and where it still has the potential to go if existing machine learning research was incorporated into a case law publication platform. We review existing research into the use of computational methods on UK judgments, such as rhetorical role labelling and summarisation. To visualise how this research can be leveraged in a practical way to improve public access to judgment data in the UK, we develop a proof of concept system (\u2019MyJudgments\u2019) that incorporates the work delivered by the Find Case Law platform and the other machine learning research2.\n2. Background\n2.1. Linked Data and LegalDocML\nThe majority of research surrounding the development of a Case Law publication system has focused on the optimisation of the data within the judgment itself. This includes the addition of links between different Government applications, such as a legislation and case law database [6]. Adding direct links between different sources of legal principles allows those who are non-experts to interact with the data in the same way that an expert might infer the links between sources simply by reading the judgment text. An attempt to standardise this was introduced by the European Union in the form of the European Law Identifier (ELI) and the European Case Law Identifier (ECLI) [6].\nIn order to improve the accessibility of judgments in the UK, the Government has chosen to adopt the Legal Document Mark-Up Language (LegalDocML) [7]. LegalDocML is a standard developed for legal documents, including legislative, parliamentary, and judicial documents. Leveraging LegalDocML allows the different functions of Government to interact by sharing common document and metadata points. The implementation of the LegalDocML standard,\n1https://www.bailii.org 2https://github.com/mdrresearch/MyJudgments\n52\n\nparticularly with reference to case law and legislation links, will be discussed further in the next section.\n2.2. Machine Learning Research on Judgments\nThere have been a number of various research strands relating to applying machine learning and other computational techniques to judgment data, often with the motivation of improving public access to the data. This includes research into automatic summarisation systems both in the UK [8, 9] and other jurisdictions [10, 11], the analysis of agreement statements between judges to identify the majority opinion [12], and other text classification experiments [13]. Leveraging this research to enhance existing case law publication systems has the ability to improve the accessibility of judgments, and put experts and non-experts on a level playing field. We explore UK-focused research in further detail in the below sections.\n2.3. Motivation\nOur motivation for this paper is to review the available machine learning (ML) research on UK judgment data and demonstrate how the current research can be leveraged to improve the understanding of judgments, and thus public access to case law. It is important to focus on research undertaken on UK judgment data given the way that judgments are drafted varies between jurisdictions.\nExisting research has largely focused on extracting information from judgments but less so on knowledge presentation techniques for legal documents. The combination of the release of the Find Case Law service in the UK and recent advancements within legal ML research suggest it is the perfect time to focus on combining Government case law platforms with the work done in academia. We propose a proof of concept system that we suggest can easily make use of existing computational techniques to surface valuable judgment information to improve access to judgments for the average citizen. For the purposes of our platform, we suggest that the average citizen is an ordinary, non-legally trained, user without access to the paid-for private case law services typically available for commercial legal teams.\n2.4. Considerations\nThe development of any system for public use must be balanced with the needs and wants of the public as well. A recent report by the Legal Education Foundation aimed to understand how members of the public viewed commercial access to judgments and data in court records [14]. The key findings indicated that \u2019respondents overwhelmingly found it important\u2019 for there to be controls around who can access court data, how they access it, and what they can do with it. Thus, any system developed to aid the public in understanding and accessing judgments needs to be balanced against the wishes of the public. In addition to the above considerations, any work in this area must be balanced against The National Archives\u2019 own licensing restrictions. Any computational analysis on judgments from TNA\u2019s Find Case Law system can not take place until a transactional licence is obtained3.\n3https://caselaw.nationalarchives.gov.uk/transactional-licence-form\n53\n\n3. UK Judgment Enrichment Pipeline\nThe UK\u2019s \u2019Find Case Law\u2019 service went live in April 20224, and consisted of a publishing service, a public facing user interface and enriched judgment XML content downloadable from the website. This paper focuses on the development of the XML enrichment engine, rather than the user interface. The enrichment engine consists of five separate annotators; for case law citations, legislation, legislation provisions, oblique references to legislation, and abbreviations. The development of the annotators and resulting output, which uses LegalDocML format, will be explained briefly in the following section, the code for which is available on Github5.\n3.1. Case Law Annotator\nThe first annotator in the pipeline is the Case Law Annotator, which detects both well-formed and malformed references to UK judgments and links them to the corresponding judgment available on the Find Case Law website. It was important to be able to detect both well-formed and malformed references, as these references are often written incorrectly in UK judgments due to the specificity that is required and the variation of citations between the different courts.\nIn order to detect malformed citations, a rule-based approach was used where each rule represented a well-formed citation or a sub-set of the most common malformed versions of the citation. These were then stored as rules in spaCy\u2019s EntityRuler [15]. The XML that is wrapped around the identified citation includes the canonical, or well-formed, citation and the link to the case. An example of this is: <ref href=\"https://caselaw.nationalarchives.gov.uk/ewca/civ/2021/1308\"uk:\ncanonical=\"[2021] EWCA Civ 1308\"uk:isneutral=\"true\"uk:type=\"case\"uk:year=\"2021\">2021 EWCA.Civ\n1308</ref>.\n3.2. Abbreviation Annotator\nThe purpose of the abbreviation annotator is to detect abbreviations and resolve the short form (for example, HRA) to the long form (Human Rights Act). In LegalDocML, this is represented by the following: <abbr title=\"Human Rights Act\">HRA</abbr>. We adapted the abbreviation detector from the Blackstone library6, which itself was an adaptation of scispaCy [16].\nThe abbreviation detector previously worked by identifying items in brackets and walking backwards to see if the preceding words started with the same letters. In order to account for the way in which traditional abbreviations are defined in UK judgments, we constricted this to apply only where there were brackets and then quotations around the short form. This ensures that only abbreviations of courts or legislation, for example, are detected rather than information in brackets that are not traditional abbreviations (such as an alternative defendant name).\n4https://caselaw.nationalarchives.gov.uk 5https://github.com/nationalarchives/ds-caselaw-data-enrichment-service 6https://github.com/ICLRandD/Blackstone\n54\n\n3.3. Legislation Annotators\nThere are three different annotators that are used to link to relevant legislation as referenced in the judgment. The first legislation annotator in the pipeline applies LegalDocML to the primary legislation referred to. For example, <ref href=\"http://www.legislation.gov.uk/id/ ukpga/2006/46/\"uk:canonical=\"2006 c. 46\"uk:type =\"legislation\">Companies Act 2006</ref>, links to the relevant legislation instrument on the legislation.gov.uk website. In order to identify the correct legislation, the annotator uses a combination of exact string and fuzzy matching that references a lookup table of existing Acts. The table is updated every seven days by querying a www.legislation.gov.uk SPARQL endpoint.\nIn addition to references to the Act itself, we implemented a legislation provision annotator that identifies and links to specific sections of the legislation. An example of this is: <ref href=\"http://www.legislation.gov.uk/id/ukpga/2006/46/section/17\"uk:canonical=\"2006 c. 46 s. 17\"uk:type=\"legislation\">section 17</ref>. The final legislation annotator identifies oblique references (such as this Act or the Act) and links them to the relevant piece of legislation. In LegalDocML, this is represented as <ref href=\"http://www.legislation.gov.uk/id/ukpga/1972/68\" uk:canonical=\"1972 c. 68\"uk:type=\"legislation\">this Act</ref>.\nThe legislation provision and oblique reference annotators were implemented using similar methods. Using the previously enriched judgment, we extracted sentences where a piece of primary legislation had been detected. When we identify reference to an oblique reference or provision using regex, we use the location of the citation to find the closest piece of legislation within a certain threshold. Where sections or oblique references are re-defined to a difference piece of legislation, the subsequent references will be linked to the newly referenced legislation.\n4. Application Development and Legal Machine Learning Research\nIn this section we review existing machine learning research performed on UK judgment data, and suggest how this can be incorporated into a case law publication platform to enhance the public\u2019s experience interacting with judgments. In order to do this, we develop a proof of concept system, MyJudgments as shown in Figure 1, that provides a simple interface to demonstrate how existing lines of machine learning research could be incorporated into a system that provides value to the end user. With little effort, we demonstrate it is possible to expose levels of detail and additional insight into judgments that are typically only available to those with licenses to private commercial judgment products, or to those who are legally trained and able to infer the contextual information.\nBefore exploring existing research it\u2019s important to explain the proof of concept system, MyJudgments. It is a simple user interface designed to be complementary to the Find Case Law platform. It was built with a React back-end, allowing the user to view judgments with access to an additional layer of informative details extracted from the judgment and surfaced in a user-friendly way. The core purpose of the application is to make judgments available in a format that is easy to navigate and digest for the average, non-legally trained, user.\n55\n\nFigure 1: MyJudgments view of an example case.\n4.1. LegalDocML\nIn the first instance, we use the enrichment engines provided via the open source Find Case Law Github repository to annotate case law citations, legislation citations and abbreviations. Although the labelling of case law and legislation provisions with LegalDocML result in linked references to the relevant citations, which itself is improving the user experience of interacting with the judgment, we suggest it can be taken further by exposing the underlying information in a visual and interactive way.\nWe leverage the information provided in the XML to expose the number of case law and legislation citations, a definition key to the abbreviations used within the judgment, and a list of the cases cited within a judgment. Figure 3 displays an example judgment with a citation to a provision within a legislative instrument at the bottom of the page. On the right-hand side there is a hyperlinked list of case law citations that are referenced within the current judgment, allowing them to navigate to the cited case law. For those unfamiliar with certain cases, having instant access to the available citations allows users to understand the precedent that influenced the decision. In a common law system, understanding the case law cited within a given judgment is imperative to comprehend the law itself.\nIn addition, legal judgments are drafted in a way that makes it difficult for those without\n56\n\nFigure 2: Example judgment with highlighted sentences containing the grounds of the claim and a citation to a legislative instrument.\ntraditional legal training to quickly grasp the decision and other subtle contextual information, that someone with legal training may easily understand. A common example of this is abbreviations, which are used frequently in judgments, particularly for things such as courts and other legal terms. While those who practice or work in the legal sector are familiar with common abbreviations, others are likely to have to frequently refer back to where the terms were first defined. By providing a definition key that simply extracts the list of abbreviations from the XML of the judgments, as shown in Figure 3, we are exposing readily available information that is hidden within the XML and making it easier for the user to interact with the contents of the judgment.\n4.2. Rhetorical Roles\nResearch into the automatic classification of rhetorical roles on legal judgments across jurisdictions has been plentiful. In the UK, much of the research into rhetorical role classification stems from the early 2000s work by Hachey and Grover [17, 8]. Hachey and Grover\u2019s rhetorical role annotator labelled sentences with a label of FACT, PROCEEDINGS, BACKGROUND, FRAMING, DISPOSAL, TEXTUAL, or OTHER. They experiment with different machine learning techniques to assign the roles on a sentence-level. Their best result was a 60.6% F-Score with a support\n57\n\nvector machine classifier. Bhattacharya et al. used deep learning techniques to classify rhetorical roles on a sentence-\nlevel on UK judgments [18]. They found that neural methods such as Hierarchical BiLSTM architectures performed better compared to other ML techniques. They also found that it was better to train models on data from the target jurisdiction, underlying the need for further legal ML research on UK judgments.\nMuch of the research surrounding the classification of rhetorical roles in judgments has suggested that there would be a large benefit if the roles were to be exposed to the end-user. We demonstrate this in Figure 3, where the highlighted block of sentences has been automatically labelled with the rhetorical role of \u2019grounds\u2019. To label the sentences we use a light-weight decision tree classifier, trained on manually labelled grounds sentences in judicial review judgments. This allows the user to quickly locate the grounds of the judgment and understand the context of the sentences with respect to the rest of the judgment, in a clear and visual way. The same exercise could be repeated with rhetorical roles such as \u2019fact\u2019 or \u2019background\u2019, for which classifiers have been built within a legal context [9].\n4.3. Summarisation\nThe automatic summarisation of judgments would allow a user to have immediate access to summaries of newly released judgments as well as judgments from lower courts, which typically do not have dedicated individuals to write manual summaries. Using machine learning techniques to generate these summarises in combination with a judgment publication system could also allow for the user to customise the summaries to their desired length, or with the contextual content they require.\nThe research undertaken by Hachey and Grover mentioned above fed into their automatic summarisation system, the SUM system [8]. They used the rhetorical role classifier in addition to a relevance classifier, which classified sentences as relevant or not. They selected the most relevant sentences from across the respective rhetorical roles to automatically create a summary reflecting the leading manually written UK judgment summaries. Ray et al. built on their research with the SUMO system, implementing a conditional random fields classifier to perform the rhetorical and relevance classifications [9].\nIn Figure 3, we show how a summary can be included when a user selects \u2019Show Judgment Information\u2019. Currently a limited number of judgments in the UK have publicly available manually-written summaries. By integrating an automatic case summarisation system we allow for users to quickly gain an understanding of key issues and outcome of all relevant cases instantaneously, rather than being limited to cases deemed legally significant or to cases from higher courts which have manually-written summaries. This eases the understanding gap that might otherwise exist, as well as the access gap to those who have access to paid-for subscriptions to obtain case summaries.\n5. Future Work\nThe proof of concept, MyJudgments, presented in this paper was built in a short amount of time with limited resources. It is intended to demonstrate the opportunities existing research has\n58\n\nFigure 3: Example judgment with an expanding overlay containing an overview of critical information about the judgment including citation statistics, abbreviations, rhetorical role categories and a summary.\nopened up for better access to public judgements. While our system is a solid starting point we recognise a number of potential improvements.\nIn the first instance, we would like to engage a group of users that includes those that have no legal training through to practising solicitors to understand the value that they would gain from the suggested features. The immediate barrier is the effort and resources required to undertake a software development project of this scale. However, the immediate aim should be a collaboration between researchers and those working on open access case law publication systems.\nExamples include a search functionality across the whole system that allows searching for judgment titles, neutral citations, free text, grounds pleaded and other rhetorical role categories in the judgment. Within the single-judgment view the user-experience could be improved by incorporating hyperlinks that jump to a specific part of the judgment such as the target of the action or a specific citation. Moreover, utilising linked data to allow for contextual queries and complex visualisations of legal information is a useful tool to provide a better understanding of the role of a single judgment with respect to the entire judgment landscape.\n6. Conclusions\nThere is a large amount of research in to the use of machine learning techniques on legal documents, whether for the goal of summarising judgments, identifying rhetorical roles, or\n59\n\nextracting entities. Often it is suggested that these methods could help increase accessibility of judgments, levelling the field between legal experts and the average citizen. However, there has been little work done to demonstrate how this might be possible.\nIn this paper we explained how the UK\u2019s new judgment publishing pipeline Find Case Law was developed, including the core annotators and the use of LegalDocML. We demonstrated how the work done to develop Find Case Law can be exposed to the end-user with little development effort with our system MyJudgments, which has been made publicly available on Github 7. We also reviewed existing machine learning research undertaken on UK judgment data, demonstrating where the current state of UK legal ML research. We suggest that collaboration between researchers and public case law publication platform providers would improve the access to justice gap in the UK. Integrating various lines of machine learning research into a case law publication platform would provide greater insight into judgments in the UK, ensuring that citizens have clearer transparency of the inner workings of the judicial system.\nReferences\n[1] The European Commission, The 2018 EU Justice Scoreboard, The European Commission, 2018. URL: https://ec.europa.eu/info/sites/default/files/justice_scoreboard_2018_en.pdf.\n[2] D. Hoadley, J. Tomlinson, E. Nemsic, C. Somers-Joce, How public is public law? the current state of open access to administrative court judgments, Judicial Review (2022). URL: https://doi.org/10.1080/10854681.2022.2111966.\n[3] D. Hoadley, Part 2: Open access to english case law (the gaps), http://carrefax.com/ articles-blog/2018/5/9/part-2-open-access-to-english-case-law-knackered-plumbing, 2018. Accessed: 2022-08-10.\n[4] D. N. Byrom, Digital Justice: HMCTS data strategy and delivering access to justice, The Legal Education Foundation, 2019. URL: https://assets.publishing.service.gov.uk/government/ uploads/system/uploads/attachment_data/file/835778/DigitalJusticeFINAL.PDF.\n[5] Boost for open justice as court judgments get new home, https://www.gov.uk/government/ news/boost-for-open-justice-as-court-judgments-get-new-home, 2021. Accessed: 202207-17.\n[6] E. Filtz, S. Kirrane, A. Polleres, The linked legal data landscape: linking legal data across different countries, Artificial Intelligence and Law 29 (2021) 485\u2013539.\n[7] M. P. Fabio Vitali, V. Parisse, Akoma ntoso naming convention version 1.0, 2019. URL: http: //docs.oasis-open.org/legaldocml/akn-nc/v1.0/akn-nc-v1.0.html, last accessed 8 August 2022.\n[8] B. Hachey, C. Grover, Extractive summarisation of legal texts, Artificial Intelligence and Law 14 (2006) 305\u2013345.\n[9] O. Ray, A. Conroy, R. Imansyah, Summarisation with majority opinion., in: JURIX, 2020, pp. 247\u2013250.\n[10] D. Locke, G. Zuccon, Towards automatically classifying case law citation treatment using neural networks, in: Proceedings of the 24th Australasian Document Computing Symposium, 2019, pp. 1\u20138.\n7https://github.com/mdrresearch/MyJudgments\n60\n\n[11] A. Kanapala, S. Pal, R. Pamula, Text summarization from legal documents: a survey, Artificial Intelligence Review 51 (2019) 371\u2013402.\n[12] J. Valvoda, O. Ray, K. Satoh, Using agreement statements to identify majority opinion in ukhl case law, in: Legal Knowledge and Information Systems, IOS Press, 2018, pp. 141\u2013150.\n[13] J. S. T. Howe, L. H. Khang, I. E. Chai, Legal area classification: A comparative study of text classifiers on singapore supreme court judgments, arXiv preprint arXiv:1904.06470 (2019).\n[14] J. Gibson, R. Patel, C. Paskell, C. Peto, Justice data matters: Building a public mandate for court data use, https://research.thelegaleducationfoundation.org/wp-content/uploads/ 2022/07/Justice-Data-Matters-Report-Final-.pdf, 2022. Accessed: 2022-08-09.\n[15] M. Honnibal, I. Montani, S. Van Landeghem, A. Boyd, spacy: Industrial-strength natural language processing in python (2020). URL: https://doi.org/10.5281/zenodo.1212303.\n[16] M. Neumann, D. King, I. Beltagy, W. Ammar, ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing, in: Proceedings of the 18th BioNLP Workshop and Shared Task, Association for Computational Linguistics, Florence, Italy, 2019, pp. 319\u2013327. URL: https://www.aclweb.org/anthology/W19-5034. doi:10.18653/v1/ W19-5034. arXiv:arXiv:1902.07669.\n[17] B. Hachey, C. Grover, A rhetorical status classifier for legal text summarisation, in: Text Summarization Branches Out, 2004, pp. 35\u201342.\n[18] P. Bhattacharya, S. Paul, K. Ghosh, S. Ghosh, A. Wyner, Deeprhole: deep learning for rhetorical role labeling of sentences in legal case documents, Artificial Intelligence and Law (2021) 1\u201338.\n61\n\n",
        "hash_id": "ecc15c1bdc6e9688a865e77b1e0e3304"
    },
    {
        "key": "BK4A9WC2",
        "version": 26,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/BK4A9WC2",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/BK4A9WC2",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/FZ8IT82V",
                "type": "application/json"
            },
            "enclosure": {
                "type": "text/html",
                "href": "https://api.zotero.org/groups/4848934/items/BK4A9WC2/file/view"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 27372,
                "username": "colarusso",
                "name": "David Colarusso",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/colarusso",
                        "type": "text/html"
                    }
                }
            }
        },
        "citation": "<span>Snapshot, https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data (last visited Nov 15, 2022).</span>",
        "fulltext": "Skip to main content \nThe Verge The Verge logo. \n \n    The Verge homepage The Verge The Verge logo. / \n    Tech / \n    Reviews / \n    Science / \n    Entertainment / \n    More Menu Expand  \n \nThe Verge The Verge logo. Menu Expand \n \n    Artificial Intelligence  \n \nThe scary truth about AI copyright is nobody knows what will happen next \nThe last year has seen a boom in AI models that create art, music, and code by learning from others\u2019 work. But as these tools become more prominent, unanswered legal questions could shape the future of the field.  \n \nBy James Vincent \nNov 15, 2022, 10:00 AM EST | 5 Comments / 6 New \nShare this story \n \nA collage comprised of faces, each in a unique art style over a dark background with abstract digital elements. \nIllustration: Max-o-matic \n \nGenerative AI has had a very good year. Corporations like Microsoft, Adobe, and GitHub are integrating the tech into their products; startups are raising hundreds of millions to compete with them; and the software even has cultural clout, with text-to-image AI models spawning countless memes. But listen in on any industry discussion about generative AI, and you\u2019ll hear, in the background, a question whispered by advocates and critics alike in increasingly concerned tones: is any of this actually legal? \n \nThe question arises because of the way generative AI systems are trained. Like most machine learning software, they work by identifying and replicating patterns in data. But because these programs are used to generate code, text, music, and art, that data is itself created by humans, scraped from the web and copyright protected in one way or another. \n \nFor AI researchers in the far-flung misty past (aka the 2010s), this wasn\u2019t much of an issue. At the time, state-of-the-art models were only capable of generating blurry, fingernail-sized black-and-white images of faces . This wasn\u2019t an obvious threat to humans. But in the year 2022, when a lone amateur can use software like Stable Diffusion to copy an artist\u2019s style in a matter of hours or when companies are selling AI-generated prints and social media filters that are explicit knock-offs of living designers, questions of legality and ethics have become much more pressing.  \n \nGenerative AI models are trained on copyright-protected data \u2014 is that legal? \n \nTake the case of Hollie Mengert, a Disney illustrator who found that her art style had been cloned as an AI experiment by a mechanical engineering student in Canada. The student downloaded 32 of Mengert\u2019s pieces and took a few hours to train a machine learning model that could reproduce her style. As Mengert told technologist Andy Baio, who reported the case : \u201cFor me, personally, it feels like someone\u2019s taking work that I\u2019ve done, you know, things that I\u2019ve learned \u2014 I\u2019ve been a working artist since I graduated art school in 2011 \u2014 and is using it to create art that that [sic] I didn\u2019t consent to and didn\u2019t give permission for.\u201d  \n \nBut is that fair? And can Mengert do anything about it?  \n \nTo answer these questions and understand the legal landscape surrounding generative AI, The Verge spoke to a range of experts, including lawyers, analysts, and employees at AI startups. Some said with confidence that these systems were certainly capable of infringing copyright and could face serious legal challenges in the near future. Others suggested, equally confident, that the opposite was true: that everything currently happening in the field of generative AI is legally above board and any lawsuits are doomed to fail.  \n \n\u201cI see people on both sides of this extremely confident in their positions, but the reality is nobody knows,\u201d Baio, who\u2019s been following the generative AI scene closely, told The Verge . \u201cAnd anyone who says they know confidently how this will play out in court is wrong.\u201d \n \nAndres Guadamuz, an academic specializing in AI and intellectual property law at the UK\u2019s University of Sussex, suggested that while there were many unknowns, there were also just a few key questions from which the topic\u2019s many uncertainties unfold. First, can you copyright the output of a generative AI model, and if so, who owns it? Second, if you own the copyright to the input used to train an AI, does that give you any legal claim over the model or the content it creates? Once these questions are answered, an even larger one emerges: how do you deal with the fallout of this technology? What kind of legal restraints could \u2014 or should \u2014 be put in place on data collection? And can there be peace between the people building these systems and those whose data is needed to create them? \n \nLet\u2019s take these questions one at a time. \nTwo images of the Mona Lisa each in a different art style, one classical, the other more modern abstract with vibrant colors. Two images of the Mona Lisa each in a different art style, one classical, the other more modern abstract with vibrant colors. \nIllustration: Max-o-matic \nThe output question: can you copyright what an AI model creates? \n \nFor the first query, at least, the answer is not too difficult. In the US, there is no copyright protection for works generated solely by a machine. However, it seems that copyright may be possible in cases where the creator can prove there was substantial human input. \n \nIn September, the US Copyright Office granted a first-of-its-kind registration for a comic book generated with the help of text-to-image AI Midjourney. The comic is a complete work : an 18-page narrative with characters, dialogue, and a traditional comic book layout. And although it\u2019s since been reported that the USCO is reviewing its decision, the comic\u2019s copyright registration hasn\u2019t actually been rescinded yet. It seems that one factor in the review will be the degree of human input involved in making the comic. Kristina Kashtanova, the artist who created the work, told IPWatchdog that she had been asked by the USCO \u201cto provide details of my process to show that there was substantial human involvement in the process of creation of this graphic novel.\u201d (The USCO itself does not comment on specific cases.) \n \nAccording to Guadamuz, this will be an ongoing issue when it comes to granting copyright for works generated with the help of AI. \u201cIf you just type \u2018cat by van Gogh,\u2019 I don\u2019t think that\u2019s enough to get copyright in the US,\u201d he says. \u201cBut if you start experimenting with prompts and produce several images and start fine-tuning your images, start using seeds, and start engineering a little more, I can totally see that being protected by copyright.\u201d \n \nCopyrighting an AI model\u2019s output will likely depend on the degree of human involvement \n \nWith this rubric in mind, it\u2019s likely that the vast majority of the output of generative AI models cannot be copyright protected. They are generally churned out en masse with just a few keywords used as a prompt. But more involved processes would make for better cases. These might include controversial pieces, like the AI-generated print that won a state art fair competition . In this case, the creator said he spent weeks honing his prompts and manually editing the finished piece, suggesting a relatively high degree of intellectual involvement.  \n \nGiorgio Franceschelli, a computer scientist who\u2019s written on the problems surrounding AI copyright, says measuring human input will be \u201cespecially true\u201d for deciding cases in the EU. And in the UK \u2014 the other major jurisdiction of concern for Western AI startups \u2014 the law is different yet again. Unusually, the UK is one of only a handful of nations to offer copyright for works generated solely by a computer , but it deems the author to be \u201cthe person by whom the arrangements necessary for the creation of the work are undertaken.\u201d Again, there\u2019s room for multiple readings (would this \u201cperson\u201d be the model\u2019s developer or its operator?), but it offers precedence for some sort of copyright protection to be granted.  \n \nUltimately, though, registering copyright is only a first step, cautions Guadamuz. \u201cThe US copyright office is not a court,\u201d he says. \u201cYou need registration if you\u2019re going to sue someone for copyright infringement, but it\u2019s going to be a court that decides whether or not that\u2019s legally enforceable.\u201d \nTwo images of the Marilyn Diptych each in a different art style. Two images of the Marilyn Diptych each in a different art style. \nIllustration: Max-o-matic \nThe input question: can you use copyright-protected data to train AI models? \n \nFor most experts, the biggest questions concerning AI and copyright relate to the data used to train these models. Most systems are trained on huge amounts of content scraped from the web; be that text, code, or imagery. The training dataset for Stable Diffusion, for example \u2014 one of the biggest and most influential text-to-AI systems \u2014 contains billions of images scraped from hundreds of domains ; everything from personal blogs hosted on WordPress and Blogspot to art platforms like DeviantArt and stock imagery sites like Shutterstock and Getty Images. Indeed, training datasets for generative AI are so vast that there\u2019s a good chance you\u2019re already in one (there\u2019s even a website where you can check by uploading a picture or searching some text ). \n \nThe justification used by AI researchers, startups, and multibillion-dollar tech companies alike is that using these images is covered (in the US, at least) by fair use doctrine , which aims to encourage the use of copyright-protected work to promote freedom of expression.  \n \nWhen deciding if something is fair use, there are a number of considerations, explains Daniel Gervais, a professor at Vanderbilt Law School who specializes in intellectual property law and has written extensively on how this intersects with AI. Two factors, though, have \u201cmuch, much more prominence,\u201d he says. \u201cWhat\u2019s the purpose or nature of the use and what\u2019s the impact on the market.\u201d In other words: does the use-case change the nature of the material in some way (usually described as a \u201ctransformative\u201d use), and does it threaten the livelihood of the original creator by competing with their works?  \n \nTraining a generative AI on copyright-protected data is likely legal, but you could use that same model in illegal ways \n \nConsidering the onus placed on these factors, Gervais says \u201cit is much more likely than not\u201d that training systems on copyrighted data will be covered by fair use. But the same cannot necessarily be said for generating content. In other words: you can train an AI model using other people\u2019s data, but what you do with that model might be infringing. Think of it as the difference between making fake money for a movie and trying to buy a car with it.  \n \nConsider the same text-to-image AI model deployed in different scenarios. If the model is trained on many millions of images and used to generate novel pictures, it\u2019s extremely unlikely that this constitutes copyright infringement. The training data has been transformed in the process, and the output does not threaten the market for the original art. But, if you fine-tune that model on 100 pictures by a specific artist and generate pictures that match their style, an unhappy artist would have a much stronger case against you.  \n \n\u201cIf you give an AI 10 Stephen King novels and say, \u2018Produce a Stephen King novel,\u2019 then you\u2019re directly competing with Stephen King. Would that be fair use? Probably not,\u201d says Gervais. \n \nCrucially, though, between these two poles of fair and unfair use, there are countless scenarios in which input, purpose, and output are all balanced differently and could sway any legal ruling one way or another.  \n \nRyan Khurana, chief of staff at generative AI company Wombo, says most companies selling these services are aware of these differences. \u201cIntentionally using prompts that draw on copyrighted works to generate an output [...] violates the terms of service of every major player,\u201d he told The Verge over email. But, he adds, \u201cenforcement is difficult,\u201d and companies are more interested in \u201ccoming up with ways to prevent using models in copyright violating ways [...] than limiting training data.\u201d This is particularly true for open-source text-to-image models like Stable Diffusion, which can be trained and used with zero oversight or filters. The company might have covered its back, but it could also be facilitating copyright-infringing uses. \n \nAnother variable in judging fair use is whether or not the training data and model have been created by academic researchers and nonprofits. This generally strengthens fair use defenses and startups know this. So, for example, Stability AI, the company that distributes Stable Diffusion, didn\u2019t directly collect the model\u2019s training data or train the models behind the software. Instead, it funded and coordinated this work by academics and the Stable Diffusion model is licensed by a German university . This lets Stability AI turn the model into a commercial service (DreamStudio) while keeping legal distance from its creation. \n \nBaio has dubbed this practice \u201c AI data laundering .\u201d He notes that this method has been used before with the creation of facial recognition AI software, and points to the case of MegaFace, a dataset compiled by researchers from the University of Washington by scraping photos from Flickr. \u201cThe academic researchers took the data, laundered it, and it was used by commercial companies,\u201d says Baio. Now, he says, this data \u2014 including millions of personal pictures \u2014 is in the hands of \u201c[facial recognition firm] Clearview AI and law enforcement and the Chinese government.\u201d Such a tried-and-tested laundering process will likely help shield the creators of generative AI models from liability as well. \n \nThere\u2019s a last twist to all this, though, as Gervais notes that the current interpretation of fair use may actually change in the coming months due to a pending Supreme Court case involving Andy Warhol and Prince . The case involves Warhol\u2019s use of photographs of Prince to create artwork. Was this fair use, or is it copyright infringement? \n \n\u201cThe Supreme Court doesn\u2019t do fair use very often, so when they do, they usually do something major. I think they\u2019re going to do the same here,\u201d says Gervais. \u201cAnd to say anything is settled law while waiting for the Supreme Court to change the law is risky.\u201d \nTwo images of Keith Haring\u2019s \u201cSkateboarders\u201d each in a different art style. Two images of Keith Haring\u2019s \u201cSkateboarders\u201d each in a different art style. \nIllustration: Max-o-matic \nHow can artists and AI companies make peace? \n \nEven if the training of generative AI models is found to be covered by fair use, that will hardly solve the field\u2019s problems. It won\u2019t placate the artists angry their work has been used to train commercial models, nor will it necessarily hold true across other generative AI fields, like code and music. With this in mind, the question is: what remedies can be introduced, technical or otherwise, to allow generative AI to flourish while giving credit or compensation to the creators whose work makes the field possible? \n \nThe most obvious suggestion is to license the data and pay its creators. For some, though, this will kill the industry. Bryan Casey and Mark Lemley, authors of \u201c Fair Learning ,\u201d a legal paper that has become the backbone of arguments touting fair use for generative AI, say training datasets are so large that \u201cthere is no plausible option simply to license all of the underlying photographs, videos, audio files, or texts for the new use.\u201d Allowing any copyright claim, they argue, is \u201ctantamount to saying, not that copyright owners will get paid, but that the use won\u2019t be permitted at all.\u201d Permitting \u201cfair learning,\u201d as they frame it, not only encourages innovation but allows for the development of better AI systems. \n \nOthers, though, point out that we\u2019ve already navigated copyright issues of comparable scale and complexity and can do so again. A comparison invoked by several experts The Verge spoke to was the era of music piracy, when file-sharing programs were built on the back of massive copyright infringement and prospered only until there were legal challenges that led to new agreements that respected copyright.  \n \n\u201cSo, in the early 2000s, you had Napster, which everybody loved but was completely illegal. And today, we have things like Spotify and iTunes,\u201d Matthew Butterick, a lawyer currently suing companies for scraping data to train AI models, told The Verge earlier this month . \u201cAnd how did these systems arise? By companies making licensing deals and bringing in content legitimately. All the stakeholders came to the table and made it work, and the idea that a similar thing can\u2019t happen for AI is, for me, a little catastrophic.\u201d \n \nCompanies and researchers are already experimenting with ways to compensate creators \n \nWombo\u2019s Ryan Khurana predicted a similar outcome. \u201cMusic has by far the most complex copyright rules because of the different types of licensing, the variety of rights-holders, and the various intermediaries involved,\u201d he told The Verge . \u201cGiven the nuances [of the legal questions surrounding AI], I think the entire generative field will evolve into having a licensing regime similar to that of music.\u201d \n \nOther alternatives are also being trialled. Shutterstock, for example, says it plans to set up a fund to compensate individuals whose work it\u2019s sold to AI companies to train their models, while DeviantArt has created a metadata tag for images shared on the web that warns AI researchers not to scrape their content . (At least one small social network, Cohost, has already adopted the tag across its site and says if it finds that researchers are scraping its images regardless, it \u201cwon\u2019t rule out legal action.\u201d) These approaches, though, have met with mixed from artistic communities. Can one-off license fees ever compensate for lost livelihood? And how does a no-scraping tag deployed now help artists whose work has already been used to train commercial AI system? \n \nFor many creators it seems the damage has already been done. But AI startups are at least suggesting new approaches for the future. One obvious step forward is for AI researchers to simply create databases where there is no possibility of copyright infringement \u2014 either because the material has been properly licensed or because it\u2019s been created for the specific purpose of AI training. One such example is \u201c The Stack\u201d \u2014 a dataset for training AI designed to specifically avoid accusations of copyright infringement. It includes only code with the most permissive possible open-source licensing and offers developers an easy way to remove their data on request. Its creators say their model could be used throughout the industry.  \n \n\u201cThe Stack\u2019s approach can absolutely be adapted to other media,\u201d Yacine Jernite, Machine Learning & Society lead at Hugging Face, which helped create The Stack in collaboration with partner ServiceNow, told The Verge . \u201cIt is an important first step in exploring the wide range of mechanisms that exist for consent \u2014 mechanisms that work at their best when they take the rules of the platform that the AI training data was extracted from into account.\u201d Jernite says Hugging Face wants to help create a \u201cfundamental shift\u201d in how the creators are treated by AI researchers. But so far, the company\u2019s approach remains a rarity. \nWhat happens next? \n \nRegardless of where we land on these legal questions, the various actors in the generative AI field are already gearing up for\u2026 something. The companies making millions from this tech are entrenching themselves: repeatedly declaring that everything they\u2019re doing is legal (while presumably hoping no one actually challenges this claim). On the other side of no man\u2019s land, copyright holders are staking out their own tentative positions without quite committing themselves to action. Getty Images recently banned AI content because of the potential legal risk to customers (\u201cI don\u2019t think it\u2019s responsible. I think it could be illegal,\u201d CEO Craig Peters told The Verge last month) while music industry trade org RIAA declared that AI-powered music mixers and extractors are infringing members\u2019 copyright (though they didn\u2019t go so far as to launch any actual legal challenges).  \n \nThe first shot in the AI copyright wars has already been fired, though, with the launch last week of a proposed class action lawsuit against Microsoft, GitHub, and OpenAI . The case accuses all three companies of knowingly reproducing open-source code through the AI coding assistant, Copilot, but without the proper licenses. Speaking to The Verge last week, the lawyers behind the suit said it could set a precedent for the entire generative AI field (though other experts disputed this, saying any copyright challenges involving code would likely be separate from those involving content like art and music). \n \n\u201cOnce someone breaks cover, though, I think the lawsuits are going to start flying left and right.\u201d \n \nGuadamuz and Baio, meanwhile, both say they\u2019re surprised there haven\u2019t been more legal challenges yet. \u201cHonestly, I am flabbergasted,\u201d says Guadamuz. \u201cBut I think that\u2019s in part because these industries are afraid of being the first one [to sue] and losing a decision. Once someone breaks cover, though, I think the lawsuits are going to start flying left and right.\u201d \n \nBaio suggested one difficulty is that many people most affected by this technology \u2014 artists and the like \u2014 are simply not in a good position to launch legal challenges. \u201cThey don\u2019t have the resources,\u201d he says. \u201cThis sort of litigation is very expensive and time-consuming, and you\u2019re only going to do it if you know you\u2019re going to win. This is why I\u2019ve thought for some time that the first lawsuits around AI art will be from stock image sites. They seem poised to lose the most from this technology, they can clearly prove that a large amount of their corpus was used to train these models, and they have the funding to take it to court.\u201d \n \nGuadamuz agrees. \u201cEveryone knows how expensive it\u2019s going to be,\u201d he says. \u201cWhoever sues will get a decision in the lower courts, then they will appeal, then they will appeal again, and eventually, it could go all the way to the Supreme Court.\u201d \nJoin the conversation 5 / 6 New \nMost Popular \n \n    Elon Musk says he fired engineer who corrected him on Twitter \n    Amazon mass layoffs will reportedly ax 10,000 people this week \n    Elon Musk ignored Twitter\u2019s internal warnings about his paid verification scheme \n    Here\u2019s why Elizabeth Holmes thinks she shouldn\u2019t go to prison \n    Meta Quest Pro review: get me out of here \n \nVerge Deals \n \n/ Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. \nEmail (required) Sign up \nBy submitting your email, you agree to our Terms and Privacy Notice . This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply. \nMore from Artificial Intelligence \n \n    A search box on top of a human brain A search box on top of a human brain \n    Your own personal Google: how Mem plans to reinvent note-taking apps with AI \n    A black KFC logo on a white background A black KFC logo on a white background \n    KFC blames its bot for promoting its cheese-covered chicken on Kristallnacht \n    A screenshot of Canva\u2019s text-to-image tool showing an AI generated image of a red panda and the options to portray it in different styles. A screenshot of Canva\u2019s text-to-image tool showing an AI generated image of a red panda and the options to portray it in different styles. \n    Design platform Canva launches text-to-image AI feature \n    Image of an Nvidia A100 card. Image of an Nvidia A100 card. \n    Nvidia\u2019s selling a nerfed GPU in China to get around export restrictions \n \nToday\u2019s Storystream \n \nFeed refreshed Two hours ago \u2022 Ugly sweater \nCrypto \nSam Bankman-Fried oversaw FTX\u2019s meltdown, and the fallout is reaching DC \nElizabeth Lopatto Two hours ago \nCapitol Hill \nGadgets \nLego\u2019s new Eiffel Tower set is taller than an average seven-year-old child \nAntonio G. Di Benedetto Two hours ago \nA five-foot tall Lego version of the Eiffel Tower, sitting on a table in a bright and airy living room. \nTech \nTaylor Swift crashed Ticketmaster \nJay Peters Two hours ago \nMTV EMAs 2022 - Show \nTwitter \nTwitter might tragically stop telling you what phone (or fridge) tweets are coming from \nJon Porter Two hours ago \nElon Musk in front of the Twitter logo. \nTranspo \nRimac Nevera hits top speed of 256 mph to become world\u2019s fastest production EV \nAndrew J. Hawkins Two hours ago \nRimac Nevera setting the world record for EV production speed \nNASA \nNASA\u2019s Space Launch System rocket weathers the storm \nGeorgina Torbet 11:10 AM EST \nNASA Prepares For Artemis Moon Mission Launch \nMust Reads \nMust Reads \nMust Reads \n \n    Phil Spencer really wants you to know that native Call of Duty will stay on PlayStation \n \n    Nilay Patel Nov 15 \n    The scary truth about AI copyright is nobody knows what will happen next \n \n    James Vincent Nov 15 \n    Tesla on trial: Autopilot and Elon Musk\u2019s $56 billion pay package under scrutiny in separate cases \n \n    Andrew J. Hawkins Nov 14 \n    Aqara\u2019s new smart feeder can give your pet control of its bowl \n \n    Jennifer Pattison Tuohy Nov 15 \n    NASA\u2019s Space Launch System rocket weathers the storm \n \n    Georgina Torbet Nov 15 \n \nTech \nHow to watch NASA\u2019s Artemis I SLS megarocket launch \nEmma Roth  and  Mary Beth Griggs 11:08 AM EST \nArtemis I Rollout \nR \nExternal Link \nRussell Brandom 11:00 AM EST \nLink \nWhat if Q made a drop and nobody noticed? \n \nThat\u2019s what happened last week, as Vice\u2019s Dan Gilbert highlights the muted reaction to last week\u2019s four separate drops from the mystery conspiracist. Even within the world of Qanon, the response has been muted at best. \n \nIt\u2019s not that people are coming to their senses exactly; they\u2019ve just stopped caring about some rando on 8kun. \n \nQ is Dead, Long Live QAnon \n \n[vice] \nGadgets \nRazer is upgrading the 2022 Blade 14\u2019s two USB-C ports to USB 4 \nCameron Faulkner 11:00 AM EST \nThe Razer Blade 14 gaming laptop sits in a black space, opened to the view to show off its 14-inch screen. \nReviews \nAsus\u2019 Steven Harrington Vivobook is a cool-looking laptop with uncool battery life \nMonica Chin 11:00 AM EST \nThe Asus Vivobook 13 Slate in laptop mode with the stylus to the right side. \nGadgets \nMeross\u2019 new smart plug is one of the first Matter devices you can buy \nJennifer Pattison Tuohy 10:42 AM EST \nA lamp plugged into a smart plug next to three smart speakers. \nGaming \nEpic says more than half of all announced next-gen games are made on Unreal Engine \nJay Peters 10:30 AM EST \nA promotional Unreal Engine screenshot showing a car in a brightly lit city. \nTech \nTech See all Tech \nTech \n \n    TP-Link is going straight to Wi-Fi 7 with its latest generation of routers \n \n    Umar Shakir Nov 14 \n    TP-Link is going straight to Wi-Fi 7 with its latest generation of routers \n    Sam Bankman-Fried oversaw FTX\u2019s meltdown, and the fallout is reaching DC \n \n    Elizabeth Lopatto Nov 15 \n    Lego\u2019s new Eiffel Tower set is taller than an average seven-year-old child \n \n    Antonio G. Di Benedetto Nov 15 \n \nA \nExternal Link \nAndrew J. Hawkins 10:23 AM EST \nLink \nThe future of Bird\u2019s electric scooter operation is in doubt. \n \nBird issued a \u201cgoing concern\u201d warning last night, disclosing that it may not have enough money to keep going for another 12 months. The warning came hours after the scooter company admitted to the SEC that it had overstated its revenue for two years and that its financial reports from 2020 and 2021 can \u201c no longer be relied upon ,\u201d And its stock price is still trading under $1-per-share, putting Bird at risk of being delisted. What a crummy week for the scooter sharing pioneer. \n \nBird Announces Third Quarter 2022 Financial Results \n \n[Bird Global, Inc.] \nApple \nSteve Jobs\u2019 raggedy old sandals just sold for $200,000 at an auction \nEmma Roth 10:22 AM EST \nA pair of worn-out Birkenstock sandals \nA collage comprised of faces, each in a unique art style over a dark background with abstract digital elements. \nThe scary truth about AI copyright is nobody knows what will happen next \n \nThe AI copyright wars are coming \nJames Vincent 10:00 AM EST \nDeals \nYou can save $50 on the Xbox Series S \nSheena Vasani 9:58 AM EST \nThe white Xbox Series S lying horizontally. \nDecoder \nPhil Spencer really wants you to know that native Call of Duty will stay on PlayStation \nNilay Patel 9:41 AM EST \nPhil Spencer smiles at the camera. \nR \nExternal Link \nRussell Brandom 9:36 AM EST \nLink \nThe next billion dollars. \n \nMukesh Ambani isn\u2019t a household name in the US, but he\u2019s one of the richest and most powerful people in India\u2019s growing tech scene. \n \nIn a series of annotated illustrations, Rest of World lays out how a single man ended up with major stakes in three video-streaming apps, two shopping platforms, a phone, and more than a dozen different retail ventures. \n \nWelcome to the Ambaniverse: How India\u2019s billionaire dominates every aspect of life \n \n[Rest of World] \nPodcasts \nPodcasts See more Podcasts \nPodcasts \n \n    Phil Spencer really wants you to know that native Call of Duty will stay on PlayStation \n \n    Nilay Patel Nov 15 \n    Today on the Vergecast: Twitter chaos, Meta chaos, crypto chaos, and did we mention all the chaos? Oh, and also, some chaos. \n \n    David Pierce Nov 11 \n    Today on The Vergecast: Bias busters, free speech, and CHIPS! \n \n    David Pierce Nov 9 \n    Why Figma is selling to Adobe for $20 billion, with CEO Dylan Field \n \n    Nilay Patel Nov 8 \n    Today on The Vergecast: the great \u201cwill you pay for Twitter?\u201d debate. \n \n    David Pierce Nov 4 \n \nR \nExternal Link \nRichard Lawler 9:24 AM EST \nLink \nWho looks at FTX and sees an investment opportunity? \n \nSure, it might sound like a bad idea to invest in a bankrupt exchange with more accusations of fraud than anything else we\u2019ve seen on this side of Enron, but the Wall Street Journal reports Sam Bankman-Fried is asking around anyway. \n \n    Mr. Bankman-Fried, alongside a few remaining employees, spent the past weekend calling around in search of commitments from investors to plug a shortfall of up to $8 billion in the hopes of repaying FTX\u2019s customers, the people said.  \n \n    In Mr. Bankman-Fried\u2019s case, the funds aren\u2019t meant to sustain a bare-bones staff, but to repay individual traders and institutional clients who have been unable to get funds out, the people said. \n \n \nWSJ News Exclusive | FTX Founder Sam Bankman-Fried Attempts to Raise Fresh Cash Despite Bankruptcy \n \n[WSJ] \nTech \nDescript\u2019s revamped video editing Storyboard feature launches for all users \nJames Vincent 9:00 AM EST \nA screenshot of the new Storyboard UI. \nMicrosoft \nClippy is the star of Microsoft\u2019s latest ugly sweater \nTom Warren 9:00 AM EST \nClippy is Microsoft\u2019s latest ugly sweater \nGadgets \nWyze is getting into the mesh router business \nJon Porter 9:00 AM EST \nTwo Wyze Mesh Router Pro routers. \nReviews \nNvidia RTX 4080 review: performance, for a price \nTom Warren 9:00 AM EST \nNvidia\u2019s new RTX 4080 is as big as an RTX 4090 \nStreaming \nElgato\u2019s new Stream Deck joins the knob mob \nAlice Newcome-Beill 9:00 AM EST \nA stock photo of the Elgato Stream Deck Plus \nMost Popular \nMost Popular \nMost Popular \n \n    Elon Musk says he fired engineer who corrected him on Twitter \n \n    Mitchell Clark Nov 14 \n    Amazon mass layoffs will reportedly ax 10,000 people this week \n \n    Chris Welch Nov 14 \n    Elon Musk ignored Twitter\u2019s internal warnings about his paid verification scheme \n \n    Casey Newton Nov 15 \n    Here\u2019s why Elizabeth Holmes thinks she shouldn\u2019t go to prison \n \n    Emma Roth Nov 14 \n    Meta Quest Pro review: get me out of here \n \n    Adi Robertson Nov 11 \n \nR \nQuote \nRichard Lawler 8:54 AM EST \nLink \nGreen light. \n \nTwitter isn\u2019t always fully operational, but rumors about what features are shut down aren\u2019t holding up everywhere, based on what we\u2019ve seen so far, but if you\u2019re having problems, then let us know . \n \nBut first, a few stories you might have otherwise missed. \n \n    Now your new iPhone can connect to satellites . \n \n    Elon Musk\u2019s two big Tesla trials that are starting this week . \n \n    Google\u2019s nearly $400 million location tracking settlemen t with 40 states, \n \n    Amazon is likely the next tech company to do mass layoffs. \n \n    The Witcher 3\u2019s big next-gen patch is a month away . \n \n \nJ \nJames Vincent 8:12 AM EST \nLink \nNot for the first time this year, the UK government is in discord. \n \nI mean, actually in it. The government\u2019s finance ministry, HM Treasury, set up a Discord server today to share news and updates . The server is read-only, of course, but that hasn\u2019t stopped the hoi polloi making their feelings known in, uh, more creative ways. \n \nPerson holding iPhone 14 Pro where the dynamic island shows satellite connectivity \nHere\u2019s what it\u2019s like to use Apple\u2019s Emergency SOS via satellite \n \nYou don\u2019t have to hold your arm up to get a signal, though there is a lot of turning around and waiting when trying to connect to satellites. \nVictoria Song 8:00 AM EST \nMicrosoft \nMicrosoft\u2019s Xbox chief settles the Call of Duty PlayStation debate once and for all \nTom Warren 7:16 AM EST \nGhost, the main character in Call of Duty: Modern Warfare II, stands in a field \nGadgets \nAqara\u2019s new smart feeder can give your pet control of its bowl \nJennifer Pattison Tuohy 7:00 AM EST \nCat eating from an automatic pet feeder. \nGadgets \nThe Garmin Bounce is an LTE smartwatch to help parents track their kids \nVictoria Song 7:00 AM EST \nThe green, black, and lilac versions of the Garmin Bounce on a wood table. \nReviews \nReviews See all Reviews \nReviews \n \n    Meta Quest Pro review: get me out of here \n \n    Adi Robertson Nov 11 \n    Meta Quest Pro review: get me out of here \n    Asus\u2019 Steven Harrington Vivobook is a cool-looking laptop with uncool battery life \n \n    Monica Chin Nov 15 \n    Nvidia RTX 4080 review: performance, for a price \n \n    Tom Warren Nov 15 \n \nAmazon \nAmazon Clinic launches as a message-based virtual care service \nJess Weatherbed 6:50 AM EST \nAn illustration of a person sitting on a park bench. They are messaging a healthcare provider on their mobile device. \nJ \nTwitter \nJess Weatherbed 6:30 AM EST \nLink \nOn this day in 1971, Intel created the world\u2019s first microprocessor. \n \nDespite being founded just three years prior, Intel rocked the tech world with the 4004 microprocessor, a complete general-purpose CPU on a single chip that allowed small machines to perform the same calculations as much larger systems. \n \nJust four engineers are credited with the majority of its development , which formed the base of what eventually became the first personal computers. \n \nTwitter \nBuying ads on Twitter is \u2018high-risk\u2019 according to the world\u2019s biggest ad agency \nMitchell Clark Nov 14 \nIllustration of a black Twitter bird in front of a red and white background. \nPlatformer \nElon Musk ignored Twitter\u2019s internal warnings about his paid verification scheme \nCasey Newton  and  Zoe Schiffer Nov 14 \nElon Musk illustration \nTwitter \nElon Musk says he fired engineer who corrected him on Twitter \nMitchell Clark Nov 14 \nElon Musk in front of the Twitter logo. \nN \nExternal Link \nNilay Patel Nov 14 \nLink \n\u201cThey\u2019re all a bunch of cowards.\u201d \n \nFired Twitter engineer Eric Frohnhoefer talks to Forbes, revealing that he discovered he was fired via Elon tweet and his laptop getting locked down; no one from the company actually called him. \n \n    \u201cNo one trusts anyone within the company anymore,\u201d he said. \u201cHow can you function? Employees don\u2019t trust the new management. Management doesn\u2019t trust the employees. How do you think you\u2019re supposed to get anything done? That\u2019s why there\u2019s production freezes \u2013 you can\u2019t merge code, you can\u2019t turn things on without permission from VPs.\u201d \n \n \nTwitter Engineer Fired On Twitter Calls Musk\u2019s Team \u201cA Bunch Of Cowards\u201d \n \n[Forbes] \nScience \nScience See all Science \nScience \n \n    NASA\u2019s Artemis I launch is delayed again as Tropical Storm Nicole approaches \n \n    Mary Beth Griggs Nov 9 \n    NASA\u2019s Artemis I launch is delayed again as Tropical Storm Nicole approaches \n    NASA\u2019s Space Launch System rocket weathers the storm \n \n    Georgina Torbet Nov 15 \n    How to watch NASA\u2019s Artemis I SLS megarocket launch \n \n    Emma Roth Nov 15 \n \nM \nExternal Link \nMonica Chin Nov 14 \nLink \nAsk A Manager has weighed in. \n \nA nervous Twitter employee, whose boss and coworkers are now gone, has written to the popular work advice column A sk A Manager , asking how the heck they should handle their company\u2019s whole...situation. \n \nColumnist Alison Green\u2019s advice: Stick it out, if you can. \u201cStaying at least gives you the option of severance down the road...and gives you an ongoing income and health insurance,\u201d Green wrote. She added, \u201cI\u2019m sorry something you helped build is being needlessly destroyed.\u201d \n \nI work at Twitter ... what do I do? \n \n[Ask a Manager] \nCrypto \nNike is still trying to make NFTs happen with .Swoosh \nEmma Roth Nov 14 \nThe .Swoosh homepage \nS \nSean Hollister Nov 14 \nLink \nThe Tech Winter, visualized. \n \nThe mass layoffs at Twitter, Meta, and soon possibly Amazon are just the tip of the iceberg . \n \nTech job marketplace TrueUp.io has been keeping track \u2014 and it says over 183,000 people have already been hit by tech layoffs so far in 2022. You can see the details in the company\u2019s interactive tracker here . \n \nDataviz: TrueUp.io/layoffs \nR \nThe Verge \nRichard Lawler Nov 14 \nLink \nThe Pixel Watch companion app just got its first update. \n \nOne month after Google launched the Pixel Watch, its app is getting its first update. \n \nThe update is rolling out over the next few weeks, so you may not see it right away, but according to the support page , the November 2022 update has some bug fixes, plus improvements like bringing access Fitbit sync information and integration settings to the home screen. \n \nPixel Watch on top of a Pixel 7 and Pixel 7 Pro \nGoogle Pixel Watch review: it\u2019s a smarter Fitbit \nVictoria Song Oct 12 \nPagination \nMore Stories \nCreators \nCreators See all Creators \nCreators \n \n    The scary truth about AI copyright is nobody knows what will happen next \n \n    James Vincent Nov 15 \n    The scary truth about AI copyright is nobody knows what will happen next \n    Descript\u2019s revamped video editing Storyboard feature launches for all users \n \n    James Vincent Nov 15 \n    TikTok is testing its long-awaited in-app shopping feature \n \n    Mia Sato Nov 11 \n \nThe Verge The Verge logo. \n \n    Terms of Use \n    Privacy Notice \n    Cookie Policy \n    Do Not Sell My Personal Info \n    Licensing FAQ \n    Accessibility \n    Platform Status  \n \n    Contact \n    Tip Us \n    Community Guidelines \n    About \n    Ethics Statement  \n \nThe Verge is a vox media network \n \n    Advertise with us \n    Jobs @ Vox Media  \n \n\u00a9 2022 Vox Media , LLC. All Rights Reserved \n \n",
        "hash_id": "f4b5619599e45489b3c9473d4edce275"
    },
    {
        "key": "FZ8IT82V",
        "version": 24,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/FZ8IT82V",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/FZ8IT82V",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/BK4A9WC2",
                "type": "application/json",
                "attachmentType": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 27372,
                "username": "colarusso",
                "name": "David Colarusso",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/colarusso",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Vincent",
            "parsedDate": "2022-11-15",
            "numChildren": 1
        },
        "citation": "<span>James Vincent, <i>The scary truth about AI copyright is nobody knows what will happen next</i>, <span style=\"font-variant:small-caps;\">The Verge</span> (2022), https://www.theverge.com/23444685/generative-ai-copyright-infringement-legal-fair-use-training-data (last visited Nov 15, 2022).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "7MWIW2YW",
        "version": 22,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/7MWIW2YW",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/7MWIW2YW",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/VPHAX48D",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/7MWIW2YW/file/view",
                "title": "Funk - 1987 - The Paperwork Reduction Act Paperwork Reduction M.pdf",
                "length": 7513848
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://heinonline.org/HOL/PDFsearchable?handle=hein.journals/hjl24&#38;collection=journals&#38;section=6&#38;id=&#38;print=section&#38;sectioncount=1&#38;ext=.pdf&#38;nocover=&#38;display=0 (last visited Nov 14, 2022).</span>",
        "fulltext": "DATE DOWNLOADED: Mon Nov 14 09:45:59 2022 SOURCE: Content Downloaded from HeinOnline\nCitations:\nBluebook 21st ed. William F. Funk, The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law, 24 HARV. J. oN Legis. 1 (1987).\nALWD 7th ed. William F. Funk, The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law, 24 Harv. J. on Legis. 1 (1987).\nAPA 7th ed. Funk, W. F. (1987). The paperwork reduction act: paperwork reduction meets administrative law. Harvard Journal on Legislation, 24(1), 1-116.\nChicago 17th ed. William F. Funk, \"The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law,\" Harvard Journal on Legislation 24, no. 1 (Winter 1987): 1-116\nMcGill Guide 9th ed. William F. Funk, \"The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law\" (1987) 24:1 Harv J on Legis 1.\nAGLC 4th ed. William F. Funk, 'The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law' (1987) 24(1) Harvard Journal on Legislation 1\nMLA 9th ed. Funk, William F. \"The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law.\" Harvard Journal on Legislation, vol. 24, no. 1, Winter 1987, pp. 1-116. HeinOnline.\nOSCOLA 4th ed. William F. Funk, 'The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law' (1987) 24 Harv J on Legis 1\nProvided by: The Moakley Law Library at Suffolk University Law School\n-- Your use of this HeinOnline PDF indicates your acceptance of HeinOnline's Terms and Conditions of the license agreement available at https://heinonline.org/HOL/License\n-- The search text of this PDF is generated from uncorrected OCR text. -- To obtain permission to use this article beyond the scope of your license, please use:\nCopyright Information\n\nARTICLE\nTHE PAPERWORK REDUCTION ACT: PAPERWORK REDUCTION MEETS ADMINISTRATIVE LAW\nWILLIAM F. FUNK*\nIn the PapervorkReduction Act of 1980, Congress createdadministrative procedures aimed at decreasing federal papervork. Building upon earlier legislation, the Act centralized in the Office of Management and Budget (OMB) the review and clearance of all federal collections of informationfrom the generalpublic.In addition, the Act containeda \"Public Protection\"provision intended to provide a means of private enforcement of the Act.\nIn this Article, ProfessorFunk reviews the legislative history and implemnentation of the PaperworkReduction Act andargues that in practice the Act has severe shortcomings. He contends that, despite its title, the Act in fact provides OMB with significant power over the substantive programs of other agencies. Professor Funk also argues that notwithstanding the intent of the drafters the Public Protection provision is virtually useless. Finally, Professor Funk calls for an end to specialized legislation aimed at government papenvork. Instead, he argues, paperwork requirements should be evaluated by the same cost-benefit analysis used in reviewing all other government regulations.\n\nContents\n\nI. Background: The FederalReports Act .................\n\n7\n\nA. The Problem Perceived: The Legislative Histoty of\n\nthe FederalReports Act ...........................\n\n7\n\nB. Practice Under the FederalReports Act ............\n\n10\n\nC. Inadequaciesand the Regulatoty Agencies ..........\n\n13\n\nD. JudicialReview and the FederalReports Act ........ 16\n\nI. The Paperwork Comnmnission...........................\n\n20\n\nIII. The Information Collection Budget ....................\n\n22\n\n* Professor of Law, Lewis and Clark Law School. B.A., Harvard University, 1967;\nJ.D., Columbia University, 1973. This Article is based on a study prepared for the Administrative Conference of the United States. The author wishes to thank the numerous individuals who were interviewed in connection with that study, without whose memories and insight this Article could not have been written. In addition, the author wishes to thank Jeffrey Lubbers of the Administrative Conference for his support and suggestions as well as the number of people who provided valuable comments on an earlier draft of this Article submitted to the Rulemaking Committee of the Conference.\n\n2\n\nHarvardJournalon Legislation [Vol. 24:1\n\nIV. The PaperworkReduction Act ......................... 26\n\nA. Legislative Histoty ...............................\n\n26\n\nB. The Act Generally ................................\n\n30\n\nC. The ClearanceFunction........................... 33\n\nV. InitialImplementation................................\n\n36\n\nA. The Treasury-OMBDispute........................ 36\n\nB. The Justice Department Opinion ................... 40\n\nC. The OMB Regulations ............................ 50\n\nD. CongressionalReaction ...........................\n\n58\n\nE. The Inherent Conflict-ReportingRequirements\n\nUnder the Administrative ProcedureAct ............ 63\n\nVL The PublicProtectionProvision ....................... 70\n\n'A. Its Limitations ...................................\n\n70\n\nB. Other Basesfor JudicialReview ...................\n\n78\n\nVII. Centralized Oversight ................................\n\n82\n\nA. Background......................................\n\n82\n\nB. Executive Order 12,291 ............................\n\n85\n\nC. Review of PaperworkRegulations..................\n\n93\n\nD. OMB's Power To Determine the Need for\n\nInform ation ......................................\n\n97\n\nE. OMB Misuse of Its Legal Authority ................\n\n103\n\nVIII. Conclusion ..........................................\n\n110\n\nOften equated with government red tape,' and sometimes used as a symbol for government regulation generally, govern-\nment paperwork is as ubiquitous as government itself. The eleventh law passed by the First Congress of the United States\nimposed a paperwork requirement, the documentation of marine vessels. 2 And the Constitution itself mandates one of the largest\ngovernment paperwork requirements-the decennial census.' Government paperwork has generally been defined as the\ngovernment obtaining facts or opinions through the use of forms, questionnaires, reporting or recordkeeping requirements, or similar methods,4 and has traditionally been distinguished from individually targeted information gathering, as. is involved, for example, in criminal and civil investigations. Information gathering for statistical purposes is clearly encompassed within the\n\nThe original bill that became the Paperwork Reduction Act was entitled the Paperwork and Redtape Reduction Act. S. 1411, 96th Cong., Ist Sess. (1979).\n2 Act of Sept. 1, 1789, ch. 11, 1 Stat. 55 (1789). By 1980, 42 different forms were required to document a marine vessel. See Oversight of the Paperwork Reduction Act of 1980: HearingBefore the Subcomm. on Information Management and Regulatory Affairs of the Senate Comm. on Governmental Affairs, 98th Cong., 1st Sess. 5 (1983) (statement of Christopher DeMuth, Administrator for the Office of Information and Regulatory Affairs, Office of Management and Budget) [hereinafter 1983 Senate Over-\nsight Hearing]. 3U.S. CONST. art. I, \u00a7 2. 4 44 U.S.C. \u00a7 3502(3) (1982).\n\n19871\n\nPaperworkReduction Act\n\nconcept of government paperwork requirements. And while\n\npolls, studies, surveys, and censuses were once viewed as the\n\nmost burdensome of paperwork requirements, today statistical\n\ninformation gathering is only a small percentage of government\n\npaperwork. them to be\n\n5\na\n\nNevertheless,* many substantial burden. 6\n\npeople\n\ncontinue\n\nto\n\nperceive\n\nThe most burdensome form of government paperwork is tax\n\nreporting. While dissatisfaction stems in part from the subject\n\nmatter, and tax forms are often extensive and difficult to fill out,\n\ntax reporting scores at the top of every study of the most oner-\n\nous paperwork requirements. Another burdensome and contro-\n\nversial form of paperwork is that related to regulatory or com-\n\npliance activities. Into this category falls the vast array of\n\nreporting and recordkeeping requirements associated with en-\n\nvironmental, health and safety, and business regulatory activi-\n\nties. Some would extend the concept of government paperwork\n\neven to required product Commission registration\n\nalnadbedliinsgcloasnudreSerucluersi.ti7es\n\nand\n\nExchange\n\nDistinct from mandatory reporting and recordkeeping require-\n\nments such as those mentioned above, which cdtn usually be\n\nenforced with civil or criminal penalties in the case of willful\n\nrefusals, 8 are paperwork requirements that citizens must comply\n\nwith in order to obtain some government benefit. Even these\n\nrequirements, however, are usually mandatory if one wishes to\n\nobtain the benefit in.question. Within this category are appli-\n\ncations for various personal benefits-social security, Veterans\n\nAdministration, unemployment, disability, welfare-as well as\n\napplications for government grants and paperwork associated\n\n' U.S. OFFICE OF MANAGEMENT AND BUDGET, INFORMATION COLLECTION BUDGET\nOF THE UNITED STATES GOVERNMENT: FISCAL YEAR 1982 7 (1981) [hereinafter 1982\nICB]. 6 In polls reported by the Small Business Administration, censuses were viewed, after\ntax reporting, as constituting the greatest paperwork burden on small business. U.S.\nSMALL BUSINESS ADMINISTRATION, SMALL BUSINESS PAPERWORK: PROBLEMS AND\nPROGRESS 6 (1983). Even among larger businesses, census reporting is commonly a cause for industry to unite to oppose particular requirements. See, e.g., BUSINESS ADVISORY COUNCIL ON FEDERAL REPORTS, BACF REPORT 1-2 (June 1985)(describing comments made on a proposed Energy Information Administration Manufacturing Energy Consumption Survey (MECS) Form EIA 846).\n7Indeed, the Office of Management and Budget's [hereinafter OMB] regulations under the Paperwork Reduction Act would appear to reach so far. See 5 C.F.R. \u00a7 1320.7(c) (1985) (definition of \"collection of information\").\n8 See, e.g., 13 U.S.C. \u00a7\u00a7 221, 224 (1982) (fines for refusal to answer census questions); 26 U.S.C. \u00a7 7203 (1982) (criminal penalties for failure to file tax returns); 42 U.S.C. \u00a7.6928(d) (Supp. 11 1984) (criminal penalties for generators, transporters, and disposers of hazardous waste for failing to file required reports).\n\n4\n\nHarvardJournalon Legislation [Vol. 24:1\n\nwith government cootracts. In 1981, it was estimated that almost twenty percent of the paperwork reporting burden imposed by the government was attributable to reporting or recordkeeping required to obtain or retain a benefit.9\nDespite the inevitability of government paperwork, for almost\nhalf a century Congress has not only railed against it but also taken affirmative actions to limit or reduce it. 10 The most recent legislative action on the subject was the Paperwork Reduction Act of 1980.\" I Responding to the alleged $100 billion cost of federal paperwork 2 and the perceived inadequacies of existing law, 3 and riding a crest of public and political dissatisfaction with government bureaucracy generally, 4 Congress attempted\nto create a legislative scheme and administrative procedures by which unnecessary and burdensome paperwork requirements would be substantially diminished, if not eliminated.\nBuilding upon the then-existing Federal Reports Act's concept of an independent, supervisory agency reviewing the paperwork requirements of other agencies, the Paperwork Reduction Act assigned to the Office of Management and Budget\n(OMB) the review and clearance of all federal collections of information from the general public. 15 Moreover, the Paperwork Reduction Act was to clarify and codify an expansive reading\nof the paperwork requirements covered by the Federal Reports Act, explicitly covering recordkeeping and paperwork requirements contained in regulations. To facilitate enforcement of the Paperwork Reduction Act against agencies, Congress also created a \"public protection\" provision to shield persons from pen-\n\n9 1982 ICB, supra note 5, at 6. The same document estimated that a third of the government's reporting and recordkeeping requirements, excluding the Treasury De. partment, were required to obtain or retain a benefit.\n1oThe Federal Reports Act of 1942, Pub. L. No. 77-831, 56 Stat. 1078, was passed in 1942 and amended in 1973. Pub. L. No. 93-152, \u00a7 409, 87 Stat. 576 (1973).\n\" Pub. L. No. 96-511, 94 Stat. 2812 (1980) (codified at 44 U.S.C. \u00a7\u00a7 3501-20 (1982)). 12 COMMISSION OF FEDERAL PAPERWORK, FINAL SUMMARY REPORT I (Oct. 3, 1977)\n[hereinafter FINAL REPORT]. The Commission estimated that the $100 billion per year in paperwork costs were imposed as follows: on the federal government-43 billion;\non private industry-25-32 billion; on state and local government-$5-9 billion; on individuals-$8.7 billion; on farmers-350 million; and on labor organizations-\n$75 million. Id. at 5. 3 Id. at 8-9. The FINAL REPORT specified 770 recommendations for reducing govern.\nment paperwork. '4 The presidential election of 1976, as well as that of 1980, contained a substantial\nanti-Washington flavor. Candidate Carter campaigned against the federal bureaucracy and when in office undertook a number of initiatives designed to improve the responsiveness and effectiveness of the bureaucracy and to reduce the burdens of government regulation and paperwork. See, e.g., infra text accompanying notes 122-25, 141.\n\" 44 U.S.C. \u00a7 3503 (1982).\n\n1987]\n\nPaperwork Reduction Act\n\nalties imposed for failing to provide or maintain information, if the information requirement does not display a current OMB control number.16\nThe new powers granted to OMB to enable it to meet its paperwork reduction responsibilities, as well as the continuation\nin a new environment of certain powers created by the Federal\nReports Act, have raised a number of questions. Primary among them is the institutional role of OMB as central overseer of all\ngovernment information collection activities. While OMB's role as central overseer of agency regulations pursuant to presidential order 17 has been much discussed in the legal literature, 8 that role has been limited to one of reviewing and commenting\n\n16 Id. In a further effort to reduce government paperwork burdens, Congress required\nOMB to set goals for the reduction of specified percentages of the existing paperwork burden, id. \u00a7\u00a7 3505(1), 3505(2)(E), and required OMB to establish a Federal Information Locator System to help eliminate duplicative reporting and recordkeeping requirements. Id. \u00a7 3511.\nIn addition to the paperwork reduction aspects of the new Act, Congress included a number of provisions which might affect paperwork reduction, even though the motivation and support for these other provisions stemmed from independent concerns. Foremost among these were provisions relating to general information policy functions, statistical policy functions, records management functions, privacy functions, and federal automatic data processing and telecommunications functions. Id. \u00a7 3504.\nThis Article does not address these topics except to the limited extent that they directly intersect with paperwork reduction issues. Persons interested in these aspects of the Paperwork Reduction Act (of 1980) are advised to consult the annual reports filed by OMB under the Act; a 1983 GAO Report, IMPLEMENTING THE PAPERWORK REDUCTION ACT: SOME PROGRESS, BUT MANY PROBLEMS REMAIN, REPORT BY THE COMPTROLLER GENERAL TO THE CHAIRMAN, COMMITTEE OF GOVERNMENT OPERATIONS, HOUSE OF REPRESENTATIVES, GAO/GGD-83-35 (1983); and PapervorkReduction Act Amendments of 1983: Hearingson H.R. 2718 Before the Subcomm. on Legislationand National Security of the House Comm. on Government Operations, 98th Cong., 1st Sess. (1983). See also O'Reilly, Who's on First?:The Role of the Office of Management and Budget in FederalInformation Policy, 10 J. OF LEGIS. 95, 115-19 (1983).\nRecent amendments to the Act in October 1986 mandate greater OMB attention to statistical policy, information resources policy, and the Federal Information Locator System. The amendments also realign the roles of GSA and OMB with respect to ADP purchases. See Pub. L. No. 99-500, Title VIII, 100 Stat. - (1986).\n'7 Exec. Order No. 12,498, 3 C.F.R. 323 (1985), reprinted in 5 U.S.C. \u00a7 601, at 40 (Supp. 11 1984); Exec. Order No. 12,291, 3 C.F.R. 127 (1981), reprinted in 5 U.S.C. \u00a7 601, at 431 (1982) [hereinafter E.O. 12,291]; Exec. Order No. 12,044, 3 C.F.R. 152 (1978), reprinted in 5 U.S.C. \u00a7 553, at 70 (1976 & Supp. 11 1978) [hereinafter E.O. 12,0441.\n18See, e.g., Breyer, Reforming Regulation, 59 TULANE L. REV. 4 (1984); DeMuth & Ginsburg, White House Review ofAgency Rulemaking, 99 HARV. L. REV. 1075 (1986); Morrison, OMB Interference with Agency Rulemaking: Tile Wrong Way to Write a Regulation, 99 HARV. L. REV. 1059 (1986); Olson, Tile Quiet Shift of Poiver: Office of Management and Budget Supervision of EnvironmentalProtectionAgency Rulemaking Under Executive Order 12,291, 4 VA. J. NAT. RESOURCES L. 1 (1984); Rosenberg, Beyond the Limits of Executive Power: Presidential Control of Agency Rulemaking Under Executive Order 12,291, 80 MICH. L. REV. 193 (1981); Shane, Presidential Regulatory Oversightand the Separationof Powers:The Constitutionalityof Executive Order12,291, 23 ARIZ.-L. REV. 1235 (1981).\n\nHarvardJournalon Legislation [Vol. 24:1\non regulations proposed by Executive Branch agencies. 9 Under the Paperwork Reduction Act, however, OMB's powers extend even to the so-called independent agencies 0 and, as to agencies in the Executive Branch, the Act authorizes OMB to issue binding decisions forbidding information collection activities that it deems unnecessary.2' Given the allegations of abuse that have attended OMB review of regulations pursuant to executive orders, the implications of the Paperwork Reduction Act, where OMB's powers are even greater, deserve careful consideration.\nThe Act's treatment of regulations that contain information collection requirements, and the distinction drawn between such collections and those that are not contained in regulations, also deserves particular consideration. Although one of the purposes of the Paperwork Reduction Act was to eliminate an ambiguity as to whether earlier paperwork legislation encompassed regulatory information requirements, the manner in which regulations are to be treated is an issue which has haunted the implementation of the Act since its enactment.\nWere information collection requirements discrete and separable from the substantive activities of agencies, the above issues might be of only academic, or at most, limited interest. In reality, however, the information requirements of a regulatory program often reflect judgments about the program itself. If OMB is to have the last word regarding information requirements,22 this may imply the authority to exercise final judgment as to the substance of agency programs.\nDespite OMB's potentially awesome powers over paperwork requirements, Congress was not confident, given past history, that OMB would vigorously enforce the requirements of the Act against agencies. Congress therefore added a public protection provision to enable the public to be a partner in that enforcement effort. That provision, however, has never been successfully invoked, raising the question whether that provision has fulfilled its promise.\n19E.O. 12,291, supra note 17, \u00a7\u00a7 l(d), 3. As a practical matter, however, OMB can extend its review indefinitely and thereby delay indefinitely the promulgation of an agency rule.\n20 44 U.S.C. \u00a7\u00a7 3502(1), (10) (1982). 21Id. \u00a7 3508. 22With respect to independent agencies, OMB does not have the last word. By a majority vote of its members, an independent agency may override OMB's disapproval of an information collection. Id. \u00a7 3507(c) (1982).\n\n1987]\n\nPaperworkReduction Act\n\nThe purpose of this Article is to address these questions and issues against the backdrop of the history of attempts to control government paperwork. In so doing, this Article presents a case study of the tensions involved in attempts by Congress to mandate particular executive action and the Executive's translation\nof that mandate into its own agenda. It also highlights certain aspects of the current wisdom regarding regulatory reform that legislators should consider carefully before enacting broad reforms in regulatory procedure.\n\nI. BACKGROUND: THE FEDERAL REPORTS ACT\n\nA. The Problem Perceived: The Legislative Histot-y of the\nFederalReports Act\nOn May 16, 1938, President Roosevelt wrote a letter to the\nCentral Statistical Board in which he commissioned it to study\nthe problem of government paperwork:\nI am concerned over the large number of statistical reports which Federal agencies are requiring from business and industry .... I desire to know the extent of such reports and how far there is duplication among them. Accordingly, I am requesting the Central Statistical Board ... to report to me on the statistical work of the Federal agencies, with recommendations looking toward consolidations and changes which are consistent with efficiency and economy, both to the Government and to private industry.23\nThe Central Statistical Board reported back to President Roo-\nsevelt that for the fiscal year ending June 30, 1938, agencies had collected over 135 million returns from individuals and businesses 24 and concluded that, although most of the requested\ninformation was necessary to the government, reporting should be coordinated but remain decentralized.2 5\n\"- S. REP. No. 479, 77th Cong., Ist Sess. 21-22 (1941). The Central Statistical Board later became part of the Bureau of the Budget. Id. at 2.\n21Id. at 22-23. The Central Statistical Board broke down the information collected into 97.5 million administrative returns (including approximately 10 million income tax and informational returns), and 38 million non-administrative returns (information collected for the formulation of public policy).\nI See id. at 25. Centralization was deemed unadvisable and impractical. The main argument against a centralized data-gathering agency was a perception that the disadvantages of separating statistical processes from agencies' administrative operations would outweigh any advantages. It was felt that the expertise of the administrative agency would require less information which would be used more effectively, since the information requested would be intrinsically related to the programs administered by the agency. There was also a fear that centralization of data-gathering would decrease both the accuracy and the timeliness of the information. See, e.g., 91 CONG. REC. A5419 (1945) (statement of Rep. Clarence Cannon (D-Mo.)).\n\n8\n\nHarvardJournalon Legislation [Vol. 24:1\n\nPartially as a result of the Central Statistical Board's study and recommendations, but probably more as a result of increased complaints from business, particularly small business, 26 Congress began to view seriously the federal paperwork burden. In 1941, after conferences between representatives of the Bu-\nreau of the Budget (BoB). and the Senate Special Committee to Study the Problems of American Small Business, the Committee reported a bill entitled The Federal Reports Act of 1941.27 Essentially, it provided two means of coordinating reporting between the agencies. First, BoB would be empowered to direct\none agency to collect information on behalf of two or more agencies. Second, BoB would be authorized to direct one agency to provide to another agency data the first agency had collected for itself. 28 This bill was referred to the Senate Com-\nmittee on Education and Labor. The outbreak of World War II, however, spawned a host of\nnew reporting requirements which threatened to dwarf previous paperwork burdens. 29 Both the Office of Price Administration\nand the War Production Board were singled out as creating particularly onerous requests for information.3 \u00b0 In 1942, the bill\nreported by the Senate Committee on Education and Labor required BoB clearance of any agency's plans or forms for the collection of information from ten or more persons, authorized BoB to make a determination whether the collection of any information by an agency was necessary for the proper performance of its functions, barred any collection of information which BoB determined unnecessary for any reason, and ex-\n\n26 See generally S. REP. No. 479, supra note 23, at 2. The report includes over 120\nexcerpts from letters by small businessmen complaining about the paperwork burden. One businessman complained that a third of his office-labor overhead was the result of government requests for information. Another complained that his costs to fill out the various reports had increased by over 700 percent during the previous five years. A third letter complained about being \"audited to death.\" Yet another felt that \"the taxes are more or less unjustified [sic]-but to keep the records and fill out the forms [is] quite a burden.\" A fifth businessman wondered if anyone in Washington ever read the mountains of reports after he \"burn[ed] night oil to prepare [them].\" Id.\n27Id. at 3-4. 28 Id.\n29 See, e.g., U.S. COMMISSION ON FEDERAL PAPERWORK, THE REPORTS CLEARANCE\nPROCESS 5 (1977) [hereinafter THE REPORTS CLEARANCE PROCESS].\n\" See 88 CONG. REC. 9158 (1942). According to one report, in 1942 the War Production Board wanted to know the exact location of all of one company's products sold since 1939. The Board requested serial numbers, sizes, and other detailed information due within five days, and supplied one page for the answers; the company returned 107\npages to Washington. Reynolds, BureaucraticQuestionnairesAdd to Troubles of War Plants, Cedar Rapids Gazette, Dec. 6, 1942, reprintedin 88 CoNG. REc. 9437-38 (1942).\n\n1987]\n\nPaperworkReduction Act\n\nempted the Treasury Department from the bill. 31 The Commit-\n\ntee's report described the bill's purpose as \"reducing the cost\n\nto the Government of obtaining. . . information and minimizing the burden upon business enterprises and other persons. '32\nIn the House, a number of amendments were made. 33 In\n\naddition to empted the\n\ndeleting General\n\nthe Treasury exemption, Accounting Office (GAO)\n\ntfhroemHtohueseAcetx3-4\n\nand provided that persons who failed to, furnish information to\n\nagencies could only be subjected to the penalties provided by law.35 The House-Senate conference committee reinstated the\n\nTreasury exemption in a somewhat narrowed form-only the\n\nInternal Revenue Service (IRS), the Comptroller of the Cur-\n\nrency, the Bureau of the Public Debt, the Bureau of Accounts,\n\nthe Division of Foreign Funds Control, and the federal bank supervisory agencies were exempted.3 6 Most of the other House amendments were retained. 37 On December 24, 1942, the bill\n\nbecame law as the Federal Reports Act of 1942.38\n\nThe limited nature of the Act was generally recognized. Dur-\n\ning floor debates, Senator Arthur H. Vandenberg (R-Mich.), the\n\nmajority leader, complained that the bill did not \"remotely\n\n31 S. REP. No. 1651, 77th Cong., 2d Sess. 2-5 (1942). Incredibly, one of the expla-\nnations for the Treasury exemption was that few of the complaints received were directed at the Treasury. To the contrary, the most cited complaint of the 120 excerpts in the Special Committee's report involved tax forms. A second justification for the Treasury exemption in the report was the need to maintain the confidentiality of tax returns. In this regard, the bill was also amended to increase the confidentiality protections of information shared by agencies. See id. at 4-5.\nI2d. at 2. See 88 CONG. REC. 9164-65 (1942). The House bill, H.R. 7756, was reported in late 1942 by the House Committee on Expenditures in the Executive Departments in the same form in which the Senate bill, S. 1666, had passed the Senate, with one exception. 4 H.R. REP. No. 2658, 77th Cong., 2d Sess. 1 (1942). The reason for the GAO exemption was GAO's need, as the investigative arm of Congress, to collect data unhampered by an executive branch clearance process. See 88 CONG. REC. 9163 (1942). In addition, Rep. William M. Whittington (D-Miss.), the floor manager of the bill, claimed that he was unable to find a single complaint regarding GAO information requests. 11This amendment was added in response to tales of agencies inventing non-monetary penalties for violations of their regulations. See 88 CONG. REc. 9164 (1942). 36H.R. REP. No. 2722, 77th Cong., 2d Sess. 6 (1942). While these exemptions were\nbased on the need to protect the confidentiality of the information, see 88 CONG. REC. 9437 (1942) (statement of Rep. Whittington), it seems clear that of equal importance with respect to the IRS was the fear of interfering with the collection of taxes. See id. at 9158 (statement of Rep. Whittington).\n17 The other House amendment deleted by the conference was a proviso that the Act did not apply to information required by law to be reported or withheld. See H.R. REP. No. 2722, supra note 36, at 5.\n18Pub. L. No. 77-831,56 Stat. 1078 (1942) (codified as amended at 44 U.S.C. \u00a7\u00a7 350120 (1982)).\n\nHarvardJournalon Legislation [Vol. 24:1\ntouch[] the magnitude of the problem . . . [of] endless paperwork dictated from Washington, '39 but he endorsed the bill as a fine start in the right direction. 40 By requiring the Director of the Bureau of the Budget to review agencies' requests to collect information, Congress hoped to abate somewhat the escalating pressures from various agencies for additional statistics, to avoid some duplicative reports, and to reduce the burden associated with large numbers of seemingly unnecessary reports. 41 Even then, however, questions were raised as to whether agencies would comply with the Act and whether some penalty against administrators for non-compliance should be included. 42\nB. Practice Under the Federal Reports Act\nUnder the Act, the BoB required each agency (other than GAO and the exempted Treasury units) seeking information from ten or more persons to submit the proposed questionnaire to the BoB's Office of Statistical Standards, accompanied by a request for clearance on its Standard Form 83. 43 The request had to include a full explanation as to how the information was to be collected, whether it was to be a one-time or repetitive requirement, the number of respondents, and who they were and how they were selected. Moreover, the request was required to contain a statement of justification, describing the situation or problem which made the collection necessary and how the data would be used. The agency was required to list the consultations it had had with interested groups and the results of the consultations. The request also had to contain an estimate of the time that would be required for a typical respondent to furnish the information. 44\n-988 CONG. REc. 9076 (1942). 10See id. at 9078. See also id. at 9437 (statement of Rep. Whittington). 1'See, e.g., id. at 9078 (statement of Sen. Vandenberg); see also id. at 9159, 9161, 9165 (statement of Rep. Whittington). 42See, e.g., id. at 9436 (statements of Reps. Harry Sauthoff (Progressive-Wis.), Clare E. Hoffman (R-Mich.), Clarence J. Brown (R-Ohio)); id. at 9439 (statement of Rep. Leland M. Ford (R-Cal.)). 41Bowman, Administrationof the FederalReports Act, 18 ADMIN. L. REV. 109, 10910 (1965). Later the clearance unit's name became the Statistical Policy Division. See THE REPORTS CLEARANCE PROCESS, stpra note 29, at 19. Still later, it became the Office of Regulatory and Information Policy. See S. REP. No. 930, 96th Cong., 2d Sess. 8 (1980), reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6241, 6248. The Standard Form 83, however, has retained its designation to this day. 44Bowman, supra note 43, at 110-11.\n\n1987]\n\nPaperworkReduction Act\n\nWhen the BoB received a request, the request was logged in and sent to the staff member assigned to that area of specialization. 45 In 1947, the number of professionals reviewing agency requests was forty-seven; by 1965 they numbered only twentyfive, a level which remained constant at least through the 1973 fiscal year.46 Each of these professionals was charged with being fully informed as to all information collected by all agencies in\nhis or her field-without the benefit of computerization or even a manual indexing system or catalog. 47\nIn addition to depending on its own staff, the BoB relied heavily on outside consultation in its approval process. One source of consultation was persons in government agencies who used the type of data in question, who obtained similar data, or who for other reasons might be experts on the issues. In this way, the information request was coordinated among interested\ngovernment agencies. The BoB also consulted private persons or organizations who were interested as consumers of the data\nor as respondents. The primary entity for outside consultation was the Advisory\nCouncil on Federal Reports. 48 This council was formed in 1942 at the request of the then Director of the Bureau of the Budget, who asked five leading national business organizations to name\nrepresentatives to a group he could call upon for advice on federal reports. While the Council itself met quarterly and con-\nsidered broad questions, it formed standing committees concerned with particular industries (consisting of representatives of those industries) with which BoB could consult. In cases where a report did not fall within a standing committee's jurisdiction, ad hoc panels were formed. These consultations were, like the consultations with government personnel, informal \"shirt-sleeve\" sessions, usually including representatives of the\n\n4 5 d. at I11. 46 S. REP. No. 125, 93d Cong., 1st Sess. 25 (1973). In addition to clearing requests\nfor information, the reviewers' duties also included writing budgets for the statistical agencies, maintaining statistical standards, and developing statistical consistency at an international level. Id. at 21. For an overview of BoB functions, see generally Rappaport, The Bureau of the Budget: A View from the Inside, 101 J. ACCT. 31 (1956).\n47 S. REP. No. 125, supra note 46, at 26. In 1971, the responsibility for substantive review of forms to be used for regulatory or program administration purposes was shifted from the Statistical Policy Division to the budget examiners responsible for the agency involved. Given the budget examiners' other duties, they were understandably\nreluctant to devote much time to paperwork review. See THE REPORTS CLEARANCE\nPROCESS, supra note 29, at 23. 4- Bowman, supra note 43, at 112.\n\n12\n\nHarvardJournalon Legislation [Vol. 24:1\n\nagency seeking the information. 49 This friendly, if not cozy, relationship eventually led to criticism, congressional investi-\ngations, and, in 1972, enactment of the Federal Advisory Committee Act (FACA).50 The Council was then reorganized as the Business Advisory Council on Federal Reports, an industry\ntrade group, rather than as an advisory committee under FACA.\nThis ended the special relationship that had existed, but did not end OMB's consultations with the group. 5'\nThe review given a particular request for information varied greatly.52 Significant requests could require extensive review and\na considerable length of time, which often resulted in complaints of delays.53 While the greatest attention was given to the form and methodology of the request, with which the employees of the statistical office were expert, the office also scrutinized the need for the information and the burden caused by its collection, as required by the Federal Reports Act. It was estimated that about ten percent of the new requests for information were denied.5 4 Approved reports were assigned an approval number and an expiration date, which were required to be printed on the form. 55\nFor most of the post-war period, the number of approved reports remained fairly constant.5 6 Even the distribution of the\n\n49 Id. at 113.\n10Pub. L. No. 92-463, 86 Stat. 770 (1972) (codified; at 5 U.S.C. app. \u00a7\u00a7 1-15). See also S. REP. No. 1098, 92d Cong., 2d Sess. 2, 6 (1972); Advisory Committees: Hearings Before the Subcomm. on Intergovernmental Relations of the Senate Conin. on Governmental Operations on S. 3067, 91st Cong., 2d Sess. 1-2 (1970) [hereinafter Hearings]\n(statement of Sen. Lee W. Metcalf (D-Mont.)); id. at 209-21, 226-33 (statement of Ralph Nader). For the history behind the Federal Advisory Committee Act, see generally Levine, The Federal Advisory Committee Act, 10 HARV. J. ON LEOls. 217 (1973).\nSI See THE REPORTS CLEARANCE PROCESS, supra note 29, at 23. BoB became OMB in 1970. See Reorg. Plan No. 2 of 1970, reprinted in 5 U.S.C. app. at 1129 (1982), and in 84 Stat. 2085 (1970).\n12See Bowman, supra note 43, at 115-16. -3See THE REPORTS CLEARANCE PROCESS, supra note 29, at 24. See also Hearings, supra note 50, at 209-21 (statement of Ralph Nader). 4 Bowman, supra note 43, at 117. See also THE REPORTS CLEARANCE PROCESS, supra\nnote 29, at 24. I- Bowman, supra note 43, at 115. Writing in 1965, Raymond T. Bowman described\nthe control number and its uses: [The] number is a visible evidence of Budget Bureau approval and readily identifies the form with our record of review. It is a useful control device whereby we and the agencies keep track of what is approved. Moreover it is an announcement to the public that the form has been approved by the Bureau of the Budget. Many businessmen familiar with the requirements for Budget Bureau review question the status of any form which they receive without such a number.\nId. 5 From 1959 to 1963, the number of OMB-approved repetitive surveys was \"slightly\n\n1987]\n\nPaperwork Reduction Act\n\n13\n\ntypes of reports did not change greatly. Statistical reports accounted for a little more than a quarter of the forms, application forms for a little less than a quarter, program evaluation and other management reports about forty percent, and regulatory reports less than ten percent.5 7\nWhile the clearance process had only been one part of the Federal Reports Act, it was the part that always received the greatest attention both from within and without BoB (and later OMB). In the earliest years, however, BoB achieved some remarkable results under its coordinating function by developing uniform standards.5 8 For example, the Standard Industrial Clas-\nsification (SIC) codes, which are today the basic taxonomy of commercial activity, were adopted under BoB tutelage.59\n\nC. Inadequaciesand the Regulatory Agencies\n\nOver the years, certain congressional committees periodically held hearings and issued reports complaining that either the\nFederal Reports Act was inadequate or that BoB (and later\nOMB) was not adequately enforcing the Act. In 1965, a House Post Office and Civil Service subcommittee remarked that \"the\nFederal Reports Act, as presently administered, fully serves the interests of neither the Government nor the public. ' 60 The subcommittee recommended that \"loopholes which have been exploited by the Federal agencies during the past 23 years\" be plugged, 6' and exemptions for fiscal and banking agencies be\nrepealed, in order to bring all commissions, presidential committees, and regulatory agencies under the Act.62\n\nmore than 5000.\" Bowman, supra note 43, at 124. In 1977 the number was 4753. See THE REPORTS CLEARANCE PROCESS, supra note 29, at 19. At this time, GAO reviewed\n\nthe reports of independent regulatory agencies. GAO reported that it had 470 such\n\nreports on file in 1976. Id. at 32. The burden was not consistent. In 1965 the total burden\n\nof repetitive reports was estimated to be 95 million hours. See Bowman, supra note 43, at 126. By 1977, the report forms in OMB's inventory (which did not include the forms\n\nused by independent regulatory agencies) were estimated to involve 134 million burden\n\nhours. THE REPORTS CLEARANCE PROCESS, supra note 29, at 19. None of these numbers\n\ninclude the Internal Revenue Service's tax forms. They also do not include procurement\n\nforms.\n\n17Compare Bowman, supra note 43, at 125 with THE REPORTS CLEARANCE PROCESS,\n\nsupra note 29, at 19, 32. I8 See BUREAU OF THE BUDGET, CENTRALIZATION AND COORDINATION OF FEDERAL\n\nSTATISTICS, reprintedin 91 CONG. REC. A5420-23 (1945).\n\n5609TSHeeE\n\nid. at A5423. REPORTS CLEARANCE\n\nPROCESS,\n\nsupra note 29,\n\nat 41.\n\n61Id.\n\n6Id. at 41-42.\n\nHarvardJournalon Legislation [Vol. 24:1\nIn 1972, the Senate Select Committee on Small Business concluded there was \"an indifference of OMB officials towards their basic responsibilities .... Since only a relative handful (between one and five percent) of forms [were] disapproved, [the] committee [could] only conclude that hundreds of unnecessary or duplicative forms [were] being imposed on the public. ' '63 The Committee further concluded that OMB, being so far removed from small business, was unaware of the problems these businesses faced. The Committee also believed that OMB incompetently adapted data requests to respondents' records, 64 consistently lacked initiative in pursuing the directives of the Act,65 and refused to adequately staff or equip the office responsible for carrying out the Act. 66 The Committee then recommended that Congress transfer authority for administering the Act from OMB to the GAO. 67 Complaints of inadequate enforcement were buttressed by the refusal of certain agencies, among them the Securities and Exchange Commission (SEC) and the Federal Trade Commission (FTC), to submit regulatory reporting to OMB for clearance. 68\nThese charges of inadequacy, however, contrasted sharply with complaints that OMB was preventing the regulatory agencies from fulfilling their statutory functions. 69 The uneasy relationship between OMB and the Advisory Council on Federal Reports fueled this criticism. 70 On the one hand, the sometimes lengthy clearance process led to delay. Ralph Nader and Professor William Rogers catalogued OMB's role in delaying approval and then representing industry's position with respect to information collections in the areas of pollution control, gas pipeline regulation, and highway safety. 71 On the other hand, OMB's review of the need for the information, even though clearly authorized by the Act, seemed to infringe upon the regulatory agencies' statutory duty. The FTC, for example, claimed that its ability to study concentration in industries was\n63S.REP. No. 125, supra note 46, at 25-26. &I4d. at 30. 61Id. at 34. 66Id. at 60. 67Id. at 63. 63See THE REPORTS CLEARANCE PROCESS, supra note 29, at 43. 69See id. 70See Hearings,supra note 50.\n71Id.\n\n1987]\n\nPaperworkReduction Act\n\nstymied by OMB's refusal to grant FTC requests for information. 72\nThese latter complaints came to a head with the first Arab oil embargo in late 1973. The FTC and the Federal Power Commission (FPC) had not completed investigations into the oil and gas industries, and many blamed OMB and the Advisory Council on Federal Reports. 73 As a result, Congress adopted an\namendment to the Act, exempting independent regulatory agencies from OMB oversight. 74 The amendment transferred from\nOMB to GAO the responsibility for reviewing these agencies' reporting requirements for duplication and unnecessary burden, but not the authority to review the agencies' need for the infor-\nmation. As a precondition for the collection of information, the new provision required independent regulatory agencies to submit their plans or forms to GAO and to receive GAO's \"advice\" that the information was not already available within the government and that the compliance burden was minimized.75 GAO was required to render this advice within forty-five days of request. 76\nWhile this amendment was a victory for those who had viewed OMB as doing too much under the Federal Reports Act, those who felt too little was being done had their victory the following\nyear. Late in 1974, Congress established a Commission on Federal Paperwork and directed it to study and report what changes should be made in laws, regulations, and procedures to assure\n\n72 119 CONG. REC. 23,883 (1973) (statement of Sen. Philip A. Hart (D-Mich.)).\n7- See id. 74Pub. L. No. 93-152, \u00a7 409, 87 Stat. 576 (1973) (codified at 44 U.S.C. \u00a7 3512 (1976)).: This amendment was added to the Trans-Alaska Pipeline Authorization Act on the Senate floor, linked to another floor amendment to strengthen the FTC's subpoena\npower. See 119 CONG. REC. 23,883-87, 24,084-86 (1973). The conference committee accepted the provision, but the House members questioned its wisdom. See, e.g., id. at 36,604 (statement of Rep. Harold E. Collier (R-Ill.)), 36,604-05 (statement of Rep. Frank Horton (R-N.Y.)). Rep. Horton, a leader in criticizing the OMB for its failure to reduce reporting, said \"I have never been told that the present system allows too few requests for information.\" Id. See also id. at 36,610-11 (statement of Rep. Bill Frenzel (R-Minn.)). But see id. at 36,609-10 (statement of Rep. Harold T. Johnson (D-Cal.)), 36,616 (statement of Rep. Elizabeth Holtzman (D-N.Y.)), 36,616-17 (statement of Rep. Michael Harrington (D-Mass.)). OMB objected, GAO had doubts, and even the FTC\nsaid the Federal Reports Act change was not significant. See id. at 36,605, 36,610. What carried the day, however, was the perceived need to authorize the Trans-Alaska pipeline\nas soon as possible and the recognition that recommitting of the entire bill merely on account of the Federal Reports Act amendment would delay the bill for another year.\nSee id. at 36,605-06 (statement of Rep. Morris Udall (D-Ariz.)).\n7- 44 U.S.C. \u00a7 3512(c) (1976). 76Id. The Act contained no time limitation for OMB's review.\n\nHarvardJournalon Legislation [Vol. 24:1\nthat necessary information was obtained by the government with minimal burden, duplication, and cost.77\nD. JudicialReview and the FederalReports Act\nFor its first thirty years, the Federal Reports Act escaped judicial review. This period of dormancy ended with the famous line of cases challenging the FTC's line of business reports (LB reports). Since 1970, the FTC had apparently been half-heartedly seeking approval from OMB to undertake a broad-scale inquiry to determine areas of the economy in which profits were excessively high or low and to assess the relationships between market structure and financial performance. 78 This information, it was hoped, would be valuable in identifying areas of the economy where antitrust actions should be brought. These LB reports were conceived to be annual reports filed by 500 large manufacturing corporations, representing seventy percent of the nation's manufacturing capacity.79 The FTC would seek the financial data from each company according to a uniform accounting system developed by the FTC and based upon OMB's Standard Industrial Classification codes. This was to ensure that the data would be aggregated according to neutral, uniform classifications, rather than historical accident or managerial preferences that defined each company's own accounting classifications. Because firms did not maintain their financial data on the basis of the FTC classifications, each company would have to reclassify its financial data according to the FTC system in order to complete the report. While the FTC estimated the median cost of reclassification per firm to be $50,000, five corporations submitted estimates that indicated the cost to each would be over $1 million. 80\nOMB's delay in approving this report was one of the motivating factors behind the amendment to the Federal Reports Act that eliminated OMB review of independent agencies' reports. 81 After the amendment, the FTC submitted the LB report form to GAO for its review. Within two months, GAO completed its\n77See Pub. L. No. 93-556, 88 Stat. 1789 (1974) (codified at 44 U.S.C. \u00a7 3501 (1982)). 78See 119 CONG. REC. 23,883 (1973) (statement of Sen. Philip A. Hart). 79A.O. Smith Corp. v. FTC, 396 F. Supp. 1108, 1110 n.1 (D. Del. 1975), prelim. injunction vacated, 530 F.2d 515 (3d Cir. 1976). 80396 F. Supp. at 1118. S See 119 CONG. REc. 23,883 (1973) (statement of Sen. Hart).\n\n1987]\n\nPaperworkReduction Act\n\nreview of the report.82 This review, however, included a conclusion by the GAO staff that the data generated by the LB reports would \"be unreliable at best, and may be seriously misleading. 83\nThe LB report was challenged by respondents in several district courts on various grounds. In A.O. Smith Corp. v. FTC8,4 the plaintiffs sought a preliminary injunction against the reporting requirement on, inter alia, the ground that the Comptroller\nGeneral of the GAO had erred in approving the report. The court found, however, that the plaintiffs had no substantial prob-\nability for success on this ground. Instead, the court found that the FTC's requirement for firms to submit an LB report was a \"rule\" under the Administrative Procedure Act (APA). 85 Because adoption of this requirement had not followed APA rulemaking procedures, the court preliminarily enjoined the enforcement of the reporting requirement. 86 This decision was reversed by the Third Circuit on the ground that plaintiffs had failed to show irreparable harm.87\nBy now, however, it was time for the next LB report. The FTC revised the reporting requirement and submitted it to GAO\nfor clearance in July 1975. GAO reviewed and cleared the LB report form within forty-five days. Thereafter, orders were\nserved on 450 firms to complete the reports. When various firms failed to report, the FTC sought enforcement of the orders in district court.8 8 The respondents defended on the basis that the LB report orders were issued in violation of the rulemaking requirements of the APA and were further invalid because the Comptroller General's approval was in error.89 The court found that the LB report order was not a rule under the APA, but rather an investigative act, for which the APA does not define\n\nv See A.O. Smith Corp. v. FTC, 396 F. Supp. at 1111.\n8I Id.at 1118.\n8 396 F. Supp. 1108 (D. Del. 1985). 15Id. at 1119, 1121-24. 86 Id. at 1112, 1125. 17 A.O. Smith Corp. v. FTC, 530 F.2d at 525-29. 8 In re FTC Corporate Patterns Report Litig., 432 F. Supp. 291, 301-04 (D.D.C. 1977), aff'd sub nom. Appeal of FTC Line of Business Report Litig., 595 F.2d 685, 692-93 (D.C. Cir. 1978) (per curiam), cert. denied sub nom. Milliken & Co. v. FTC, 439 U.S. 958 (1978). This case also involved challenges to the Corporate Patterns Report form, which was to be used by the FTC for a similar industry survey, but which was aimed at over 1100 large manufacturing firms. Inre FTC Corporate Patterns Report Litig., 432 F. Supp. at 298.\nlI9d.at 298-99.\n\nHarvardJournalon Legislation [Vol. 24:1\n\nany particular procedure. 90 This determination was upheld by the court of appeals. 91\n\nWith respect to the Comptroller General's approval, the first issue was whether it could be reviewed at all. The FTC main-\n\ntained that because \"advice\" under the\n\nthe Comptroller Federal Reports\n\nGeneral's decision was only Act, the decision was com-\n\nmitted to agency discretion by law and therefore unreviewable. 92\n\nThe court rejected this argument because the Federal Reports\nAct \"clearly sets forth two criteria under which the Comptroller is to make his review: avoiding unnecessary duplication and minimizing compliance burden.\" 93 That the Act characterized\n\nthe communication of these findings, which were a prerequisite\n\nto the agency collection of information, as \"advice,\" did not alter the fact that there was law to apply. 94 Having determined\n\nthat the Comptroller General's review was itself reviewable, the\n\ncourt addressed the defendants' claim that the Comptroller General had erred in approving the LB report form. This claim\n\nstemmed from the language in the Act that \"the Comptroller\n\nGeneral shall determine ... the appropriateness of the forms for the collection of such information.\" 95 This language, the firms said, required the Comptroller General to make a cost-benefit\n\ndetermination as to the appropriateness of the forms for their intended purpose. 96 The court rejected this interpretation as\n\ninconsistent with the legislative history which exempted inde-\n\npendent agencies from just that sort of review. Rather, the \"appropriateness of the forms\" was read as a short-hand reference to the burden of reporting inquiry.97\n\nThe defendants also claimed that the Comptroller General's approval was arbitrary and capricious, because the clearance\n\nletter had acknowledged that the burden problems of the 1973 LB report were \"still incompletely resolved\" in the 1974 form,\nand that \"a rather long test period of data collection and analysis\n\n90Id. at 301-04. 91Appeal of FTC Line of Business Report Litig., 595 F.2d at 693-96. 9 In re FTC Corporate Patterns Report Litig., 392 F. Supp. at 307. Two district courts in dictum had accepted this argument from the FTC in earlier LB report litigation. See General Electric Co. v. FTC, 411 F. Supp. 1004, 1006 (N.D.N.Y. 1976); Westinghouse Electric Corp. v. FTC, 1976-1 Trade Cas. (CCH) 60,871 (S.D. Ohio 1976). 91In re FTC Corporate Patterns Report Litig., 432 F. Supp. at 307-08 (relying on Citizens to Preserve Overton Park, Inc. v. Volpe, 401 U.S. 402, 410 (1971)). 94 Id.\n9544 U.S.C. \u00a7 3512(d) (1976). 96 In re FTC Corporate Patterns Report Litig., 432 F. Supp. at 308. 97 Id.\n\n1987]\n\nPaperworkReduction Act\n\n19\n\nwill be required to fully reconcile FTC's data needs with minimum burden.\" 98 The court rejected this claim as well. While the court did not subject the Comptroller General's determination\nto searching scrutiny, it found that he had considered and re-\njected defendants' claims of undue burden before approving the form. 99 This rejection, in light of the FTC's efforts to improve the form and the limited time available for his review, was not unreasonable. 00 On appeal, the defendants raised only the \"ap-\npropriateness\" claim, which was rejected by the court of appeals on the same basis as the court below. 101\nDespite the defendants' lack of success on the merits, Federal\nReports Act claims continued to be brought. In FTC v. Rockefeller,02 several banks defended against FTC subpoenas in part\non the basis that they had not been approved by the GAO. The court held that subpoenas were not the collection of \"information\" as defined in the Act. 0 3 The FTC was not the only agency\nso afflicted. The Federal Power Commission (FPC) also had its\nreporting requirements challenged under the Federal Reports Act. 0 4 Later the Department of Energy (DOE) found its reporting and recordkeeping requirements challenged.105 In Shell Oil Co. v. DOE,0 6 several energy producing companies chal-\n\n\"Id. at 309, 311.\n9 Id.\n11 Id. at 309-11. The court also rejected a claim that GAO's approval was procedurally flawed because it had violated its own regulations in approving the forms.\n101Appeal of FTC Line of Business Report Litig., 595 F.2d at 708-10. 102441 F. Supp. 234 (S.D.N.Y. 1977). 103Id. at 243-44. See also FTC v. Carter, 464 F. Supp. 633, 642 (D.D.C. 1979).\n104 See Superior Oil Co. v. FERC, 563 F.2d 191 (5th Cir. 1977). In this case, the FPC had adopted a report form by rulemaking, and the report form had been approved by GAO. Respondents claimed, however, that the FPC had announced it would not resubmit the form annually as required by GAO. The court rejected the claim on the basis that the FPC's announcement was meant simply to streamline pre-clearance procedure. Id. at 202-03. See also Union Oil Co. v. FPC, 542 F.2d 1036 (9th Cir. 1976). In this case, the court set aside a report form adopted by rulemaking. The court found unsupported by substantial evidence the premise that the need for the information outweighed the burden its collection placed on industry. Interestingly, the FPC defended in part on the basis that this was a determination for GAO, which had approved the collection. The court rejected this claim, pointing out that the determination of need and the ultimate question whether to collect the information rested on the collecting agency, not on GAO. See 542 F.2d. at 1043.\n105See Shell Oil Co. v. DOE, 477 F. Supp. 413 (D. Del. 1979), aff'd 631 F.2d 231 (3d Cir. 1980) (per curiam), cert. denied, 450 U.S. 1024 (1981). See also Olympian Oil v. Schlesinger, [1974-80 Binder] Energy Mgmt. (CCH) (N.D. Cal. March 1, 1979) 26,140 (DOE recordkeeping requirement contained in regulation challenged on basis that it was not approved by OMB or GAO; summary judgment for DOE denied because, while regulation itself might not require approval under Federal Reports Act, implementing documents might).\n'0 477 F. Supp. 413.\n\nHarvardJournalon Legislation [Vol. 24:1\nlenged the Financial Reporting System of the Energy Information Administration (EIA), a component of DOE. This reporting\nprogram required the submission of substantial financial and operational data for the years 1977 and 1978. Although limited to energy producing companies, the report was similar to the FTC LB report form and raised similar problems for the companies involved. In addition, EIA explicitly stated its willingness to provide disaggregated (company-specific) data to other agencies, such as the FTC and the Antitrust Division of the Department of Justice. The respondents challenged the report form on a number of grounds, including the claimed invalidity of the report's clearance by OMB. 0 7 The court rejected DOE's arguments that OMB's review was committed to agency discretion by law, citing A.O. Smith Corp. 08 and In re FTC Corp. Patterns Litigation.0 9 On the merits, the court assessed whether OMB's approval had been arbitrary and capricious. It had little difficulty concluding that the approval was reasonable, but the court did subject OMB's decision to some scrutiny, including OMB's conclusion that the need for the information warranted the burden and cost involved.\"l0 On appeal, the respondents apparently did not even raise the Federal Reports Act claims.\"'\nNot only had it taken a long time to discover the Federal Reports Act as a possible means to challenge reporting or recordkeeping requirements, but also its zero success rate suggested that such challenges would not remain as a standard weapon in the arsenal of industry lawyers. Nevertheless, the willingness of courts to review GAO and OMB determinations\non the merits at least held out the hope of judicial invalidation of a reporting or recordkeeping requirement in an egregious case. Before that case appeared, however, the Federal Reports Act was replaced by the Paperwork Reduction Act.\nII. THE PAPERWORK COMMISSION\nIn 1974, responding to continuing constituent complaints about paperwork burdens and to already perceived difficulties\n107 See id. at 428. The respondents asserted that GAO rather than OMB should have reviewed the form. The court found, however, that while the term \"independent regulatory agency\" was not defined in the Act, the history of the amendment indicated that only regulatory agencies with quasi-judicial functions were intended to be exempted from OMB review. Because EIA had no such functions, OMB was the proper entity to review the report form.\n1 396 F. Supp. 1108 (D. Del. 1985). 104932 F. Supp. 291 (D.D.C. 1977). 1o See Shell Oil, 477 F. Supp. at 429-30. \"I See Shell Oil, 631 F.2d at 232-33.\n\n1987]\n\nPaperworkReduction Act\n\n21\n\nwith GAO's role as overseer of the regulatory agencies, Congress created the Commission on Federal Paperwork.\"12 The Commission's charter required it to make a number of studies to determine the nature of the federal paperwork problem and to make recommendations for changes in statutes, rules, regulations, procedures, and practices.\" 3 The final report was to be submitted within two years of the Commission's first meeting.II 4\nOn October 3, 1977, the Commission submitted its final report, summarizing the findings from thirty-six previously issued\nspecialized reports, which included no less than 770 recommendations.\" 5 Many of these recommendations related to specific reporting or recordkeeping requirements identified in the course of the Commission's investigations. Many more involved easy, non-controversial administrative or procedural changes, so that\nby the time of writing the summary report, the Commission could claim that almost half of the 770 recommendations contained in the specialized studies had already been adopted.\" 6\nIts findings at the general level produced no surprises. Seven key causes of bad paperwork, defined as duplicative, excessive, costly, contradictory, intimidating, or confusing\" 7 were identified: lack of communication between government and those subject to the paperwork requirements; bureaucratic insensitivity to the problems paperwork causes; incomprehensible forms and instructions; overlapping organizations requiring the same\nor similar information; poor program design; poor information practices leading to unnecessary paperwork requirements; and inconsistent and ineffective confidentiality policies creating con-\n\n112Pub. L. No. 93-556, 88 Stat. 1789 (1974), 44 U.S.C. \u00a7 3501 note (1976) (eliminated\nin 1982 U.S.C.). W Id. \u00a7 3, 88 Stat. at 1789-90.\n114 Id. 115 FINAL REPORT, supra note 12.\n116Id. at 1. Under the legislation creating the Paperwork Commission, OMB was\nrequired to monitor and report on executive agencies' actions in response to Commission recommendations. This was done in three reports: U.S. OFFICE OF MANAGEMENT AND BUDGET, PAPERWORK AND RED TAPE: NEW PERSPECTIVES, NEW DIRECTIONS (Reports to The President and Congress, June 1978, October 1978, and September 1979). The Paperwork Reduction Act specifically required the Director of OMB to \"complete action on recommendations of the Commission,\" 44 U.S.C. \u00a7 3505(3)(D) (1982), and this requirement spawned an additional and final report on the recommendations. U.S.\nOFFICE OF MANAGEMENT AND BUDGET, FEDERAL INFORMATION COLLECTION: AGENCY ACTIONS ON COMMISSION ON FEDERAL PAPERWORK RECOMMENDATIONS, Vol. I (Multi-agency Recommendations) December 1980, Vol. II (Recommendations to Departments) March 1982, Vol. III (Recommendations to Independent Agencies and the Office of Management and Budget) January 1983.\n117 FINAL REPORT, supra note 12, at 12.\n\nHarvardJournalon Legislation [Vol. 24:1\nfusion and mistrust.\"18 The Commission also described several failures of the existing system of control: legislation and regulations were drafted with little consideration of the paperwork consequences; controls had only been established over the forms themselves, not over, the reason the forms existed; the controls in existence had been designed to control statistical paperwork, rather than program management and operation which constituted the vast bulk of federal paperwork; existing controls were spread among a number of different agencies; and there was no central listing of what information was being collected by the government. 1 9\nMany of the Commission recommendations read like old texts for good government. Congress should consider paperwork consequences at each stage of the legislative process; agencies should give interested persons a full opportunity to participate in the rulemaking process; and everyone should engage in more planning. 20 However, one recommendation reflected the increasingly popular attempt to import economic concepts into political organization and management. Information should not be treated as a free good, but as a scarce resource. The government should manage information just as it manages financial, material, and human resources.' 2' It followed that there should be a federal budget for this resource.\nIII. THE INFORMATION COLLECTION BUDGET\nPresident Carter launched the budget concept in 1979. By executive order, he required each agency in the Executive Branch to prepare an annual paperwork budget. 2 2 Rather than dollars, the budget would use what had become the standard\n118Id. at 6-8. \"9 1d. at 8-9. 120Id. at 15-16.\n121Id. at 16, 56, 63.\n12 Exec. Order No. 12,174, 3 C.F.R. 462 (1979), reprintedin 5 U.S.C. \u00a7 552 note (Supp. III 1979)(eliminated in 1982 U.S.C.) [hereinafter E.O. 12,174]. Actually, many of the features of a budget for paperwork had been introduced earlier in the Carter Administration. OMB had already mandated across-the-board percentage reductions in paperwork requests by establishing ceilings and allocating them to the executive agencies. See generallyTHE REPORTS CLEARANCE PROCESS, supranote 29, at 13-14. Nevertheless, in 1981 the Director of OMB referred to the Federal Information Collection Budget for Fiscal Year 1981 as the \"first paperwork budget.\" U.S. OFFICE OF MANAGEMENT AND BUDGET, INFORMATION COLLECTION BUDGET OF THE UNITED STATES GOVERNMENT: FISCAL YEAR 1981 (memorandum of transmittal from James T. McIntyre, Jr. to the President, Jan. 13, 1981) [hereinafter 1981 ICB].\n\n1987]\n\nPaperworkReduction Act\n\n23\n\nmeasuring tool under the Federal Reports Act-burden hours. Burden hours were simply the number of hours it took to comply with a particular request for information multiplied by the number of persons subject to that request. Each agency would submit its proposed budget to OMB, specifying the information requests the agency planned to use in the following fiscal year. The Director of OMB was to review and was authorized to modify the proposed budget. An agency could exceed its budget only upon further OMB approval. 23\nA budget approach was particularly useful to OMB for a number of reasons. First, the clearance process for individual information requests had required OMB to focus on the trees, failing to see the forest. An annual review of the entire mass of federal information requests, while presenting its own difficulties, provided a valuable perspective on federal paperwork requirements. Second, because the President was invoking his general constitutional authority to coordinate executive action, 124 he was able to include in the budget those executive agencies that avoided OMB review under the Federal Reports Act, notably the Treasury Department. Third, as OMB had already learned, a budget was a useful tool in forcing general reductions in agency paperwork requirements. 125\nReducing paperwork was the name of the game. President Ford had set a goal in 1976 of reducing by ten percent the number of reports requested from the public. 26 This goal was exceeded, but the burden hours associated with the remaining\n\n'2 E.O. 12,174, supra note 122. In addition to requiring an annual paperwork budget,\n\nthe Order required agencies to review all requests for information within two years of\n\ntheir initial issuance and at least every five years thereafter. The Director of OMB was further required to establish a federal information locator system, which would list all\n\nthe types of information collected by the various agencies. The Paperwork Commission\n\nhad recommended such a system as a means by which to avoid agencies duplicating\n\neach others' information requests. See FINAL REPORT, supra note 12, at 17. Finally,\n\nthe Order required the Director of OMB to publish an annual paperwork calendar of\n\nsignificant requests for information.\n\nOMB issued a proposed rule to implement the Order, as well as to strengthen its\n\ncontrols over agencies under the Federal Reports Act. Controlling Paperwork Burdens\n\non the Public, 45 Fed. Reg. 2586 (1980) (proposed Jan. 11, 1980). The proposed rule\n\nwas never adopted and was replaced with a new proposed rule after passage of the Paperwork Reduction Act. Controlling Paperwork Burdens on the Public, 47 Fed. Reg.\n\n39,515 (1982) (proposed Sept. 8, 1982) (codified at 5 C.F.R. pt. 1320). '2 E.O. 12,174, supra note 122.\n\n12.OMB's use of ceilings for one-time and repetitive reports to be allocated among\n\nthe agencies, see supra note 122, had been critical to the administration's claimed 15%\n\nreduction in government paperwork in two years. See Federal Paperwork Reduction:\n\nRe1m26 aTrHksE\n\non Signing Exec. Order No. 12,174, 1979 PUB. REPORTS CLEARANCE PROCESS, supra note 29,\n\nPAPERS at 11.\n\n2176\n\n(Nov.\n\n30,\n\n1979).\n\n24\n\nHarvardJournalon Legislation [Vol. 24:1\n\nreports actually increased by four percent. 127While this increase was the result of two new federal programs (HUD's Uniform\nSettlement Statement and the Employee Retirement Income Security Act), the Paperwork Commission concluded that even with respect to the already existing programs there had been little reduction in real burden. 12 The Carter Administration built upon the framework of the Ford program, affirming the five\npercent goal it had set for 1977.129 By 1979, President Carter\ncould claim a fifteen percent reduction in burden hours under his administration.130 For fiscal year 1981 the first paperwork budget, officially denominated the Information Collection Budget (ICB), set a goal of a further reduction of almost four percent. 131\nThe Paperwork Reduction Act does not specify any budget for government paperwork or even mandate a budget process.132\n\n127 Id.\n'2 Id. at 12. 12I9d. at 13. 130Federal Paperwork Reduction: Remarks on Signing Exec. Order No. 12,174, 1979\nPUB. PAPERS 2176 (Nov. 30, 1979). 131See 1981 ICB, supra note 122, at 1. The mathematics of burden reduction are\nfascinating. President Carter's 15% reduction between 1977 and 1979 would have reduced paperwork burden to 720 million burden hours from a base of 847 million burden hours. Federal Paperwork Reduction: Message to Congress, 1979 PUB. PAPERS 2180, 2181 (Nov. 30, 1979). In 1981, however, OMB stated that in fiscal year 1980 \"[a]gencies subject to the information collection budget imposed a paperwork burden of 1,276 million hours on the public.\" 1981 ICB,supranote 122, at 1.Thus, despite claims of reductions, the admitted burden hours had increased during the Carter Administration by 50%.\nSimilarly, the Reagan Administration, while claiming to have surpassed the Paperwork Reduction Act's 25% reduction goal for its first two years, has watched its burden hours increase from 1,534 million hours at the end of FY 1981 to 1,924 million hours for FY 1985. U.S. OFFICE OF MANAGEMENT AND BUDGET, INFORMATION COLLECTION BUD-\nGET OF THE UNITED STATES GOVERNMENT: FISCAL YEAR 1985 [hereinafter 1985 ICB]. While there are logical explanations for these apparent anomalies, there is an element of comedy in a system that annually reports reductions in paperwork but over time reports consistently increasing burden hours.\nOne explanation for this seeming anomaly is the practice of recomputing existing paperwork burden at the end of each year and only measuring reductions in future years against that recomputed base figure. For example, in FY 1984, OMB first included the\ngreat mass of agency procurement paperwork in its calculation of the existing paperwork burden. This inclusion resulted in the addition of hundreds of millions of burden hours to the base figure, even though this did not reflect any actual increase in paperwork.\nSee U.S. OFFICE OF MANAGEMENT AND BUDGET, INFORMATION COLLECTION BUDGET\nOF THE UNITED STATES GOVERNMENT: FISCAL YEAR 1984 17-18 [hereinafter 1984 ICB]. Past years' reported percentage reductions, however, were not recomputed in light of the substantially increased base figure. Thus, each year's \"year-end adjustments\" can result in increased base figures to be used for setting percentage reductions in the following year, but not in correcting the past years' reported reductions.\n13,The absence of express statutory authority for a paperwork budget under the Paperwork Reduction Act has led some to question the lawfulness of the budget process. See PapenvorkReduction Act Amendments of 1984: Hearings on S.2433 Before the Subcomm. on Information Management andRegulatory Affairs of the Senate Cotnn.\n\n19871\n\nPaperworkReduction Act\n\nNevertheless, OMB has continued the ICB as a principal mechanism by which OMB fulfills its responsibilities under the Act to establish general policies and procedures for controlling information collections.1 33 As described by OMB, the process for establishing the ICB is similar to the process for determining the fiscal budget.134 Each year, agencies submit to OMB a proposed budget describing all collections of information the agency proposes to impose on the public during the new fiscal year, including the agencies' best estimate of the burden hours involved in the collections. OMB reviews each submission and holds meetings with agencies until a final budget is completed. Under OMB's regulations, an agency then cannot impose any collection not included in the budget unless the agency provides for an offsetting reduction or obtains a supplemental authorization from OMB.135 OMB publishes the ICB annually, describ-\ning the past year's reductions and the present year's paperwork budget for the government.\nBoth OMB and the agencies agree that the ICB is a potent force for reducing the reported burden hours involved in information collections. 136 Many agencies complain, however, that the meat-ax approach used by OMB in the ICB can result in inappropriate reductions or counterproductive trade-offs.1 37 That is, an agency may feel forced by OMB's ceiling to eliminate a reporting requirement it believes to be very important, only\nbecause it believes other requirements are even more important. Moreover, in these circumstances internal bureaucratic power, rather than the merits of competing collections, may play a greater role in a decision to retain one collection rather than another. An agency may even go so far as to substitute a more costly procedure, such as on-site inspections of a facility, for a cheaper reporting requirement, because the substitute procedure does not count toward its ICB.'3 8 Because OMB's ceilings are viewed as essentially arbitrary (or political), where agencies\n\non Governnental Affairs, 98th Cong., 2d Sess. at 240 (1984) (Memorandum of Law by the Public Citizen Litigation Group).\n,\" 1985 ICB, supra note 13 1, at 2. See also 5 C.F.R. \u00a7 1320.10 (1986) (OMB regulation on ICB).\n,341985 ICB, supra note 131, at 2.\n\"5 5 C.F.R. \u00a7 1320.10 (1986). 136 Anonymous interviews with a number of paperwork personnel in executive agencies. 137 Id. 138 Id.\n\nHarvardJournalon Legislation [Vol. 24:1\nbelieve these ceilings interfere with their programs, there is substantial question as to the net benefits of the ICB.\nOMB understandably denies the negative effects of the ICB and asserts that the problems faced by the agencies are no different from those faced in the fiscal budget process. 13 9 While there is substantial truth in this observation, one major difference exists between the fiscal budget process and the ICB process. The final OMB fiscal budget is merely a proposal to Congress; it is not the last word. The ICB is the final word. The absence of inevitable and continuing congressional oversight of the ICB suggests, that OMB should be somewhat more solicitous of agency concerns in the development of the ICB. As a practical matter, however, the absence of meaningful congressional oversight is likely to result in OMB being less solicitous of agency views.\nIV. THE PAPERWORK REDUCTION ACT\nA. Legislative History\nIn 1977, with the publication of the Final Report of the Paperwork Commission, Congress did not rush to legislate the recommendations made by the Commission. Rather, the interested members of Congress, especially Representative Frank Horton (R-N.Y.) and Senator Thomas McIntyre (D-N.H.), the Chairman and Co-Chairman of the Commission, together with Senator Lawton Chiles (D-Fla.), focused on congressional oversight of OMB's activities as a means of implementing the Commission's recommendations. 140 Furthermore, paperwork reform was only one of a number of administrative reforms afoot in the Administration and Congress.\nPresident Carter had come to Washington in part by running against the bureaucracy. A centerpiece of his reform, Executive Order 12,044, promulgated in March 1978, required executive agencies to use cost-benefit analyses in justifying regulations; to minimize paperwork burdens on the public; and before adopt-\n119Interview with Nell Minow, special assistant to the Administrator of Office of Information and Regulatory Affairs, in Washington, D.C. (June 17, 1985).\n140See Efforts to Reduce FederalPapenvorkBurdens:HearingBefore the Subcotnin. on FederalSpending Practicesand Open Government of the Senate Comm. on Governmental Affairs 95th Cong., 2d Sess. (1978).\n\n1987]\n\nPaperworkReduction Act\n\ning any significant regulations, to estimate the reporting and recordkeeping requirements necessary for compliance.1 41\nAdministrative reform was also popular in Congress. The bill that became the Regulatory Flexibility Act was first passed in the Senate in 1978.142 Sunset legislation was in vogue as well.1 43 By 1979, both the Executive and Congress had launched major reform initiatives. 44 In March the administration unveiled its regulatory reform bill, essentially extending E.O. 12,044 to all agencies. 45 Also in March, Representatives Frank Horton and\nJack Brooks (D-Tex.) introduced H.R. 3570, the Paperwork and\nRedtape Reduction Act of 1979. In June, Senators Chiles, John\nDanforth (R-Mo.), and Lloyd Bentsen (D-Tex.) introduced a\ncompanion bill, S. 1411, and a hearing was held on November 1.146 Later that month, President Carter issued his executive order on paperwork reduction 147 and, in the statement that\naccompanied it, specifically endorsed Senator Chiles'\nlegislation. 148 In February 1980, Representatives Brooks and Horton intro-\nduced H.R. 6410 in the House as a new companion measure to S. 1411. Hearings were quickly held. 149 The House Committee on Government Operations, chaired by Represenatative Brooks, amended the bill to include a number of provisions relating to\nautomatic data processing (ADP) and telecommunications technologies, matters of special interest to Representative Brooks. 150 The Committee unanimously reported the bill, as amended, in early March,' 5' and on March 24, the House passed H.R. 6410 without any member speaking against it.' 52 The marriage of\n\n14l E.O. 12,044, supra note 17, at \u00a7\u00a7 l(e), 2(d)(6).\n142Regulatory Flexibility Act, Pub. L. No. 96-354, 94 Stat. 1164 (1980) (codified at 5\nU.S.C. \u00a7\u00a7 601-12 (1982)) (requires federal agencies to consider the impact of proposed regulations on small businesses, organizations, and governmental entities).\n'43See, e.g., S. REP. No. 981, 95th Cong., 2d Sess. (1978); S. REP. No. 326, 95th\nCong., Ist Sess. (1977). 'I\"See generally Increasing Attention Focused on Regulatory Reform Plans, 37\nCONG. Q. WEEKLY REP. 560-63 (Mar. 31, 1979).\n241 See S. 755, 96th Cong., 1st Sess., 125 CONG. REC. 6152-59 (1979); and H.R. 3263,\n96th Cong., Ist Sess., 125 CONG. REC. 6326-28 (1979) (summary). 146PaperworkandRedtape Reduction Act of 1979: Hearingbefore the Subcomm. on\nFederal Spending Practicesand Open Government of the Senate Comm. on Governmental Affairs, 96th Cong., 1st Sess. (1979).\n147E.O. 12,174, supra note 122. '41Remarks on Signing Executive Order 12,174, 1979 PUB. PAPERS 2176. '49See Papenvork Reduction Act of 1980: Hearingsbefore a Subcomm. of the House\nComm. on Government Operations, 96th Cong., 2d Sess. (1980). 15oSee H.R. REP. No. 835, 96th Cong., 2d Sess. (1980). 151Id. 152126 CONG. REC. 6212-14 (1980).\n\nHarvardJournalon Legislation [Vol. 24:1\npaperwork reduction, dear to Representative Horton, and ADP, a subject of special concern to Representative Brooks, gave the bill momentum at a time when other reform measures were losing support.\nIn the Senate, the Governmental Affairs Committee, to which H.R. 6410 was referred, reported S. 1411 in August in a form very similar to H.R. 6410.151 One problem surfaced in committee. Several senators were concerned about the impact of the ADP amendments on defense and intelligence agencies.'54 The principals agreed, therefore, to support a floor amendment to delete the bill's coverage of ADP with respect to defense command-and-control and intelligence systems. 155\nA last snag developed when Senator Edward Kennedy's (DMass.) staff discovered the bill's implications for reporting and recordkeeping requirements contained in agency regulations. 56\nAs drafted, the bill appeared to require OMB's approval before any reporting or recordkeeping regulation could become effective, and this approval was limited to a maximum of three years. Senator Kennedy's staff feared that an OMB hostile to a regulatory program would be able to effectively repeal a regulation adopted after notice and comment without any public process.'57 This fear intensified with Ronald Reagan's election in the first week in November. As a result, last-minute negotiations produced a floor amendment to section 3504(h).' 58 This amendment created an elaborate and special procedure to govern collection of information requirements contained in agencies' proposed rules, the effect of which was to require OMB to comment publicly on the proposed rule if it wished to affect the reporting or recordkeeping requirement. Only if the agency's public response was unreasonable would OMB be able to disapprove the requirement in the rule. 159\n'53See S. REP. No. 930, supra note 43. Virtually the only substantive difference was the addition of a procedure for OMB approval in emergencies, which is found at 44 U.S.C. \u00a7 3507(g) (1982).\n'54See S. REP. No. 930, 96th Cong., 2d Sess. 64-73 (1980), reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6241, 6303-12 (additional views of Sens. Jackson (DWash.), Cohen (R-Me.), Glenn (D-Ohio), and Stevens (R-Alaska)).\n115 Interview with Robert Coakley, Professional Staff Member, Subcommittee on Federal Expenditures, Research and Rules, Senate Committee on Governmental Affairs, in Washington, D.C. (June 20, 1985).\n156 Telephone interview with Alan Morrison, Public Citizen Litigation Group (July 1985).\n157 Id. 158See 126 CONG. REC. 30,177 (1980). \"19 As part of the deal, Senator Kennedy also obtained certain other changes in the\n\n1987]\n\nPaperworkReduction Act\n\n29\n\nOn November 19, on the floor of the Senate, Senator Danforth\nin Senator Kennedy's absence explained the amendment's purpose as \"prevent[ing] OMB from undoing a collection of information requirement specifically contained in an agency rule after that requirement has gone through the administrative rulemak-\ning process if the OMB Director ignored the rulemaking process.' 60 In the hurried process of negotiating this floor amendment some of its implications were overlooked. This failure was to cause problems in the future.' 6' At the time, however, sup-\nporting paperwork reduction was the paramount issue and the bill passed by acclamation. 62\nIn the House, Representative Brooks took issue with the defense-related complaints concerning ADP. 163 Representative\nHorton explained the Kennedy amendment.164 Not having been a party to the negotiation, Representative Horton was not sympathetic to the provision. 165 Indeed, Representatives Brooks and Horton considered forcing the bill to conference, 66 but, given\nthe limited time left in the session, this would have effectively killed the bill. Politics being the art of compromise, they settled for the bill as it was. 67 Representative Horton, however, took\nthe opportunity to supply a little legislative history by explaining the Kennedy amendment. He declared that under the amend-\nment, OMB could \"disapprove any collection requirement\nwhich it finds 'unreasonable'-which is to say, not of sound judgment in the opinion of the OMB Director.' 68 This was\n\nbill. Thus, in section 3518(e), the statement that nothing in the act affects the authority of the President or OMB with respect to the substantive policies and programs of agencies was amended to specify the enforcement of civil rights laws as one of those programs. Also, a list of exempted collections of information was amended to add compulsory process under the Federal Trade Commission Improvements Act of 1980. Finally, even with respect to reporting requirements not in regulations, Senator Kennedy bargained for language requiring OMB to seek public comment and to make its decisions\npublic. 160126 CONG. REc. 30,178-79 (1980). The CongressionalRecord also contains two\ncolumns of a statement by Senator Kennedy, 126 CONG. REc. 30,178 (1980), which was inserted after the fact. Senator Kennedy's statement explains the procedure in section 3504(h), as well as its purpose. Senator Danforth's explanation of the purpose is totally consistent with the Kennedy statement.\n161See infra text accompanying notes 235-265. 162 126 CONG. REC. 30,193 (1980). 163Id. at 31,227.\n11,Id. at 31,228. 165Interview with Steve Daniels, legislative assistant to Rep. Horton, in Washington,\nD.C. (June 19, 1985). 166 Id.\n167 Id. 168126 CONG. REC. 31,228 (1980).\n\n30\n\nHarvardJournalon Legislation [Vol. 24:1\n\nclearly at odds with the provision's language that authorized the director to disapprove a collection only if the agency's response\nto OMB's comments was unreasonable. In addition, Represen-\ntative Horton stated that no decisions by OMB under the Kennedy amendment were reviewable in court. 169 The provision, however, stated that there would be no judicial review of the\nDirector's decision to approve or not to act upon a collection of information requirement, 170 implying that there could be review of a decision to disapprove. In any case, no one objected to the bill, and it was passed by a voice vote.' 71 On December 11, 1980, President Carter signed the bill into law. 172\n\nB. The Act Generally\nThe linchpin of the Act was the role of OMB as policymaker and overseer of government paperwork activities. This capitalized upon OMB's experience under the Federal Reports Act and the Carter Administration's use of OMB as the central overseer of the executive branch's regulatory process. The Paperwork Reduction Act specifically created the Office of Information and Regulatory Affairs (OIRA) in OMB, to which the Director of OMB was directed to delegate his functions under the Act. 173 These specific paperwork control functions were to\n\n169Id.\n11404 U.S.C. \u00a7 3504(h) (1982).\n171126 CONG. REC. 31,228 (1980). 172Paperwork Reduction Act of 1980, Pub. L. No. 96-511, 94 Stat, 2812 (1980),\n(codified as amended at 44 U.S.C. \u00a7\u00a7 3501-3520 (1982)). 17344 U.S.C. \u00a7 3503 (1982). Prior to the Act, OMB had created an Office ofRegulatory\nand Information Policy which had responsibility for OMB's paperwork functions (the\nFederal Reports Act, E.O. 12,174, supra note 122, and the follow-up of the Paperwork Commission's recommendations). This office also had responsibility for OMB's oversight of agency activities under E.O. 12,044, supra note 17, the Carter Administration's order to improve government regulations generally. Thus, the Paperwork Reduction Act's creation of OIRA and its specified functions essentially codified existing arrangements. See S. REP. No. 930, 96th Cong., 2d Sess. 8 (1980), reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6241, 6248. While recognizing the value of locating in one office both the oversight of government regulations and the government information collection activities, however, Congress expressed concern with the possibility that regulatory reform initiatives, if also assigned to that office, would dilute the attention and resources devoted to paperwork issues. Id. at 8-9, 1980 U.S. CODE CONG. & ADMIN. NEWS at 6248-49. These concerns proved well-founded, as in the Reagan Administration OIRA was the point office for both regulatory reform (under the new E.G. 12,291, supra note 17) and paperwork reduction. Thereafter, OIRA was criticized for having not devoted sufficient attention or resources to its information management responsibilities under the Act. See generally U.S. GENERAL ACCOUNTING OFFICE,\nIMPLEMENTING THE PAPERWORK REDUCTION ACT: SOME PROGRESS, BUT MANY PROB-\nLEMS REMAIN, REPORT BY THE COMPTROLLER GENERAL TO THE CHAIRMAN, COMMIT-\nTEE ON GOVERNMENT OPERATIONS, HOUSE OF REPRESENTATIVES (1983) [hereinafter IMPLEMENTING THE PAPERWORK REDUCTION ACT].\n\n1987]\n\nPaperworkReduction Act\n\n31\n\ninclude: the review and approval of information collection re-\n\nquests proposed by agencies; the determination of whether a\n\ncollection of information by an agency was necessary for the\n\nproper performance of the functions of the agency, \"including\n\nwhether the information will have practical utility for the\n\nagency;\" ensuring that the various procedural requirements for\n\ninformation collection requests were met; designating central\n\ncollection agencies; setting burden reduction goals; overseeing\n\naction on the Paperwork Commission recommendations; and\n\nsetting up a Federal Information None of these functions, however,\n\nLocator System (FILS). 174 were new to OMB.175\n\nWhat was new was the extent to which OMB's clearance\n\nfunction reached. For example, Treasury's exemption was elim-\n\ninated, and the so-called independent agencies were again brought under OMB's scrutiny. 176 Moreover, under the Federal\n\nReports Act, there had been disputes as to the meaning of the term \"information,\"1 77 with several agencies denying that it\n\nreached recordkeeping and reporting requirements contained\n\n17444 U.S.C. \u00a7 3504(c) (1982). 17-The clearance by OMB of new information collection activities by most agencies had, of course, existed under the Federal Reports Act. 44 U.S.C. \u00a7 3509 (1976). Even the explicit authority for OMB to determine the necessity of the information collection to carry out the agency's functions had been expressly provided in the Federal Reports Act. 44 U.S.C. \u00a7 3506 (1976). The term \"practical utility\" and its use as a means for screening information collection requests was adopted from the guidelines that had been issued by OMB, first under President Ford's paperwork program and later President\nCarter's paperwork program. See, e.g., 45 Fed. Reg. 2586, 2588 (1980) (Proposed OMB rule to implement E.O. 12,174 and the Federal Reports Act). OMB had also been responsible under the Federal Reports Act for designating a central collection agency when the information needs of two or more agencies would be adequately served by a single entity. 44 U.S.C. \u00a7 3504 (1976). OMB had already been responsible for overseeing action on the Paperwork Commission's recommendations. Pub. L. No. 93-556, \u00a7 3(d),\n88 Stat. 1789 (1974), 44 U.S.C. \u00a7 3501 note (1976) (eliminated in 1982 U.S.C.). Executive Order 12,174 had required OMB to establish a federal information locator\nsystem and, by requiring the annual paperwork budget, had put OMB in charge of across-the-board burden reduction goals.\n17644 U.S.C. \u00a7 3502(1) (1982). The only independent agency exempted from OMB\noversight was the Federal Election Commission. Id. While brought under OMB's oversight, the independent agencies retained a veto power over OMB determinations with respect to particular information collections. 44 U.S.C. \u00a7 3507(c) (1982).\n7 The term \"information\" in the Federal Reports Act was defined as\nfacts obtained or solicited by the use of written report forms, application forms, schedules, questionnaires, or other similar methods calling either for answers to identical questions from ten or more persons other than agencies, instrumentalities, or employees of the United States or for answers to questions from agencies, instrumentalities, or employees of the United States which are to be used for statistical compilations of general public interest.\n44 U.S.C. \u00a7 3502 (1976).\n\nHarvardJournalon Legislation [Vol. 24:1\ndirectly in regulations.178 The Paperwork Reduction Act clearly encompassed both these requirements. t79 OMB's power to review all information collection requests was reinforced by a new provision, entitled \"Public Protection.' ' 8 0 This provision insulated persons from \"any penalty for failing to maintain or provide information to any agency if the information collection request... does not display a current control number assigned by [OMB].\"'' In this way, if agencies ignored OMB and its clearance process, the people subject to the resulting information collection would have an explicit basis for not complying with it.\nAlso new were the statutory deadlines set for OMB. In a little less than two years, OMB was to set goals to reduce the existing paperwork burden by fifteen percent, and by the following year, a further ten percent. 182 Within one year of enactment of the Act, OMB was to establish FILS, identify areas of duplication in information collection and develop a schedule and methods for eliminating them, and identify initiatives to reduce by ten percent the paperwork burden associated with grant administration. Within two years, OMB was to complete action on the Paperwork Commission's recommendations. 83\nAgencies also were assigned responsibilities. t 4 This was responsive to a common criticism of the clearance process under the Federal Reports Act: the process was too little too late. The primary responsibility for ensuring the elimination of duplication and unnecessary information collections should rest with the agencies. OMB should be the overseer or check, not the front line of attack against paperwork abuse. 85 Each agency was required to designate a senior official to carry out that agency's\n171See THE REPORTS CLEARANCE PROCESS, supra note 29, at 34. See also S. REP. No. 930, 96th Cong., 2d Sess. 13 (1980), reprintedin 1980 U.S. CODE CONG. & ADMIN. NEws 6241, 6253; H.R. REP. No. 835, 96th Cong., 2d Sess. 19 (1980).\n179\"Information collection request\" was defined to mean \"a written report form, application form, schedule, questionnaire, reporting or recordkeeping requirement, or other similar method calling for the collection of information.\" 44 U.S.C. \u00a7 3502(11) (1982). Moreover, the procedures applicable to information collections contained in proposed regulations were specifically addressed. See 44 U.S.C. \u00a7 3504(h) (1982).\n180 44 U.S.C. \u00a7 3512 (1982). 18 Id. ' 44 U.S.C. \u00a7 3505(l) (1982). ,8434 U.S.C. \u00a78 3505(2), (3) (1982). These are only the deadlines directly pertaining to OMB's paperwork functions. In addition, there were a number of deadlines relating to information resource management and automatic data processing. Id. 11444 U.S.C. \u00a7 3506 (1982). ''5See FINAL REPORT, supra note 12, at 18.\n\n1987]\n\nPaperworkReduction Act\n\nresponsibilities under the Act. 186 In order to ensure that that individual was sufficiently senior to manage those responsibilities, the Act required that the person report directly to the agency head.1 87 Moreover, the Act provided that OMB may even delegate its clearance functions as to an agency's paperwork requests to the designated senior official of that agency. 88 Nevertheless, despite the new statutory duties placed upon agencies with respect to paperwork' 89 and the newly focused responsibility placed upon the designated senior officials, the power provided by the Act belongs to OMB and that power rests in the clearance function.\n\nC. The ClearanceFunction\nUnlike the Federal Reports Act, the Paperwork Reduction Act specifies many of the details of the clearance process. The Act requires that an agency submit to OMB the proposed \"information collection request\"' 90 together with an explanation of how the agency has assured that the information sought is not already available within the federal government, that the burden on respondents is minimized to the extent practicable, and that the information will be tabulated in a manner to enhance its usefulness.' 9' The agency must publish a notice in the Federal Register announcing the submission of the request to OMB.192\nOMB then has sixty days within which to approve or disapprove the request, a period that OMB may extend for an addi-\n\n1- 44 U.S.C. \u00a7 3506(b) (1982). 187Id.\n,144 U.S.C. \u00a7 3507(e) (1982). This delegation can be made only if the Director of OMB finds the agency's designated senior official to be \"sufficiently independent of program responsibility to evaluate fairly whether proposed information collection requests should be approved and has sufficient resources to carry out this responsibility effectively.\" Id. The delegation is made by rule after public notice and opportunity for comment. Id.\nOMB did not make its first delegation until June 15, 1984-to the Board of Governors of the Federal Reserve System. See 49 Fed. Reg. 20,792 (1984) (codified at 5 C.F.R. \u00a7 1320.9(d), pt. 1320 app. A). OMB has, however, issued a general call encouraging agencies with effective internal control systems to seek delegations. See U.S. OFFICE\nOF MANAGEMENT AND BUDGET, MANAGING FEDERAL INFORMATION RESOURCES,\nFOURTH ANNUAL REPORT UNDER THE PAPERWORK REDUCTION ACT OF 1980, at 18\n(1985) [hereinafter FOURTH ANNUAL REPORT]. ,8C\"ompare44 U.S.C. \u00a7 3506 (1982) with 44 U.S.C. \u00a7 3501 (1976). See also 44 U.S.C.\n\u00a7 3507(a)(1) (1982). ,9044 U.S.C. \u00a7 3507(a)(2)(A) (1982). ,9I,d. \u00a7 3507(a)(1). 192Id. \u00a7 3507(a)(2)(B).\n\n34\n\nHarvardJournalon Legislation [Vol. 24:1\n\ntional thirty days. 193 If OMB is silent, approval is inferred. t94\nBefore approving an information collection request, OMB is to determine if the collection of information is \"necessary for the\nproper performance of the functions of the agency, including whether the information will have practical utility.\"' 95 If OMB\nfor any reason determines the collection to be unnecessary, the agency may not collect the information. 96 Approvals are for\nperiods specified by OMB, which may not exceed three years, or one year if the approval is inferred from OMB's silence. 197\nWhen OMB approves the collection, it assigns a control number to the information collection request. 198 OMB's determinations must be made publicly available. 99 If OMB disapproves a pro-\nposed information collection request from an independent regulatory agency,200 that agency may override OMB's veto by a\n\n,9I1d. \u00a7 3507(b). In emergency situations, the head of an agency may require OMB\nto approve or disapprove a request in a shorter period of time, and, if the request is approved, the agency may collect the information without further compliance with the Act for a period of 90 days from the date when the request was submitted to OMB. See\n44 U.S.C. \u00a7 3507(g) (1982). This authority has been utilized 23 times since April 1981. Seventeen of these were in the first year and six in the second. The authority has not\nbeen invoked since March 1983, when OMB's regulations implementing the Act were promulgated. Controlling Paperwork Burdens on the Public, 48 Fed. Reg. 13,666 (1983) (codified at 5 C.F.R. pt. 1320 (1986)). Those regulations suggested agencies request\nexpedited plenary consideration instead of invoking the mandatory emergency, but limited, authorization. 5 C.F.R. \u00a7\u00a7 1320.17(e), (f) (1985).\n194 44 U.S.C. \u00a7 3507(b) (1982). Approval by silence is the exception. Between April 1, 1981 and September 30, 1982, of the almost 6000 appovals of paperwork requests,\nonly 12 were by silence. See IMPLEMENTING THE PAPERWORK REDUCTION ACT, supra note 173, at 58.\n194-4 U.S.C. \u00a7 3508 (1982). \"Practical utility\" is defined as \"the ability of an agency to use information it collects, particularly the capability to process such information in\na timely and useful fashion.\" 44 U.S.C. \u00a7 3502(15) (1982). '9 44 U.S.C. \u00a7 3508 (1982). This substantive standard is unchanged from the Federal\nReports Act. See 44 U.S.C. \u00a7 3506 (1976). ,9474 U.S.C. \u00a7\u00a7 3507(b), (d) (1982). I98Id. \u00a7 3507(b).\n199 Id.\n200 \"Independent regulatory agency\" is defined at 44 U.S.C. \u00a7 3502(10) (Supp. II 1984):\nthe Board of Governors of the Federal Reserve System, the Commodity Futures Trading Commission, the Consumer Product Safety Commission, the Federal Communications Commission, the Federal Deposit Insurance Corporation, the Federal Energy Regulatory Commission, the Federal Home Loan Bank Board, the Federal Maritime Commission, the Federal Trade Commission, the Interstate Commerce Commission, the Mine Enforcement Safety and Health Review Commission, the National Labor Relations Board, the Nuclear Regulatory Commission, the Occupational Safety and Health Review Commission, the Postal Rate Commission, the Securities and Exchange Commis-\nsion, and any other similar agency designated by statute as a Federal independent regulatory agency or commission.\n\n1987]\n\nPaperworkReduction Act\n\nmajority vote of its members. 20 1 0MB must then assign a control number to that request. 20 2\nA somewhat different procedure is prescribed when a \"collection of information requirement\" 20 3 is contained in an agency's proposed rule published for public comment. 204 This is the\nprocedure created by the Kennedy Amendment. The agency in\nthis circumstance must send a copy of the proposed rule to OMB not later than its publication in the FederalRegister.20 5\nWithin sixty days of the publication of the notice of proposed\nrulemaking, OMB may file public comments on the collection of information requirement. 20 6 If OMB does not file any comments within the sixty days, then it generally may not disap-\nprove the collection of information requirement contained in the rule. 20 7 If OMB does file comments, the agency must respond to them when it promulgates the final rule.20 0MB is then able\nto disapprove the collection of information requirement in its discretion if, within sixty days of the publication of the final\nrule, OMB finds the agency's response to its comments was unreasonable. 20 9 0MB may in its discretion also disapprove the\ncollection of information requirement in a final rule if OMB finds\nthat between the proposed and final rule, the collection of information requirement was substantially modified, so that OMB\nfailed to have adequate notice for at least sixty days as to the final requirement. 210 Any disapproval and the reasons for it must be made publicly available. 211 Any OMB disapproval of an in-\n\n.'440U.S.C. \u00a7 3507(c) (1982). This override power has rarely been exercised. Through\nSeptember 1985, independent agencies had overridden OMB denials on only three occasions. See U.S. OFFICE OF MANAGEMENT AND BUDGET, MANAGING FEDERAL\nINFORMATION RESOURCES, SECOND ANNUAL REPORT UNDER THE PAPERWORK REDUCTION ACT OF 1980, at 5 (1983) and FOURTH ANNUAL REPORT, supra note 188, at 8. Since then, the Federal Communications Commission has overruled OMB twice. See infra note 571.\n20244 U.S.C. \u00a7 3507(c) (1982). 201The term \"collection of information requirement,\" unlike \"information collection request,\" is not defined in the Act. It is used exclusively in section 3504(h), relating to collections of information specifically required by agency rules adopted after notice and comment. 44 U.S.C. \u00a7 3504(h) (1982). ,04Id. \u00a7 3504(h)(8). 2-Id. \u00a7 3504(h)(1). - Id. \u00a7 3504(h)(2). 207Id. \u00a7 3504(h)(4). But see id. \u00a7 3504(h)(5). 20-Id. \u00a7 3504(h)(3). 209Id. 88 3504(h)(5), 3504(h)(5)(C). 210Id. \u00a7 3504(h)(5)(D). OMB may also disapprove a collection of information require-\nment contained in a rule if the agency failed to provide OMB notice not later than the publication of the notice of proposed rulemaking. Id. \u00a7 3504(h)(5)(B).\n211Id. \u00a7 3504(h)(6).\n\nHarvardJournalon Legislation [Vol. 24:1\ndependent regulatory agency's collection of information requirement is subject to the same override provisions as OMB disapprovals of those agencies' information collection requests. 212\nV. INITIAL IMPLEMENTATION\nA. The Treasuiy-OMB Dispute\nCongress recognized that the paperwork requirements of the Act would be entirely new to some agencies and that certain aspects of the clearance process would be new to all agencies. Consequently, Congress delayed the effective date of the Act until April 1, 1981.213 The public protection provision, which allowed persons to ignore information collection requests not displaying control numbers, applied only to information collection requests made after December 31, 1981.214 Thus, agencies had adequate time to ensure that information collection requests had OMB control numbers.215\nFor most agencies, implementation did not prove to be especially difficult. The independent agencies reported to OMB rather than to GAO, and most agencies, in light of the arguably broader coverage of the Act, were forced to reassess what they had been submitting under the clearance process. Nevertheless, for the most part, business continued as usual.\nThe implementation process proved to be far more difficult, however, at the Treasury Department, particularly at the Internal Revenue Service (IRS). The IRS was the largest requester of information in the governnient, 2 6 and its information collections had never before been subject to prior approval. Moreover, it faced peculiar difficulties in creating the necessary tax forms because almost annual changes to the tax laws necessitated\n212Id. \u00a7 3504(h)(7). See also supra text accompanying notes 200-02. 213See Paperwork Reduction Act, Pub. L. No. 96-511, \u00a7 5, 94 Stat. 2812, 2826 (1980)(codified at 44 U.S.C. \u00a7\u00a7 3501-20 (1982)). The Act became law on December 11, 1980. 214See 44 U.S.C. \u00a7 3512 (1982). 215 See S. REP. No. 930, 96th Cong., 2d Sess. 52 (1980), reprintedin 1980 U.S. CODE CONG. & ADMIN. NEws 6241, 6292; H.R. REP. No. 835, 96th Cong., 2d Sess. 30 (1980). 216At the time, the calculation of federal paperwork listed the Department of the Treasury as imposing the largest number of burden hours (661.2 million) of any agency, or 45% of the total federal paperwork burden. See 1982\" ICB, supra note 5, at 13. Of this, 95% was attributable to the Internal Revenue Service. Id. Later, when agency procurement was included in the calculations, the Department of Defense moved into first place. See 1984 ICB, supranote 131, at 20.\n\n1987]\n\nPaperworkReduction Act\n\ncorresponding changes to the tax regulations and forms. Substantial lead time was necessary to draft and issue regulations and to create and review the new forms. The lead time available was often minimal, and the added necessity of obtaining prior approval from OMB imposed a seemingly impossible burden. 217\nPerhaps because the IRS had not been subject to OMB or GAO review before, the initial fears of such review may have\nbeen extreme. Some IRS officials probably feared that personnel in OMB without expertise in tax laws could easily create proplems of massive proportions through an attempt to simplify reporting or minimize burdens. 218 Others may have feared that the OMB review would entail a policy review which could be construed as a political review, thereby tarnishing IRS's independence and professionalism. 2 9 In addition, officials in the IRS, accustomed to acting autonomously in the past, may have manifested a natural reluctance to be subjected to the review of another agency. Nevertheless, the Act did in fact mandate such\na regime of review.\nControversy surrounding interpretation of the Act began to arise when OMB provided guidance to agencies relating to the effective date of the public protection provision. 220 Because large numbers of existing agency regulations, including all of the IRS regulations, had never been submitted to OMB, they did not have OMB control numbers. 221 The public protection provision, however, authorized persons after December 31, 1981 to ignore \"information collection requests\" which did not have valid control numbers. 222 OMB initially directed agencies to\nforward all existing regulations lacking control numbers to OMB for its review pursuant to section 3507.223 This guidance crystallized a basic ambiguity in the statute.\n\n217Interview with anonymous official of the Internal Revenue Service in Washington,\nD.C. (June 18, 1985). 218 Id. 219Id. Initial implementation of the Paperwork Reduction Act coincided with OMB's\nimplementation of E.O. 12,291 and the initial activities of the Presidential Task Force on Regulatory Relief, whose executive director was the director of OIRA. Many of the identified \"initiatives\" of the Task Force related to the reduction of paperwork. See generally PRESIDENTIAL TASK FORCE ON REGULATORY RELIEF, REAGAN ADMINISTRATION REGULATORY ACHIEVEMENTS (Aug. 11, 1983).\n220 See 44 U.S.C. \u00a7 3512 (1982).\n22 Even when control numbers had been assigned to regulations, those regulations virtually never displayed the control number. The public protection provision allows persons to disregard information collection requests that do not \"display\" a current control number. See 44 U.S.C. \u00a7 3512 (1982).\n22 Id. 23See Memorandum from Christopher DeMuth to Agency Paperwork Officials, re:\n\nHarvardJournalon Legislation [Vol. 24:1\nThe Act uses two different terms to refer to collections of information imposed by an agency: \"collection of information requirement\" 22 4 and \"information collection request.\" 22 5 The former is undefined22 6 and appears only in section 3504(h), which establishes the procedure for OMB review of regulations which are adopted by notice-and-comment rulemaking and require collections of information. \"Information collection request\" is a defined term and is used throughout the statute to refer generally to information collections imposed by agencies. 22 7 The ambiguity lies in how these terms interrelate-are they mutually exclusive, or is \"information collection request\" an umbrella term, of which \"collection of information requirement\" is merely a subset?\nImportant consequences flow from this determination. First, the power of OMB to disapprove \"information collection requests\" is plenary, whereas its power to disapprove \"collection of information requirements\" is strictly limited. 2 8 Second, OMB can approve an \"information collection request\" for no longer than three years,2 29 but the Act fails to specify any time limitation on OMB approval of \"collection of information require-\nPaperwork Reduction Act Coverage of Information Collection Requirements in Existing Regulations (Oct. 12, 1982) (on file at HARV. J. ON LEGIS.) (referring to a \"bulletin circulated in late 1981\"). While OMB later articulated a legal justification for subjecting all collections of information to section 3507's procedures, the initial implementation seems to have arisen more from the dynamics of the situation than a considered legal opinion. The same office (OIRA), using the same personnel (agency desk officers), was already reviewing under E.O. 12,291 all proposed and final regulations prior to their publication. See infra notes 462-581 and accompanying text. OMB's review of any paperwork requirements in the rule was coincident with and largely indistinguishable from its review under the Executive Order.\n224 44 U.S.C. \u00a7 3504(h) (1982). - Id. \u00a7 3502(1 1). 226The term \"collection of information,\" however, is defined.\n[ihe term \"collection of information\" means the obtaining or soliciting of facts or opinions by an agency through the use of written report forms, application forms, schedules, questionnaires, reporting or recordkeeping requirements, or other similar methods calling for either(A) answers to identical questions posed to, or identical reporting or recordkeeping requirements imposed on, ten or more persons, other than agencies, instrumentalities, or employees of the United States; or (B) answers to questions posed to agencies, instrumentalities, or employees of the United States which are to be used for general statistical purposes. 44 U.S.C. \u00a7 3502(4) (1982). 227The definition in full reads as follows: \"the term 'information collection request' means a written report form, application form, schedule, questionnaire, reporting or recordkeeping requirement, or other similar method calling for the collection of information.\" 44 U.S.C. \u00a7 3502(11) (1982). The term is used in 44 U.S.C. \u00a7\u00a7 3502(7), (9), 3504(a), (c), (h), 3505-08, 3510(a), 3511-12, 3514(a), and 3517 (1982). \"'Compare44 U.S.C. \u00a7 3508 (1982) with 44 U.S.C. \u00a7\u00a7 3504(h)(4), (5) (1982). 229See 44 U.S.C. \u00a7 3507(d) (1982).\n\n1987]\n\nPaperworkReduction Act\n\nments.\" Third, the requirement for a control number states that the control number is to be displayed on the \"information collection request,' 230 and no mention is made of control numbers on \"collection of information requirements.\" Fourth, the public protection provision applies by its terms only to \"information collection requests. '231 Thus, if the two terms are mutually exclusive, persons would have to comply with \"collection of information requirements\" whether or not the collections have valid control numbers. On the other hand, if \"collection of information requirements\" is included within the definition of \"information collection request,\" then the failure to display a valid control number could render invalid (or repeal sub silentio) otherwise valid and effective regulations, because persons would be excused from compliance.\nClosely related to, if not dependent on, the determination of whether \"information collection request\" was an all-inclusive term was the question of how to categorize already existing regulations which had been adopted after notice and comment. Were such regulations to be treated as \"collection of information requests?\" If so, their failure to display a control number would effectively invalidate them. If not, these regulations would seem to avoid the Act altogether, because review under section 3504(h), applicable to \"collection of information requirements,\" occurs only at the time the regulation is proposed for notice and comment.\nThe OMB interpreted the term \"information collection request\" to be inclusive. 2 2 The Treasury Department did not.\nTreasury did not want to submit for OMB review the vast body of existing IRS regulations requiring persons to maintain records or report information. 233 If OMB's interpretation of the term prevailed, any lapse by either Treasury or OMB in identifying and numbering all the applicable regulations could seriously\ndisrupt the collection of taxes. The Treasury Department did not object, however, to sub-\nmitting existing IRS tax forms to OMB for approval. 234 There\n,OSee id. \u00a7 3507(f).\n-\" See id. \u00a7 3512. 2 See Memorandum from C. Boyden Gray, Counsel to the Vice President, and Michael J. Horowitz, Counsel to the Director, Office of Management and Budget, to Assistant Attorney General Theodore B. Olson (January 15, 1982)[hereinafter Gray Memorandum](on file at HARV. J. ON LEGIS.). 213 Interview with anonymous Department of Treasury official in Washington, D.C. (June 18, 1985). 231 Memorandum from Theodore B. Olson, Assistant Attorney General, to C. Boyden\n\nHarvardJournalon Legislation [Vol. 24:1\nwas less basis for objection here because there was no ambiguity in the statute. Furthermore, the potential for OMB interference was less because the substantive information required on the forms was invariably required to be submitted by regulations. The forms were merely the vehicle for collection. The area for disagreement with OMB-whether the information was necessary-would only arise in considering the underlying regulations. Finally, the mischief that might be caused by inadvertent failure to display a control number on a form was limited by the existence of the substantive reporting or recordkeeping requirement in a regulation. Thus, even if under section 3512 one could ignore the IRS form, the substantive reporting or recordkeeping requirement imposed by the regulation would remain. In other words, whether or not the form 1040 is valid, the IRS regulations require persons to report their taxable income annually. This made all the more critical Treasury's view that their regulations not be considered \"information collection requests.\"\nB. The Justice Department Opinion\nNeither Treasury nor OMB would budge from its interpretation of the Act. OMB, therefore, requested the Office of Legal Counsel (OLC) in the Justice Department to settle the dispute by rendering an interpretation that would be binding on the agencies. 235 OMB's argument was premised on the definition of\nGray and Michael J. Horowitz, re: Paperwork Reduction Act of 1980 (June 22, 1985) [hereinafter Office of Legal Counsel Opinion or OLC Opinion](on file at HARV. J. ON LEGis.) at 17.\n131See Gray Memorandum, supra note 232. Under 28 U.S.C. \u00a7\u00a7 511-12 (1982), the Attorney General is authorized to render opinions on questions of law when requested by the President and the heads of executive departments. These opinions are considered binding on executive agencies. 25 Op. Att'y. Gen. 301 (1904). For a number of years, the Office of Legal Counsel in the Department of Justice has had the responsibility for preparing these Attorney General opinions. See 28 C.F.R. \u00a7 0.25(a) (1986). As a matter of practice in recent years, opinions of the Office of Legal Counsel have virtually supplanted opinions of the Attorney General. They are also considered to be binding on the agencies, although the only means of enforcement may be an appeal to the President or the refusal of the Justice Department to appear in court in defense of a contrary opinion.\nOrdinarily, agency requests for opinions of the Office of Legal Counsel are made by the General Counsel for the agency, although it is not uncommon for the officer whose operations are involved to seek the opinion. Here, however, the request was made by neither the Administrator of OIRA nor the general counsel of OMB. Rather, the request was made jointly by the equivalent of special assistants to the Director of OMB and the Vice President. Some believed that involvement of these persons represented a conscious attempt by OMB to use its political weight to influence the decision. Telephone interview with former Office of Legal Counsel employee (May 1985).\n\n19871\n\nPaperworkReduction Act\n\n\"information collection request,\" the use of the term in the Act, and the legislative history of the Act. As defined, the term\nexpressly includes a \"reporting or recordkeeping requirement,\" and a recordkeeping requirement is further defined as \"a re-\nquirement imposed by an agency on persons to maintain specified records. 236 Although these definitions do not on their face explicitly indicate that they include regulations, they are phrased in a manner which could include regulations. 237 In addition,\nportions of the legislative history of the terms suggest an inclusive reading. 238 Moreover, to exclude regulations adopted after\nnotice and comment from the term \"information collection request\" would do substantial violence to the comprehensive\nscheme of the Act. One of the prime purposes of the Paperwork Reduction Act was to consolidate clearance functions in OMB and to eliminate exemptions from those functions that had existed under the Federal Reports Act.32 9 If regulations adopted\n\n23644 U.S.C. \u00a7 3502(16) (1982).\n27 It is possible, even from the plain language of the definition of \"information collection request,\" to argue that the definition does not include regulations. The first four specified types of \"requests\" clearly refer to reporting requirements evidenced by something that itself is to be filled in or completed, unlike a reporting requirement imposed by a regulation. The canon of ejusdem generis would suggest that the term \"reporting or recordkeeping requirement\" should be construed narrowly to be consistent with the specific examples that precede it in the list. Furthermore, if \"reporting or recordkeeping requirement\" is to be read as broadly as its language allows, what is to be done with\nstatutes which directly impose such a requirement? It seems unlikely that Congress intended that one could ignore without penalty such statutes merely because they do\nnot display a current OMB control number. 2.3 The Senate Report stated that \"[t]he imposition of a federal paperwork burden\ndoes not depend on how the questions are asked of the respondent, but rather on the fact the Federal government has asked or sponsored the asking of questions.\" S. REP. No. 930, 96th Cong., 2d Sess. 39, reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6241, 6279. This statement, however, was made in the context of a discussion about whether oral requests fell within the definition. Even Senator Kennedy, in a post-\nenactment statement, referred to an \"information collection request\" in a manner suggesting that it included \"collection of information requirements.\" He stated that his\namendment drew a distinction between \"an information collection request derived from a rule and one specifically contained in an agency rule.\" 126 CONG. REC. 34,237 (1980). In context, however, it is doubtful that the reference to \"one specifically contained in an agency rule\" reflected a considered answer to whether a \"collection of information\nrequirement\" was also an \"information collection request.\" Kennedy did not make the statement while explaining the relationship between the two terms. Instead, he made the statement while clarifying the fact that the procedures of section 3504(h) apply when a collection of information is contained in a rule, as opposed to derived from the rule.\nOther aspects of the legislative history are less helpful. For example, the Senate Report explained the definition of \"information collection request\" as \"refer[ring] to the\nactual instrument used for a collection of information.\" S. REP. No. 930, supra, at 29. This would be a strange description of a regulation.\n29 See generally S. REP. No. 930, 96th Cong., 2d Sess. 13-14, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEWS 6241, 6253-54 (strengthening the Federal Reports Act). Indeed, disputes over the Federal Reports Act's coverage of \"recordkeeping require-\n\nHarvardJournalon Legislation [Vol. 24:1\nafter notice and comment containing reporting or recordkeeping requirements were not included within the term \"information collection request,\" and the procedures of section 3504(h) applied only during the notice-and-comment rulemaking process, then existing regulations already adopted after notice and comment with reporting or recordkeeping requirements would seemingly escape coverage of the Act. Indeed, OMB argued, extension of the Act to the IRS would be virtually meaningless if so many of its existing regulations could escape review. 240\nTreasury answered these arguments by relying on the language, history, and purpose of section 3504(h). Section 3504(h) establishes a detailed procedural scheme applicable to \"collection of information requirements\" proposed for adoption in regulations through notice-and-comment rulemaking. The Act does not specifically address regulations in any other section. This, Treasury argued, gives rise to an inference that section 3504(h) is the only provision applicable to such regulations, and that the term used in section 3504(h)-\"collection of information requirement\"-is the exclusive term for collections of information imposed directly by regulation adopted after notice and comment. 241\nThe purpose and history of section 3504(h), Treasury urged, support such a reading. The section was introduced in a floor amendment by Senator Kennedy as a response to fears that the Act as then drafted \"would permit the Director of OMB to overturn a rule which was adopted by an agency without providing any procedural rights for the people affected by the rule or the agency that promulgated the rule. '242 Senator Kennedy's\nments\" were the cause of their specific inclusion in the Paperwork Reduction Act, See S. REP. No. 930, supra, at 13; H.R. REP. No. 835, 96th Cong., 2d Sess. 19 (1980).\n240 OLC Opinion, supra note 234, at 15. 241Id. at 10. 242 126 CONG. REc. 30,178 (1980). Actually, a substantial argument can be made that\nOMB did not have such power. As the bills were reported by both the House and Senate committees, section 3504(h) provided only that OMB should ensure that \"in developing rules and regulations,\" agencies use efficient means to collect, use, and disseminate information, provide reasonable opportunity for the public to comment on the proposed means of collecting information, and assess the consequences of alternative means of collecting, using, and disseminating information. No particular procedure was specified for how OMB was to ensure that this occurred. The fact that section 3504(c) dealt with OMB's \"information collection request\" clearance responsibilities and procedures, while section 3504(h) dealt with OMB's responsibilities with regard to agency regulations, perhaps suggests that agency regulations were not intended to fall under subsection (c) and the procedures applicable to \"information collection requests.\" The committee reports, morever, indicated that OMB's functions under section 3504(h), as reported, would be \"similar to the present OMB function to oversee agency activities\n\n1987]\n\nPaperworkReduction Act\n\namendment, therefore, specified the procedure by which OMB\n\ncould review and comment on agency rules proposed for notice\n\nand comment, and the narrow category of circumstances under\n\nwhich OMB could disapprove the agency's collection of infor-\n\nmation requirement. Treasury argued that subjecting already-\n\nexisting regulations (as well as future regulations after they were\n\nin effect) which were adopted after notice and comment to the\n\ncontrol number requirement, the time limitation, and section\n\n3507 review would frustrate the purposes of the Kennedy\n\nAmendment. 43 By fectively invalidate\n\nmeans of these provisions, OMB could regulations without public process3 44\n\nef-\n\nunder Executive Order 12,044.\" S.REP: No. 930, 96th Cong., 2d Sess. 8, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEWS 6248. See also H.R. REP. No. 835, 96th Cong., 2d Sess. 9 (1980). Executive Order 12,044 was President Carter's order on regulatory reform. See supra note 17, rescinded by E.O. 12,291, supra note 17, \u00a7 10.\nExecutive Order 12,044 imposed a number of specific requirements on agencies, including a requirement for regulatory analyses of \"major\" regulations, but did not specify\nany particular functions for OMB other than that of \"assur[ing] the effective implementation of this Order\" and approving procedures agencies were to adopt in complying with the order. E.O. 12,044, supra note 17, \u00a7\u00a7 3, 5. OMB did not review all regulations\nas a matter of practice. Nor did it assert the right to disapprove them. Some \"major\" regulations were reviewed by the Regulatory Analysis Review Group (RARG), which was comprised of representatives of the Council of Economic Advisors, OMB, and\nother agencies with major regulatory responsibilities. On occasion RARG attempted to influence agencies with respect to the regulation at issue. See R. LITAN AND W.\nNORDHAUS, REFORMING FEDERAL REGULATION 69-79 (1983). Consequently, section 3504(h), as reported, did not unambiguously require treating regulations as \"information collection requests.\" Nevertheless, it cannot be denied that the history is also susceptible to the interpretation that section 3504(c) applied by its terms to regulations imposing collections of information, and that section 3504(h) only granted additional responsibility and power to OMB, beyond clearance procedures, over agency regulations.\n243 OLC Opinion, supra note 234, at 11. 24Treasury also argued that section 3507(c), which relates to the power of indepen-\ndent agencies to override OMB disapproval of information collections, supported the distinction between \"collection of information requirements\" and \"information collection\nrequests.\" OLC Opinion, supra note 234, at 12-13. Section 3507(c) states that \"[a]ny disapproval by the Director ... of a proposed information collection request of an\nindependent regulatory agency, or an exercise of authority [by the Director] under section 3504(h) ... concerning such agency, may be voided [by a majority vote of the agency's members].\" 44 U.S.C. \u00a7 3507(c) (1982) (emphasis added). The only authority concerning an agency that section 3504(h) grants the Director is the authority to dis-\napprove, under certain circumstances, the \"collection of information requirements\" of an agency. Thus, section 3507(c) distinguishes between an \"information collection request\" and a \"collection of information requirement\" by specifying separately an independent agency's power to override OMB's disapproval of each. Were the \"collection\nof information requirement\" merely a subset of the universal \"information collection request,\" this separate specification would not be necessary. Treasury therefore concluded that Congress intended section 3504(h)'s \"collection of information requirement\" to be treated separately from section 3507(c)'s \"information collection request.\"\nThis argument loses some of its weight when one realizes that this language in section 3507(c) preceded the Kennedy Amendment. Thus, the section's reference to an independent regulatory agency's override of OMB's \"exercise of authority under section 3504(h),\" in the reported bill, meant only that independent agencies could, by majority vote, avoid any particular act by OMB to \"ensure that... agencies (1) utilize efficient\n\nHarvardJournalon Legislation [Vol. 24:1\nIf Treasury's arguments prevailed, the terms \"collection of information requirement\" and \"information collection request\" would be mutually exclusive. Because the Act's provisions regarding public protection, OMB control numbers, and the three year approval limitation period expressly used the term \"information collection request,\" these provisions would not apply to \"collection of information requirements.\" Moreover, all existing regulations adopted after notice and comment would completely escape OMB review, because section 3507's procedures apply only to \"information collection requests\" and section 3504(h)'s procedures apply only to proposed regulations containing collection of information requirements.\nOn the other hand, if OMB's arguments prevailed, OMB control numbers would be required for all collections of information, whether by \"information collection requests\" or \"collection of information requirements.\" Agencies would then have to submit for OMB review the vast body of existing regulations which did not have OMB control numbers. OMB would conduct its review under the substantive and procedural standards of section 3507, rather than section 3504(h), and any subsequent approval would be limited to no more than three years.\nThe key to this puzzle is clearly section 3504(h). Prior to the adoption of section 3504(h) by the Kennedy Amendment, one could hardly argue that section 3507's procedures for review of \"information collection requests\" could not be applied to reporting or recordkeeping requirements in regulations. 245 Section 3504(h), however, clearly mandates that proposed rules with reporting or recordkeeping requirements be treated in accordance with procedures of that section, rather than with those of\nmeans... ; (2) provide... opportunity for the public to comment... ; and (3) assess the consequences of alternative means . 5S...R.EP. No. 930, 96th Cong., 2d Sess. 88 (1980). It did not refer to a separate OMB disapproval of a collection of information requirement, which did not exist in the bill at that time. Thus, the notion that section 3507(c) represents a careful distinction between \"information collection requests\" and \"collection of information requirements\" does not withstand scrutiny.\n245See OLC Opinion, supra note 234. The OLC Opinion cites various statements in the reports as support for the proposition that the term \"information collection request\" did not necessarily include regulations, even before Senator Kennedy's amendment. The opinion, however, does not go so far as to conclude that, in the absence of Senator Kennedy's amendment, the OLC would interpret the term not to include regulations. See id. at 34-35, 40-41. In any case, OMB's authority under section 3504(h) as reported, see supra note 242, and section 3516 (authorizing OMB to adopt regulations necessary to exercise its authorities under the chapter) would have enabled OMB to require agencies to submit regulations for its approval according to the procedures of section 3507.\n\n1987]\n\nPaperworkReduction Act\n\nsection 3507.246 What is not clear is how section 3504(h), which by its terms deals solely with the adoption of regulations, relates\nto regulations in existence. While this issue was most acute at the time of the Treasury-OMB dispute with respect to the large number of regulations already then in existence, the same issue would recur with respect to all future regulations once adopted pursuant to section 3504(h). If a \"collection of information re-\nquirement\" were a subset of \"information collection request,\" then the control number requirement, the public protection pro-\nvision, and the time limitation all would apply to the regulations. As a result, with the expiration of the time limit, an agency would have to submit an existing regulation to OMB for clear-\nance under section 3507.247 The legislative history of section 3504(h) does not reveal the drafters' intent with regard to existing regulations, probably because those negotiating Senator Kennedy's amendment did not fully perceive the problem. 248\n\n246 OMB appears not to have fully conceded this point. In addition to its argument\nthat \"collection of information requirements\" are a subset of \"information collection requests,\" which are reviewed by OMB pursuant to section 3504(h) rather than section 3507, OMB also argued somewhat inconsistently that agencies could choose to have their proposed regulations reviewed under section 3507 rather than section 3504(h). OLC Opinion, supra note 234, at 48-49. As the OLC Opinion correctly notes, an agency's ability to make such a choice would not only undermine the purpose of section 3504(h), but would also be inconsistent with section 3504(h)'s explicit language (\"each agency shall forward to the Director a copy of any proposed rule which contains a\ncollection of information requirement ... .\" (emphasis in original)). Id. at 49-50. 247Even if OMB assigned a control number to the regulation when it was adopted\npursuant to section 3504(h), the three-year time limitation for the approval would require the regulation to be resubmitted to OMB prior to the lapse of the approval. This resubmission would occur under section 3507, because there would be no new rule-\nmaking proceeding to trigger section 3504(h). 248One key Senate staffer has indicated that he believed there was no problem because\nexisting regulations should already have had time-limited control numbers pursuant to\ntheir review under the Federal Reports Act. Therefore, existing regulations would have to be reapproved or they would be unenforceable under section 3512. Interview with Robert Coakley, Professional Staff Member, Subcommittee on Federal Expenditures, Research and Rules, Senate Committee on Governmental Affairs, in Washington, D.C. (June 20, 1985). Mr. Coakley's understanding of what the practice had been under the Federal Reports Act may well have colored his expectation of what was to occur under the Paperwork Reduction Act. Because that practice was not uniform, however, his\nexpectation was not well founded. Some agencies apparently submitted for clearance reporting requirements contained in rules, Union Oil Co. v. FPC, 542 F.2d 1036 (9th Cir. 1976), but others did not, Olympian Oil Co. v. Schlesinger, [1974-1980 Binder] Energy Mgmt. (CCH) 26,140 (N.D. Cal. Mar. 1, 1979). However, whatever the practice may have been with respect to agencies subject to the Federal Reports Act, for more than half the paperwork burden there had been no practice at all because the Act exempted the Treasury Department from its requirements. Here there would necessarily\nbe questions regarding how already-existing regulations would be brought within the new Act.\nAnother person involved in the negotiation of the Kennedy amendment indicated that the problem of existing regulations was simply overlooked. He noted that the Kennedy amendment was a last-minute fix to a bill that had not received sufficient attention from\n\nHarvardJournalon Legislation [Vol. 24:1\nNevertheless, Senator Kennedy's explanation of the purpose of section 3504(h) does create a strong argument for limiting OMB's power to disapprove regulations adopted after notice and comment. According to Senator Kennedy, the fault with the bill as reported was that it allowed OMB to veto agency regulations after they had been adopted by an agency pursuant to public notice and comment, without providing any procedural rights to either the public or to the agency involved. 49 To allow OMB to overturn an agency rule without having to justify the decision publicly, Senator Kennedy stated, would violate \"basic notions of fairness upon which the Administrative Procedure Act is based, as well as concepts of due process embodied in the U.S. Constitution. ' 25 0 Moreover, under section 3508, OMB could declare an information collection request unnecessary for any reason and thereby block the information collection.25 1 According to Kennedy, his amendment was designed to deal with these concerns by \"limit[ing] the authority of OMB to overturn reporting, recordkeeping, and other information collection requirements adopted by a Federal agency in a rulemaking proceeding ... [and] establish[ing] a procedural scheme which governs OMB's relationship with the Federal agencies. ' 252 The amendment also effectively limited OMB's power to overturn an agency's recordkeeping or reporting requirements to only those circumstances in which OMB finds the agency's response to its comments unreasonable.2 53\nIf regulations once adopted after notice and comment were included in the definition of \"information collection requests,\" OMB could then deal with them under sections 3507 and 3508 and the very evils described by Senator Kennedy would occur.\nlawyers involved with agency regulations and the administrative process. Had they thought of the problem, this person said, they certainly would have ensured that existing regulations not be reviewed under section 3507. Telephone interview with Alan Morrison, supra note 156.\n249126 CONG. REC. 30,178 (1980). Part of the Kennedy Amendment also changed section 3507 to require that OMB's decision be made available to the public. See 44 U.S.C. \u00a7 3507(b) (1982).\n15 126 CONG. REC. 30,178 (1980). 14' 4 U.S.C. \u00a7 3508 (1982). Section 3508 states that the OMB \"may give the agency\nor other interested persons an opportunity to be heard or submit statements in writing\" (emphasis added), thereby indicating that OMB has discretion not to accept public comment before making its decision. But see 44 U.S.C. \u00a7 3517 (1982) (stating that OMB \"shall provide interested agencies and persons early and meaningful opportunity to comment\").\n252126 CONG. REC. 30,178 (1980). 253 See id.\n\n1987]\n\nPaperworkReduction Act\n\nThat is, OMB could invalidate an otherwise lawful regulation in a nonpublic proceeding upon OMB's determination of the merits\nwithout any public justification and without undertaking a new rulemaking. Moreover, agencies wishing to deregulate could merely fail to seek reapproval of a regulation when its OMB approval expired. Such a sub silentio repeal of regulations adopted after notice and comment would be directly contrary to the articulated purposes of section 3504(h).\nThe Office of Legal Counsel viewed these consequences of treating an existing regulation as an \"information collection request\" to so conflict with the purposes of section 3504(h) that this outcome should be avoided if the statutory language and history could be reconciled with an alternative interpretation. 254\nTherefore, the burden shifted to OMB to demonstrate that the language and history of the statute must be interpreted to allow existing regulations to be reviewed under section 3507. Ulti-\nmately, OMB was unable to meet this burden. OMB did not advance its case with its suggested resolution\nof the conflict between the purpose of section 3504(h) and the review of existing regulations under section 3507. OMB argued that, if it disapproved a requirement contained in an existing regulation, the affected agency would still have to undertake a rulemaking to rescind that requirement. 255 That rulemaking then would be subject to the procedures of section 3504(h). This argument, however, did not adequately address the concerns of Senator Kennedy. If OMB's interpretation was correct, OMB's prior disapproval of the existing requirement would be legally effective; the subsequent rulemaking would be a formality, merely rescinding a regulatory requirement no longer in effect. Neither the public procedure, designed to expose OMB's reasoning, nor the substantive limitations on OMB's disapproval authority would be applicable. In short, all of Senator Kennedy's fears would be borne out, but they would be obscured by a formalistic rulemaking.\nThe only way to avoid the conflict and to protect existing regulations from scrutiny under sections 3507 and 3508 is to find that \"information collection requests\" do not include collections of information required by regulations adopted after notice and\n\n,54See OLC Opinion, supra note 234, at 22-27. 21 Id. at 16.\n\n48\n\nHarvardJournalon Legislation [Vol. 24:1\n\ncomment. 2 6 This conclusion is supported by the Act's distinction between \"information collection requests\" and \"collection of information requirements,\" the term used in section 3504(h) with respect to proposed rules. This conclusion necessarily means that the public protection provision, requiring control numbers for \"information collection requests,\" does not apply to regulations. Moreover, because the procedures of section 3504(h) apply only to rulemaking proceedings initiated after the Act's effective date, those procedures cannot be used to review existing regulations. In short, existing regulations requiring reporting or recordkeeping, which were adopted after notice and comment, thereby escape review under both sections 3504(h) and 3507.\nThe seeming anomaly of this conclusion, which cuts against the Act's explicit purpose of reducing existing as well as future\npaperwork burdens, had probably been OMB's best argument. 257 The OLC Opinion provided a possible solution. Although the fifty-six page opinion barely addressed the issue in its body, the final paragraph stated that OMB does have \"substantial authority over existing regulations\" under section 3504(b).258That provision identifies OMB's \"general information policy functions,\" which include \"initiating and reviewing pro-\n\n216OLC concluded that regulations adopted by agencies without notice and comment\ncould be categorized as \"information collection requests.\" See id. at 24-25. First, section\n3504(h)(8) explicitly limits application of section 3504(h) to those instances \"when an agency publishes a notice of proposed rulemaking and requests public comments.\" Second, the entire thrust of section 3504(h) was to protect the integrity of the public rulemaking process. See id. If a regulation were adopted without notice and comment, there would be no public process to protect. Whether a regulation can be characterized as an \"information collection request\" depends on whether it was in fact adopted in a\npublic proceeding, not whether it was required to be adopted in a public proceeding. Thus, if an agency utilizes notice-and-comment rulemaking, even if it is not required to do so, the regulation resulting from the rulemaking is not an \"information collection request.\" This was important to Treasury, because while the IRS normally used noticeand-comment rulemaking, the IRS had historically maintained that its regulations were interpretative and therefore exempt from the Administrative Procedure Act's require-\nments for public procedure. See 2 K. DAVIS, ADMINISTRATIVE LAW TREATISE \u00a7\u00a7 7:17, 7:21 (2d ed. 1979).\nEven so limited, however, the inclusion of any regulation within the term \"information collection request\" produces results which are unique in administrative law. OMB retains a veto over an agency's otherwise lawful regulation and an agency may effectively invalidate its own regulation by inaction. A recent article describes some of the confusion which may result. See Allen, Price CertificationsPose Dilemmas for Contractors, Legal Times, May 12, 1986, at 27.\n27 IRS regulations accounted for almost half of the federal government's paperwork burden. See 1982 ICB, supra note 5, at 13. Thus, if OMB were unable to review the IRS regulations, it would be impossible to achieve the statutory goal of a 25% reduction in the preexisting paperwork burden by October 1, 1983. See 44 U.S.C. \u00a7 3505(1) (1982).\n258 OLC Opinion, supranote 234, at 56.\n\n1987]\n\nPaperwork Reduction Act\n\nposals for changes in legislation, regulations, and agency procedures to improve information practices. 259 The Office of Legal Counsel saw nothing to stop OMB from initiating proposals under this provision for changes in existing regulations.2 60 Thus, while OMB might not be able directly to review existing regu-\nlations under the Act, it could require that agencies wishing to change existing regulations undertake rulemaking. Such rulemaking would be subject to section 3504(h).\nThis solution to the problem of how to preserve OMB's power over existing regulations while remaining consistent with the purposes of section 3504(h) was ingenious, but its interpretation of section 3504(b) is questionable. This provision probably was\nnot intended to refer to regulations containing specific reporting and recordkeeping requirements. The context of the provision\nsuggests that Congress meant it to deal with \"information policy\" as opposed to \"paperwork control\" functions.2 61 Moreover, the term \"regulations\" likely refers to those regulations which govern agency information practices generally. Nevertheless, without any analysis, the OLC opinion suggested that the provision would provide a legal basis for OMB to require agencies\nto propose changes in particular regulations containing reporting or recordkeeping requirements.\nDespite this favorable interpretation of section 3504(b), the opinion was widely viewed as a substantial defeat for OMB.2 62\nThe defeat was particularly painful because, only shortly before the Office of Legal Counsel issued its opinion on June 22, 1982,\nOMB had issued a draft guidance document to all agencies pointedly reflecting OMB's interpretation of the Act with respect to existing regulations.2 63 Indeed, there is some evidence\nthat OMB's initial reaction to the OLC Opinion was to consider ignoring or appealing the opinion.264 Before long, however, OMB\n\n-9 44 U.S.C. \u00a7 3504(b)(2) (1982). 260 OLC Opinion, supra note 234, at 56. 261Section 3504(b) is a list of the Director's \"general information policy functions,\"\nwhereas section 3504(c) is a list of the Director's \"information collection request clearance and other paperwork control functions.\"\n262See, e.g., Moore, DOJ Restricts OMB's PaperReview Power, Legal Times, June 28, 1982, at 1.\n263Draft OMB Circular No. A-40, issued for comment on June 4, 1982. 214At the time, the Deputy Administrator of OIRA, James Tozzi, intimated that OMB\nhad waited long enough for the Office of Legal Counsel to reach a position, and Tozzi further suggested that if that Office's interpretation differed from OMB's, OMB might well ignore it. Apparently, informal soundings of the Attorney General by administration officials led OMB not to pursue such an extraordinary route. Interview with Robert Coakley, supra note 248.\n\nHarvardJournalon Legislation [Vol. 24:1\nperceived the power implicit in the ability to \"initiat[e] ... proposals for changes in... regulations ... to improve information practices. \"'265\n\nC. The OMB Regulations\n\nOn September 8, 1982, OMB proposed a regulation to imple-\n\nment the Director's paperwork control functions under the Act.266 In addition to the procedures for forms under section\n\n3507 and for proposed rules under section 3504(h), OMB created\n\na special procedure applicable to existing regulations. The pro-\n\ncedure mandated that all reporting and recordkeeping require-\n\nments in existing regulations be submitted periodically to OMB\n\nfor review. This review would duplicate that performed under\n\nsection 3507, except that OMB would only \"initiate proposals\n\nfor change in the requirement, '267 rather than disapprove the\n\nreporting or recordkeeping requirement in the agency regula-\n\ntion. The agency would have a reasonable period of time, not\n\nto exceed 120 days, to publish a notice of proposed rulemaking\n\nwith respect to the requirement, and the procedures of section\n\n3504(h) would would remain\n\nthen come into play.268 The in effect during the pendency\n\neoxfitshtiengrulreemquaikrienmge.2n6t9\n\nOMB's proposed rule also required OMB control numbers for\n\nall collections of information, whether they were \"collection of\n\ninformation requirements\" or \"information collection re-\n\nquests.\"270 OMB noted that, while the OLC Opinion had con-\n\ncluded that the Act required OMB control numbers only for\n\n\"information collection requests,\" the opinion \"did not address\n\nwhether the Director could, in the exercise of his policymaking\n\nand regulatory authorities under the Act, require that agencies\n\ndisplay a valid OMB control number on information collection\n\nrequirements in agency regulations. '2 7' OMB gave several rea-\n\nsons for requiring control numbers for regulations, among which\n\nwas the importance of the control number as a mechanism to\n\n26544 U.S.C. \u00a7 3504(b)(2) (1982). 266Controlling Paperwork Burdens on the Public, 47 Fed. Reg. 39,515 (1982) (codified\nat 5 C.F.R. pt. 1320). 267Id. at 39,528 (codified at 5 C.F.R. \u00a7 1320.14(d)).\n2 Id. at 39,529 (codified at 5 C.F.R. \u00a7 1320.14(h)). 269Id. at 39,528 (codified at 5 C.F.R. \u00a7 1320.14(d)). 270Id. at 39,527-28. 271 Id. at 39,517.\n\n1987]\n\nPaperworkReduction Act\n\nmonitor the paperwork burden. 272 OMB did not indicate what effect failure to display a control number on a regulation would have, stating that this issue \"must ultimately be determined by the courts. 273\nWhile the proposed rule purported to recognize the force of the OLC's reasoning, the tone of the preamble seems hostile toward the OLC Opinion. 274 The consistency of the proposed rule with the opinion was questionable enough that OMB asked the Office of Legal Counsel to issue a memorandum essentially blessing OMB's approach. 275 OLC's memorandum in response did not suggest any reconsideration of the earlier opinion's conclusions. Rather, it restated forcefully OMB's power under section 3504(b)(2) to initiate proposals for change in existing regulations. 276 The memorandum then stated that OLC had no legal objection to seven specific elements of OMB's proposed regulations: (1) the mandatory submission to OMB by July 1, 1983, of all existing regulations not previously approved by OMB, to be resubmitted thereafter at least every three years; (2) OMB's review of these submissions under the substantive standard of section 3504(c)(2), which requires that the information be \"necessary for the proper performance of the functions of the agency,\" and under OMB's authority to direct the agency to initiate a proposal for change if OMB determines that the regulation is unduly burdensome or unnecessary; (3) the legal obligation of the agency then to prepare and publish a notice of proposed rulemaking as to whether to retain, rescind, or modify the collection of information requirement in the regulation; (4) OMB's authority to treat the notice of proposed rulemaking under section 3504(h), including the authority to disapprove any\n\n272Id. at 39,518. Other reasons included the following: requiring control numbers was\nconsistent with legislative intent;.the fact that many agencies were routinely submitting regulations to OMB for assignment of control numbers demonstrated the feasibility of the process; the presence of control numbers would alert the public to the application of the Act; and displaying control numbers on regulations would ensure that, if a court disagreed with the OLC's opinion that section 3512 (the public protection provision) did not apply to regulations, the regulatory requirement would not be upset. Id.\n273 Id. at 39,519. 274 Id. at 39,517. This is particularly noticeable in OMB's repeated statement that the\ncourts might disagree with the OLC Opinion and hold a regulation unenforceable because it failed to display a control number. See supra note 272.\n275 See Memorandum from Robert B. Shanks, Deputy Assistant Attorney General, Office of Legal Counsel, to Michael E. McConnell, Assistant General Counsel, Office of Management and Budget, re: Paperwork Reduction Act (Sept. 24, 1982) [hereinafter September 24 OLC Memorandum].\n276 Id. at 1.\n\nHarvardJournalon Legislation [Vol. 24:1\nrequirement in the proposed rule under the standards contained in section 3504(h)(5); (5) OMB's authority to disapprove a collection of information requirement in an existing regulation if the agency involved refuses to comply within a reasonable time with an OMB directive to initiate a change or to publish a final rule containing the new requirement; (6) OMB's authority to disapprove not only the new proposed requirement but also the preexisting requirement; and (7) OMB's authority to require agencies to display control numbers on regulations containing approved collection of information requirements, although the absence of a control number would not invalidate the collection of information requirement. 277\nWith two exceptions, these seven elements of OMB's proposed rules are fully consistent with the earlier OLC opinion. 278 The two exceptions relate to OMB's authority under certain circumstances to disapprove, and thereby render legally invalid, collection of information requirements in existing regulations. First, the OLC memorandum stated that if an agency failed to comply with an OMB directive to issue a notice of proposed rulemaking amending a collection of information requirement in an existing regulation, or failed to publish a final rule after the proposed rule, OMB could simply disapprove the existing requirement in the regulation. 279 Second, the memorandum sanctioned OMB's authority to disapprove not only newly proposed requirements, but also the underlying, existing requirements which the new proposals were designed to replace.2 80 An agency, therefore, would not be able to protect existing regulations by merely proposing a new requirement which would be equally objectionable to OMB. The OLC memorandum stressed, however, that OMB's power to disapprove existing regulations was subject to the limitations of section 3518(e)-that nothing in the\n277Id. at 2-3. 278 OMB's authority to require agencies to display control numbers on regulations purely for inventory and management purposes would seem to raise little question under the Act, see 44 U.S.C. \u00a7\u00a7 3504(b), 3516 (1982), and OLC was careful to make clear that the absence of a control number, in its view, would not have legal consequences as to the validity of the collection of information requirement. The ability of OMB to require another agency to propose a change to an existing regulation containing collection of information requirements raises more questions under the Act, but these questions are less serious than those raised by OMB's original position. Moreover, the original OLC opinion, supra note 234, clearly suggested that OMB had such a power under section 3504(b)(2) of the Act. 279 September 24 OLC Memorandum, supra note 275, at 3.\n= Id.\n\n19871\n\nPaperworkReduction Act\n\nAct \"be interpreted as increasing or decreasing the authority of [OMB] with respect to the substantive policies and programs of departments, agencies and offices ....,,281\nIn its original opinion, OLC had found no authority for OMB to disapprove collection of information requirements in existing regulations,282 and the later memorandum is clearly inconsistent with the opinion in this regard, even in the limited circumstances addressed. While an agency's disregard of a lawful OMB directive to initiate a rulemaking may be improper and even unlawful, it does not necessarily follow that the remedy for such improper or unlawful conduct is to give OMB the power to invalidate that agency's otherwise lawful regulations. OMB has other means of enforcing its will on recalcitrant agencies. Moreover, the guilt of the agency has little to do with the validity of the regulation, which may well serve valid public interests. The purpose of section 3504(h) is to assure that all parties with interests in the regulation will have an opportunity to address its merits and demerits. The effect of OLC's memorandum is to allow a bureaucratic dispute (or conspiracy) between OMB and an agency to result in the invalidation of regulations without public process. This is directly contrary to the underlying analysis of the original OLC opinion, which relied on the legislative history of the Kennedy Amendment to ensure that OMB could not invalidate regulations adopted after notice and comment except through the rulemaking process.2 83\nThe memorandum's authorization of OMB power to disdipprove existing requirements after public process may seem less egregious, but it too is inconsistent with the OLC opinion. The OLC opinion concluded that section 3504(h) applied only to rules proposed after the effective date of the Paperwork Reduction Act.284 Moreover, the opinion stated that under section 3504(h), OMB could disapprove only those rules on which it commented during the comment period or which were not submitted to it for comment.2 85 When an agency proposes a new rule containing a collection of information requirement to replace an existing rule, OMB could, subject to the standards enumerated in section 3504(h), disapprove that new rule. But\n\n281 Id. at 2. m OLC Opinion, supra note 234, at 31. 283Id. at 22, 24.\nn4 Id. at 31-32. 2- See 44 U.S.C. \u00a7\u00a7 3504(h)(4), (5)(B); OLC Opinion, supra note 234, at 23.\n\nHarvardJournal on Legislation [Vol. 24:1\njust as would be the case if the agency never adopted the new rule or a court invalidated the new rule, the old rule would remain in effect because the agency had never lawfully revoked it. As the Supreme Court made clear in Motor Vehicle ManufacturersAssociation v. State FarmMutualInsurance Corp.,286 the revocation of an existing rule requires a rulemaking. Under section 3504(h), OMB's power of disapproval applies only to the regulation adopted in the rulemaking proceeding subject to that section. Just as OMB cannot, under section 3504(h), create a collection of information requirement more to its liking when its comments have failed to persuade an agency that change is desirable, neither can OMB directly eliminate an existing requirement when its comments have not moved an agency to revoke the requirement through the rulemaking process.\nOLC's September 24 memorandum does not contain any explanation for or analysis of its conclusions, so one cannot know whether the inconsistencies with its earlier opinion were inadvertent or deliberate. Given the pressure that OMB brought upon OLC, however, the characterization of the memorandum as reflecting a compromise between OMB and OLC may well be accurate.2 87 The public perception was clearly that OLC had retreated from its opinion, 288 and while part of that perception was a result of the media's misreading of the original opinion,289 there appears to be some truth behind the perception.\nOLC's memorandum enabled OMB to assure agency officials that its proposed rulemaking was not inconsistent with OLC's original opinion.290 A memorandum from OMB to the agencies stated that OMB and OLC had consulted extensively, that OMB was in full agreement with the Justice Department, and that its position had been reviewed and approved by OLC. 29 1 The memorandum contained a summary narration of the procedures applicable to the review of existing regulations, although it failed to specify OMB's power to disapprove existing regulations in the two circumstances described above.292\n-463 U.S. 29 (1983). 2 See Tax Notes, Nov. 1, 1982, at 406; Tax Notes, Nov. 15, 1982, at 563-64. m See, e.g., Moore, New Justice Opinion Okays OMB Proposal On Paper Work, Legal Times, Nov. 8, 1982, at 1; Legal Times, Nov. 15, 1982, at 22. 2 See, e.g., Moore, DOJ Restricts OMB's PaperReview Power, Legal Times, June 28, 1982, at 1. 2 See Memorandum from Christopher DeMuth, supra note 223. 29, Id. at 1. 29 Memorandum from Christopher DeMuth, supra note 223.\n\n1987]\n\nPaperworkReduction Act\n\nOn March 31, 1983, OMB published the final rule implementing its paperwork control functions under the Act.293 In the preamble, OMB acknowledged that one of the major issues of concern to the public and the agencies was OMB's professed authority over existing regulations. 294 What followed was an extensive legal brief arguing that OMB had \"full authority\" over existing regulations. 295 Section 3504(h), OMB said, \"is solely a procedural limitation on OMB authority [over regulations] and does not diminish OMB's substantive review powers.1 296 The only issue, therefore, was the procedure to be used with respect to existing regulations, inasmuch as section 3504(h)'s procedures applied only to proposed rules. Exactly what OMB meant by \"substantive review powers\" was not entirely clear. OMB may have meant that, in reviewing existing regulations to determine if it should direct an agency to initiate a rulemaking, OMB would apply the same standard it applies in reviewing information collection requests; that is, OMB would determine whether the collection was necessary to the proper performance\nof the agency's functions. This interpretation would be consistent with the September 24 OLC memorandum. 297 On the other hand, OMB may have meant that it retained the substantive\npower generally to disapprove existing regulations. That this may have been the intended meaning was suggested by OMB's statement of its authority to disapprove collection of information requirements as well as proposed replacements for existing regulations. OMB supported this assertion by an intentional or convenient misquotation from the September 24 OLC memorandum. 298\nIn any case, OMB concluded that the result of its interpre-\ntation was consistent with that of OLC's opinion, which rec-\nognized OMB's \"broad powers ... to initiate and review pro-\n\n291Controlling Paperwork Burdens on the Public, 48 Fed. Reg. 13,666 (1983) (codified at 5 C.F.R. pt. 1320).\n9 Id. at 13,667.\n295 Id. at 13,668. Id.\n297See September 24 OLC Memorandum, supra note 275, at 2.\n.91The OLC memorandum stated that \"OMB does have authority... to disapprove collection of information requirements in proposed replacements for existing regulations.\" September 24 OLC Memorandum, supra note 275, at I (emphasis added). OMB's purported quotation of that sentence, as printed in two different places in the Federal Register, stated that \"OMB does have authority ... to disapprove collection of information requiremer ts and proposed replacements for existing regulations.\" Controlling Paperwork Burdens on the Public, supra note 293, at 13,668.\n\nHarvardJournalon Legislation [Vol. 24:1\nposals for changes in existing regulations.\" 299 OMB's interpretation resulted in essentially the same procedure for dealing with existing regulations as it had originally proposeda one-time comprehensive review of all collection of information requirements in existing regulations which had not yet been approved by OMB. 300 If OMB determined that a collection of information requirement was unnecessary or unduly burdensome, then it could direct the appropriate agency to initiate a rulemaking subject to section 3504(h) in order to change the requirement. 30 1 Regulations would continue to be periodically reviewed, at least every three years, 30 2 similar to information collection requests which cannot be approved for a period in excess of three years.303 Again, if the periodic review found the regulation unnecessary or unduly burdensome, OMB would direct the agency to initiate a rulemaking to change it.104 The\nexisting regulation would remain in effect during the pendency of the proceeding. 30 5 0MB drew special attention to its power\nto disapprove collection of information requirements in existing\nregulations in the two particular circumstances allowed by the September 24 OLC memorandum 30 6 and, indeed, specified these circumstances in the regulations themselves.3 0 7\nThe final rule did not alter the proposed rule's treatment of\ncontrol numbers for regulations. With the blessing of the OLC\nControlling Paperwork Burdens on the Public, supra note 293, at 13,666, 13,668. 0 5 C.F.R. \u00a7 1320.14 (1986). 301Id. \u00a7\u00a7 1320.14(f), (g). 'o2 Id. \u00a7 1320.14(d). 10144 U.S.C. \u00a7 3507(d) (1982).\n5 C.F.R. \u00a7 1320.14 (1986). 35 Id. \u00a7 1320.14(f). ' See Controlling Paperwork Burdens on the Public, supra note 293, at 13,681. The preamble purports to distinguish the disapproval of collection of information requirements in existing regulations from the disapproval of the regulations themselves. \"OMB ... neither claims to have nor has any power under the Act to disapprove any regulation. Its authority is limited to collections of information.\" Id. (emphasis in the original). Nevertheless, OMB's definition of \"collection of information\" expressly includes \"rules or regulations,\" 5 C.F.R. \u00a7 1320.7(c)(1) (1986), and OMB made clear in its preamble that if it disapproves requirements imposed by regulations, then the requirements are \"invalidated\" and \"may not be enforced against the public.\" Controlling Paperwork Burdens on the Public, supra. Clearly, by asserting the power to disapprove a collection of information, OMB does assert the power to render a regulation invalid. To claim that this is not the power to disapprove a regulation is only to make a semantic distinction. 3075 C.F.R. \u00a7\u00a7 1320.14(h), (i) (1986). In fact, OMB has yet to utilize this claimed power to disapprove a collection of information requirement in an existing rule because an agency has failed to initiate or complete a rulemaking as directed by OMB, though it has on occasion been tempted. Interview with OIRA official in Washington, D.C. (June 17, 1985).\n\n1987]\n\nPaperworkReduction Act\n\nmemorandum, OMB required all collection of information requirements to display OMB control numbers.0 8 The preamble reiterated that the authority for such a requirement derived not\nfrom section 3507(f) of the Act, which required control numbers for information collection requests, but from OMB's rulemaking power30 9 and general management responsibilities under the Act. 310 The preamble further explained that this requirement was legally binding on the agencies, even though it was not directly mandated by the Act.31t In the Public Protection pro-\nvision of the regulations, OMB continued to be faithful to the\nOLC opinion and expressly denied that the absence of a control\nnumber on a regulation would itself invalidate the collection of information requirement in the regulation. 312 Unlike an infor-\nmation collection request, which would be unenforceable without a control number, 31 3 a collection of information requirement lacking a control number \"will alert the public that either the agency has failed to comply with applicable legal requirements\nor the collection of information requirement has been disapproved. ' 314 In the latter circumstance, OMB provided that the\ncollection of information would be unenforceable by reason of the disapproval. 315 0MB was silent as to what the legal effect might be in the former circumstance of an agency's \"fail[ure] to\ncomply with applicable legal requirements.\" Rather, OMB\nstated that \"a member of the public confronting a collection of information requirement with no control number will be able to\nmake further inquiries and may find that the requirement is unenforceable. 316\nThe publication of the final rules implementing the Paperwork\nReduction Act ended the initial struggle over OMB's authority.\nThe Office of Legal Counsel rejected OMB's initial view of its\nauthority-the ability to disapprove existing regulations adopted\n\n3035 C.F.R. \u00a7 1320.4(a), 1320.13(f), (h), 1320.14(c), (e), (f) (1986). \"9 See 44 U.S.C. \u00a7 3516 (1982). 310Controlling Paperwork Burdens on the Public, supra note 293, at 13,666-70.\n31Id. at 13,669. The preamble relies particularly on 44 U.S.C. \u00a7 3518(a) (1982), which provides that agencies' authority to prescribe rules for federal information activities \"is subject to the authority conferred on the Director by this chapter.\"\n31 5 C.F.R. \u00a7 1320.5(b) (1986). \" Id. \u00a7 1320.5(a); 44 U.S.C. \u00a7 3512 (1982)(stating that no person shall be subject to any penalty for failure to provide information if the information collection request does not display a current control number). 3'4 5 C.F.R. \u00a7 1320.5(b) (1986).\n315 Id. 316Controlling Paperwork Burdens on the Public, supra note 293, at 13,671.\n\nHarvardJournalon Legislation [Vol. 24:1\nafter notice and comment without any public procedure. Nevertheless, OMB retained the authority to require agencies to institute rulemakings to change collection of information requirements, and in those rulemakings OMB could then substantively affect the collection requirements, even to the point of disapproving them if the agency was not responsive to OMB's suggestions. OMB's job was a bit more difficult procedurally, but OMB in effect retained the power to review existing regulations. Treasury may have won in principle, but it lost in actual effect, for it could not keep its tax regulations from OMB scrutiny.\nSimilarly, the OLC opinion rejected OMB's original interpretation of the Act, which would have limited the period of time for which a collection of information requirement could be approved, required control numbers on collection of information requirements, and subjected collection of information requirements to the public protection provision. Nevertheless, OLC did allow OMB to require resubmission of approved regulations for review at least every three years and to require control numbers for regulations. Although the absence of a control number did not make the regulation unenforceable, it could at least alert the public to the fact that something was wrong. Again, Treasury had won in principle, but OMB retained the substantive power it desired.\nD. CongressionalReaction\nAlthough OMB and Treasury had reached an accommodation of sorts, others were not satisfied. For example, some business groups, while reassured by the OMB-Justice compromise as reflected in the regulations, remained unhappy. 3 7 More importantly, Senator Lawton Chiles, sponsor of the Act in the Senate, was personally offended by the OLC opinion and what he viewed as its flouting of congressional intent. 318 The need to reauthorize the Act 319 provided Congress with an opportunity to amend it. Senate and House bills each provided a different solution to the problem. 320 The House Bill amended section\n3171983 Senate Oversight Hearings, supra note 2, at 41 (statement of James D. McKevitt, National Federation of Independent Business).\n318Interview with Robert Coakley, supra note 248. 319 44 U.S.C. \u00a7 3520 (1982) (effective April 1, 1981) authorized appropriations for fiscal\nyears 1981 through 1983 only. 320 S. 2433, 98th Cong., 2d Sess., 130 CONG. REC. S2790 (daily ed. March 15, 1984);\nH.R. 2718, 98th Cong., 1st Sess., 129 CONG. REC. H9271 (daily ed. Nov. 7, 1983).\n\n1987]\n\nPaperworkReduction Act\n\n3504(h) to eliminate the term \"collection of information requirement,\" substituting the phrase \"information collection request ... contained in, or derived from, a rule or regulation. '32' The\n\nprocedure was also changed. Rather than having OMB comment\n\non the proposed rule, the new provision would have OMB use\n\nthe same procedure for information collection requests con-\n\ntained in proposed rules as it used for other information collection requests. 322 To avoid the concerns that spawned the Ken-\n\nnedy Amendment, the provision required OMB approval of the request before the final rule could be published by the agency.323\n\nThe provision also required that any written communications\n\nbetween OMB and the agency be made part of the rulemaking\n\nrecord, and that any changes in the proposed and final forms of the information collection be explained by the agency. 324 The\n\namendment specifically required agencies to submit existing reg-\n\nulations containing information collection provisions to OMB\n\nfor review pursuant to section 3507.325 Here, however, OMB\n\ncould disapprove the collection of information in a rule without any public proceeding. 32 6 The amendments explicitly stated that\n\nboth OMB disapproval of and the absence of a control number\n\non an would\n\ninformation collection request render the request invalid and\n\nucnoenntafoinrecdeabinle.a32r7egulation\n\nThe Senate's solution was simpler. The term \"information\n\ncollection request\" would be defined to include \"collection of information requirements. '328 In this way, the procedures of\n\nsection 3504(h) would continue to apply to collection of infor-\n\nmation requirements contained in proposed rules, but the pro-\n\nvisions of the Act that refer only to \"information collection\n\nrequests\"-such as the public protection provision, the require-\n\nment for display of a control number, and the three-year limi-\n\ntation on the period of approval-would also apply to the collection requirements. 329 Although the issue of OMB's authority\n\nover existing regulations had given rise to the OLC opinion, the\n\nSenate bill made no specific attempt to resolve that problem.\n\n321 H.R. 2718, supra note 320.\n322 Id. 323 Id.\n-12H1 .R. REP. No. 147, 98th Cong., 1st Sess. 8, 16, 24-26 (1983). 115Id. at 8, 28. 26See id. at 8. 3127Id. at 8, 25-26. 328S. REP. No. 576, 98th Cong., 2d Sess. 16, 27 (1984). 329Id. at 16.\n\nHarvardJournalon Legislation [Vol. 24:1\nBy the time of the Senate report, however, agencies were supposed to have already submitted all such regulations to OMB for review. 330\nThe Senate report purportedly addressed the concerns of those who believed that agencies would use the Act as a means of avoiding a rulemaking to rescind their regulations. Some feared that agencies might try to rescind a rule imposing a reporting or recordkeeping requirement merely by failing to resubmit the requirement to OMB when the prior approval expired. 331 The report stated, however, that OMB did not believe this to be a problem because OMB's regulation required agencies to resubmit previously approved collections of information to OMB prior to their expiration date.332 Moreover, the Report continued, the Administrative Procedure Act requires agencies to undertake rulemakings to rescind or amend a rule. Thus, the rule would remain in effect after its clearance expired, but the agency simply would not be able to enforce it because of the public protection provision. 333 For those concerned about an agency using the Paperwork Reduction Act as a means of avoiding a rulemaking to rescind its regulations, these assurances could not have provided much comfort. Reliance on OMB to ensure that .an agency maintains reporting and recordkeeping requirements which it wishes to eliminate seems unrealistic. Moreover, if an agency cannot enforce its reporting and recordkeeping requirements, it makes little sense to say that the rule remains in effect.\nFears about agencies seeking to rescind regulations without public process were not totally unfounded. For example, in 1984, coincident with the Senate Committee on Governmental\n330The Senate Report was ordered to be printed August 6, 1984. Id. at 1. Under\nOMB's regulations, agencies were to submit all previously unapproved collection of information requirements contained in regulations to OMB not later than December 31, 1983. 5 C.F.R. \u00a7 1320.14(a) (1986).\n331S. REP. No. 576, supranote 328, at 16. 332 Id. at 17 (citing 5 C.F.R. \u00a7 1320.14(a)(1986)). It is doubtful whether the regulatory provision cited would indeed require submission of a previously approved collection of information to OMB if an agency had decided that it wished to discontinue the collection. The agency would not be proposing a collection of information, and both section 3504(h) and section 3507 are phrased in terms of OMB approving or disapproving a collection of information requirement or information collection request proposed by an agency. For example, OSHA recently proposed to replace a number of recordkeeping requirements with certifications. OSHA claimed that these revisions, because they would not require a collection of information, would not be subject to the Paperwork Reduction Act. See 51 Fed. Reg. 312, 313 (1986). 333S. REP. No. 576, supranote 328, at 17.\n\n1987]\n\nPaperworkReduction Act\n\nAffairs' consideration of these amendments, the Department of Energy (DOE) was involved in an attempt to rescind certain\nrecordkeeping requirements relating to price controls on crude oil and petroleum products. 334 While the price controls them-\nselves had been rescinded in January 1981, DOE continued to require all firms to retain indefinitely their records regarding compliance. 335 OMB had conditioned approval of these record-\nkeeping requirements on DOE's initiation of a rulemaking to narrow the requirements to those records necessary for investigation and enforcement actions against firms which had violated the controls. 336 DOE, however, was under congressional\npressure not to take any action which might endanger enforce-\nment activities, and its proposals to limit the recordkeeping requirements were perceived as having that effect. 337 As a result, DOE officials considered merely letting the period of OMB approval lapse, rather than going through a public rulemaking to rescind parts of the recordkeeping requirement. 338 In this way, DOE perhaps hoped to direct congressional wrath away from\nitself and toward OMB, which had, after all, moved to curtail\nthe requirement. DOE premised this plan on its belief that the absence of OMB approval would render the recordkeeping requirement unenforceable under the public protection provision. 339 OMB, however, would not join in this plan and insisted that it would extend the approval period even if DOE did nothing. 340 Consequently, DOE did seek an extension of the ap-\nproval, proposed a rule to reduce the recordkeeping requirements, and finally adopted that rule with some changes. 341\nDOE's premise that the lapse of the period of approval for the requirement in the regulation would render the requirement unenforceable was inconsistent with both the OLC opinion and\n\n114 See generally Letter from Robert P. Bedell, Deputy Administrator, Office of Information and Regulatory Affairs, OMB, to Rep. John D. Dingell (D-Mich.), Chairman of the Subcomm. on Oversight and Investigations of the Comm. on Energy and Commerce (June 21, 1985) (on file at HARV. J. ON LEcIS.) [hereinafter Bedell Letter].\n331Id. at 5. 336Id. at 6-10.\n337See, e.g., Letter from Rep. John D. Dingell, Chairman of the Subcomm. on Oversight and Investigations of the House Comm. on Energy and Commerce, to John S. Herrington, Secretary of Energy (April 30, 1985) (on file at HARV. J. ON LEGIS.) [hereinafter Dingell Letter].\n3 Interview with anonymous OMB official in Washington, D.C. (June 17, 1985). 339Id.\n340 Bedell Letter, supra note 334, at 19. 34, See id. at 9-11.\n\n62\n\nHarvardJournal on Legislation [Vol. 24:1\n\nOMB regulations. 342 This, however, would have been changed by the amendments proposed by the Senate committee. Had those amendments been in effect, DOE's no-action alternative could have resulted in the elimination of the recordkeeping requirement. Even a temporary loss of that requirement, more-\nover, could have resulted in a permanent loss to the government of the ability to examine the records subject to the requirement. The concern about the effect on enforcement was real, and despite the assurances contained therein, the Senate committee's report does not appear to have considered this problem. Instead, the report seems to have assumed that there would be\nonly a delay in the enforceability of the requirement. The report seems not to have recognized that as a result of a temporary lapse of a requirement to retain records a regulated entity could legally destroy records necessary for enforcement.\nThe House bill was passed without a recorded vote in November 1983. 343 The Senate, however, gave neither the Senate nor House bill floor consideration, and the impetus for substan-\ntive amendments to the Act died. 344 OMB was satisfied with its authority under its regulations and did not need additional pow-\ners to achieve control over agency paperwork. 345 The Treasury\n\n342 The OLC opinion had made it clear that the public protection provision and the\ntime limitations applicable to information collection requests were not applicable to collection of information requirements in regulations. OLC Opinion, supra note 234, at\n36-40. Moreover, OMB's final regulation made it clear that, while OMB could require control numbers for collection of information requirements and limit the period for those numbers' approval, the absence, or lapse, of a currently valid control number did not render a collection of information requirement unenforceable. See 5 C.F.R. \u00a7 1320.5(b) (1986).\n3 See 123 CONG. REc. H9271-73 (daily ed. Nov. 7, 1983). 34 In the spring of 1986, other events created a new motivation for reauthorization and substantive amendments. Upset with certain OMB actions, some House members sought to cut off all appropriations to OIRA, utilizing the lack of an authorization as a parliamentary weapon to accomplish this end. See Dingell Moving to Revoke Some OMB Regulatory Review Powers, INSIDE THE ADMINISTRATION, May 15, 1986, at 1. See also infra notes 345, 512, and 544. 3451983 Senate Oversight Hearings,supranote 2, at 7. The absence of any provision authorizing appropriations for carrying out the Paperwork Reduction Act was not at all burdensome to OMB. To the contrary, one of the House's purposes in attempting to reauthorize the Paperwork Reduction Act was to require OIRA to spend the authorized\nfunds only for activities relating to the Paperwork Reduction Act, not for the purpose of regulatory oversight or reform generally. See H.R. 2718, \u00a7 11, 98th Cong., 1st Sess., 129 CoNG. REc. H9271 (daily ed. Nov. 7, 1983). This was in response to a finding that OMB had utilized OIRA personnel and appropriations authorized under the Paperwork Reduction Act to support regulatory oversight and reform activities under E.O. 12,291, contrary to Congress' desires. See H.R. REP. No. 147, supra note 324, at 12-15. This restriction was opposed by OMB. See S. REP. No. 576, supra note 328, at 12-15. Absent any reauthorization provision, OMB was free to spend funds appropriated to it for any of its functions without having to account for Paperwork Reduction Act activities\n\n1987]\n\nPapeiworkReduction Act\n\nDepartment was also content. 46 The current framework, which clearly distinguishes between collection of information requirements contained in regulations adopted pursuant to notice and comment and all other collections of information, generally harmonizes the procedures of the Administrative Procedure Act with the Paperwork Reduction Act. This was the result of OLC's interpretation of section 3504(h). OMB's prior interpretation, as well as the amendments proposed by the House and Senate in 1983 and 1984, would have each raised significant questions. All would have enabled agencies to invalidate regulations through unilateral action without public procedure. While Congress probably could provide such power to agencies if it so desired, it is unlikely that it intended to do so here. Instead, Congress probably failed to recognize the potential consequences of the amendments and their conflict with the Administrative Procedure Act.\n\nE. The Inherent Conflict-Reporting Requirements Under the Administrative ProcedureAct\nTo a large degree, any attempt to treat equally paperwork requirements not contained in regulations and paperwork requirements contained in regulations is bound to raise conflicts.\nseparately from E.O. 12,291 or E.O. 12,498 activities. See E.O. 12,291, supra note 17 and Exec. Order No. 12,498, 3 C.F.R. 323 (1985), reprinted in 5 U.S.C. \u00a7 601 note (Supp. III 1985).\nIn October 1986, as part of the continuing appropriations resolution funding federal activities pending fiscal year 1987 appropriation acts, Congress enacted a three-year reauthorization of appropriations for OIRA. See Pub. L. No. 99-500, Title VIII, 100 Stat. 1783 (1986). The provision also creates a special line-item budget account for OIRA and limits the use of the funds authorized to functions under the Paperwork Reduction Act. Id. Some congressional personnel apparently believe that this latter limitation will restrict OIRA activities under E.O. 12,291 and E.O. 12,498. See OMB WATCH, PAPERWORK REDUCTION: THE QUICK Fix OF 1986 9 (Nov. 1986). This belief proved unfounded with respect to the original Act's authorization, see infra note 460 and accompanying text, and there is little reason to believe that OMB will not allocate funds from other sources to OIRA for the Executive Order activity, just as it has done in the past. The continuing resolution also made a number of amendments to the Paperwork Reduction Act itself. See generally id. Perhaps most significantly, the appointment of future OIRA administrators is made subject to Senate advice and consent.\n346Interview with anonymous Treasury official in Washington, D.C. (June 18, 1985). This is especially true because OMB has been responsive to Treasury requests for expedited clearance and has winked at Treasury's failure to give public notice of its submissions to OMB of information collection requests contained in regulations not adopted after notice and comment. Id. This latter practice would appear to be a clear violation of 44 U.S.C. \u00a7 3507(a)(2)(B) (1982), which requires that the agency prepare a notice for publica'ion in the FederalRegister stating that the agency has submitted to OMB a specified collection of information requirement.\n\nHarvardJournalon Legislation [Vol. 24:1\nThese problems arise from the uncertain status of paperwork requirements under the Administrative Procedure Act (APA) itself. The definition of \"rule\" in the APA includes a \"statement\nof general ... applicability and future effect designed to imple-\nment ... or prescribe law . . . . \"347 Whenever an agency imposes a general requiremerit for persons to report certain information or to retain certain records, the agency is making such a statement. Consequently, one might imagine that the procedures applicable to rulemaking would apply to the initial imposition of a general reporting or recordkeeping requirement. Many agencies, however, routinely impose reporting requirements without undertaking the procedures applicable to rulemaking. 348 Some of these cases fall within one or more of the exceptions to the requirements for rulemaking included in the APA, 349 although these exceptions would not explain the failure to publish these requirements in the FederalRegister pursuant to the Freedom of Information Act.350 In many cases, however,\nno exceptions would appear applicable. In the FTC's LB report case, the issue of procedural require-\nments for agency information requests was squarely presented.35' There the FTC ordered 450 of the nation's largest businesses to file reports disclosing information concerning their financial performance in 1974.352 The companies resisted the FTC orders in part on the ground that they were rules which had been promulgated without the procedure required by section 553 of the APA. 353\nWhile the FTC undeniably had the legal authority to impose report orders under section 6(b) of the Federal Trade Commis-\n3475 U.S.C. \u00a7 551(4) (1982).\n348Included among such agencies are the Census Bureau and the Energy Information\nAdministration of DOE. See Shell Oil Co. v. DOE, 477 F. Supp. 413 (D. Del. 1979). 31S9ee 5 U.S.C. \u00a7\u00a7 553(a), (b)(A), (B) (1982). Probably the exception with the greatest\nimpact is the one relating to \"public property, loans, grants, benefits, or contracts.\" Id. \u00a7 553(a)(2).\n350See id. \u00a7\u00a7 552(a)(1)(C), (D). 3-'Appeal of FTC Line of Business Report Litig., 595 F.2d 685, 693-96 (D.C. Cir. 1978) (per curiam), cert. deniedsub nom. Milliken & Co. v. FTC, 439 U.S. 958 (1978). See supra notes 78-103 and accompanying text. 352The report was notable in its requirement that the company present its financial performance statistics according to the Standard Industrial Classification (SIC) codes. FTC Line of Business, 595 F.2d at 691 n.5. See also supra text accompanying notes 7880. At the same time the FTC instituted the Corporate Patterns Report Program, which required 1100 major domestic corporations to report the value of the shipments of their domestic manufacturing establishments in terms of Census Bureau product classifications. FTC Line of Business, 595 F.2d at 692. This reporting program caused similar problems for respondents. 35F3TC Line ofBusiness, 595 F.2d at 693.\n\n1987]\n\nPaperworkReduction Act\n\nsion Act,354 that provision did not indicate what, if any, procedure was required before the report orders could be issued. The companies argued that both the Securities and Exchange Com-\nmission (SEC) and the Federal Power Commission (FPC) had engaged in rulemaking when they undertook similar large-scale reporting programs and that the FTC was required to do the same. 355 The court noted, however, that the statutory provision authorizing the SEC's reporting program expressly required that it do so only by regulations adopted in accordance with the APA. 356 Moreover, the fact that the FPC engaged in rulemaking to impose its reporting program did not indicate that it had to do so.357 There was no question that the FTC could have proceeded by rulemaking; the issue was whether it was required to do so.\nThe court determined that the \"language and history of the APA suggest a classification of agency activity into three basic categories: rulemaking, adjudication and investigation. ' 358 Orders to file informational reports, the court held, fell into the third category. This category is encompassed by section 555(c) of the APA, which merely states that \"[p]rocess, requirement\nof a report, inspection, or other investigative act or demand may not be issued, made, or enforced except as authorized by law. '359 In short, this provision requires no particular prelimi-\nnary procedure for imposing a reporting requirement.\nThe court rejected in a footnote the claim that the reporting programs were rules within the APA definition. 360 The compa-\nnies argued that, because the data would be used for regulatory purposes, the collection of the data would itself be a prescription of law.361 The court stated that use of the information collected for regulatory purposes could not serve as the touchstone of\n\n31415 U.S.C. \u00a7 46(b) (1982). 315FTC Line of Business, 595 F.2d at 694 n.46. 316Id. (citing 15 U.S.C. \u00a7\u00a7 77s(a), 78m(a) (1976)).\n311Id. at 694 n.46.\n35 Id. at 695. See also I K. DAVIS, ADMINISTRATIVE LAw TREATISE \u00a7 3.01 at 159\n(1958). The only legislative history the court cited in support of this tripartite division was a floor statement by Rep. Francis E. Walter (D-Pa.), \"a principal sponsor of the APA.\" FTC Line of Business, 595 F.2d at 695-96 (citing 92 CONG. Rac. 5648 (1948)). This statement clearly distinguishes investigative functions from the legislative or judicial powers of agencies. This statement, however, is of doubtful authority in construing the Act because it was made after the enactment of the APA (a fact the court failed to mention).\n1.9 5 U.S.C. \u00a7 555(c) (1982). 360FTC Line of Business, 595 F.2d at 695 n.48. 361 Id.\n\nHarvardJournalon Legislation [Vol. 24:1\nthis analysis for \"then all types of compulsory process-including subpoenas-would similarly require rulemaking.\" 62 The\ncompanies also asserted that the line-of-business reporting requirement would fall within the Act's definition of rule by con-\nstituting a \"prescription for the future of ... accounting, or practices bearing on ... [accounting]. '363 In responding to this argument, the court correctly noted that the FTC's reporting requirement did not impose on the companies any particular accounting system for the future, although it might require companies to rework their past accounts in order to comply with the FTC's reporting categories. 64\nWhile the D.C. Circuit's analysis in the FTC Line-of-Business case has never been seriously questioned,3 65 it should be. Undoubtedly, much information-gathering by agencies is ancillary\nor incidental to rulemaking or adjudication, even if it might technically fall within the definition of a \"rule.\" The context and\nhistory of section 555 of the APA establish that that section relates to \"ancillary matters. '366 Moreover, the primary activity\nof adjudication and rulemaking would be greatly impeded if agencies were required to conduct the ancillary informationgathering through rulemaking. It is equally clear, however, that\nagencies gather information which is not ancillary or incidental to other proceedings, real or imagined, and which cannot be deemed investigative except by the greatest stretching of the term. For example, census information 367 and information gathered for public dissemination fall into this category. The gath-\n362 Id. 363 5 U.S.C. \u00a7 551(4) (1982). 364FTC Line of Business, 595 F.2d at 695 n.48. 365 Indeed, when a number of companies resisted the Energy Information Administra-\ntion's Financial Reporting System, which bore many similarities to the FTC's Line-ofBusiness Report, they did not even raise the issue that the System had not been promulgated by rulemaking. See, e.g., Shell Oil Co. v. DOE, 477 F. Supp. 413 (D. Del. 1979), aff'd, 631 F.2d 231 (3d Cir. 1980), cert. denied, 450 U.S. 1024 (1981). See also supra text accompanying notes 106-11. More often firms have challenged reporting requirements by characterizing them as formal adjudications, thereby entitling them to other procedural safeguards. See, e.g., Genuine Parts Co. v. FTC, 445 F.2d 1382 (5th Cir. 1971).\n366See, e.g., ATTORNEY GENERAL'S MANUAL ON THE ADMINISTRATIVE PROCEDURE\nACT 61 (1947), reprintedin OFFICE OF THE CHAIRMAN, ADMINISTRATIVE CONFERENCE\nOF THE U.S., FEDERAL ADMINISTRATIVE PROCEDURE SOURCE BOOK, at 110 (1985).\n-7 Generically, census information can include not just information collected by the Census Bureau, but also basic demographic and economic data collected by any agency. The Energy Information Administration's Financial Reporting System should have fallen into this category. This category also describes the purpose of the Corporate Patterns Report, see FTC Line of Business, 595 F.2d at 692, if not the Line-of-Business Report. See id. at 691.\n\n19871\n\nPaperworkReduction Act\n\nering of such information, when accomplished by means of a general legal requirement imposed by an agency, both fits within the definition of a rule and is not ancillary to other proceedings.\nThere is neither a policy nor a legal basis for exempting such an exercise of compulsory agency power from the procedures required for rulemaking. Indeed, the policy reasons underlying section 553 of the APA apply to general requirements for reporting. The public procedure required by section 553 reflects a determination both that the agency will be better informed as\nto its rules if it obtains comments and data from the public and that the public will have a greater sense that it is being treated\nequitably if it has been able to participate in the formulation of the laws that affect it.368\nThe Paperwork Reduction Act itself reflects this understanding to a certain degree in section 3507, which requires agencies\nto give notice in the FederalRegister of any information collection request sent to OMB for approval, and in section 3508, which provides for oral and written comments on OMB determinations regarding the necessity of information. Most directly, however, section 3517 expressly requires that \"in reviewing\ninformation collection requests, the Director shall provide interested agencies and persons early and meaningful opportunity to comment. ' '369 A consistent criticism of the Act is its failure to provide even greater opportunities for public participation.3 70\nJust as the Paperwork Reduction Act recognizes the identity,\nfor paperwork control purposes, of reporting requirements and recordkeeping requirements, the APA should similarly recognize their identity, for procedural purposes, in its definition of a rule. Agency-imposed recordkeeping requirements are almost invariably imposed by rule under the APA. Recordkeeping does not fall within the terms of section 555(c), and at least where rates, wages, and prices are involved, it may be explicitly included within the definition of a rule. Nevertheless, under existing case law an agency could, without rulemaking procedures,\n\n363See, e.g., S. Doc. No. 248, 79th Cong., 2d Sess. 19-20 (1946). Of course, the Administrative Procedure Act does not require that all substantive rules be adopted\nwith public participation. 5 U.S.C. \u00a7\u00a7 553(a), (b)(B) (1982). While some of these exceptions have been criticized, see, e.g., Bonfield, Public Participationin Federal Rule-\nmaking Relating to PublicProperty,Loans, Grants, Benefits, or Contracts, 118 U. PA. L. REV. 540 (1970), to the extent that they remain, they would provide a basis for avoiding rulemaking procedures for reporting requirements that fall within their terms.\n369 44 U.S.C. \u00a7 3517 (1982). See also id. \u00a7\u00a7 3507(a)(2), 3508 (1982). 170 See, e.g., S. REP. No. 576, supra note 328, at 11-12, 45.\n\nHarvardJournalon Legislation [Vol. 24:1\nrequire firms to submit an annual report as comprehensive as the records of the firm. 371\nOMB has included within its definition of information collection \"any requirement for persons to ... publicly disclose information,\" 372 including agency activities as diverse as establishing labeling requirements and regulating proxy statements. 313 The APA and other statutes invariably require that agencies submit such activities to rulemaking proceedings. 374 As OMB argued in justifying its interpretation, there is little distinction between a requirement to report data to a federal agency which then discloses it to the public and a requirement to report the information directly to the public.375 The similarity of these requirements dictate that both the Paperwork Reduction Act and the APA should require that they be submitted to the same procedural process.\nDeterminations about whether a given reporting requirement is investigative, or is ancillary to a rulemaking or adjudication, or is a general prescription which implements law in its own right are not always easy to make. The fact that the line may at times be difficult to discern, however, is not an adequate justification for denying that it exists at all. Moreover, the Paperwork Reduction Act's rather pragmatic, or politic, distinctions between collections of information subject to the Act and those not subject to the Act suggest a good preliminary basis for line drawing. 376 Another basis for distinction could be to look to whether the reporting requirement is a one-time affair or is a recurring obligation, such as a semi-annual report.\nThe treatment of reporting requirements as rules or non-rules carries important consequences beyond the merely procedural. If reporting requirements are not considered to be rules, but are instead mere investigations, the legal interests recognized by law are usually limited to the interests of the agency and the party subject to the reporting requirement. 377 Rarely do courts\n371See FTC Line ofBusiness, 595 F.2d 685.\n372 5 C.F.R. \u00a7 1320.7(c) (1986). 373See id. \u00a7 1320.7(c)(2). 374See APA, 5 U.S.C. \u00a7 553 (1982) and see, e.g., Petroleum Marketing Practices Act, 15 U.S.C. \u00a7 2822 (1982) (disclosure of octane content on gas pumps), Federal Food, Drug, and Cosmetic Act, 21 U.S.C. \u00a7 352 (1982) (drug labeling). 375See Controlling Paperwork Burdens on the Public, 47 Fed. Reg. 39,519-20 (1982); 48 Fed. Reg. 13,675 (1983) (codified at 5 C.F.R. pt. 1320). 37644 U.S.C. \u00a7 3518(c) (1982). 377For example, with respect to a subpoena, usually only the person subject to the demand can contest it. Even the person to whom the information relates does not\n\n1987]\n\nPaperworkReduction Act\n\nperceive a separate public interest in the activity which may be vindicated by third parties. If the reporting requirements are considered to be rules, however, courts are more likely to rec-\nognize a public interest which third parties may protect. This is most apparent in the elimination of an on-going re-\nquirement to report. If the requirement is contained in a rule, few question the legal interest of persons whose actual interests\nmay be affected by the discontinuance of a reporting requirement. For example, when the Department of Labor attempted to reduce the reporting requirements of employers with respect to employees' wages and hours, no one disputed the right of the unions (on behalf of the employees) to challenge the lawfulness of this change in reporting, even though the claim of the unions related to the need for the reporting. 78 Similarly, when the Department of Transportation reduced the requirements imposed on trucking companies for truck drivers' daily logs, requirements previously imposed by regulation, no one questioned the right of the unions to challenge the reduction and to seek reinstitution of the former requirements.3 79 The unions were allowed to argue the value and importance of the logs in en-\nforcing drivers' hours-of-service limitations. In both cases, the aggrieved persons were not simply the parties whose private law rights were infringed.\nThe APA protects not only private interests from government intrusion, but the public interest in certain regulations as well.38\u00b0 In an environment where even government agencies perceive deregulation as the current wisdom, it is important to ensure not only that government justifies under law its intrusions, but also that it justifies under law removals of intrusions which are\n\ngenerally have the right to contest the subpoena if the information is in the possession of others. See, e.g., SEC v. Jerry T. O'Brien, Inc., 467 U.S. 735 (1984) (dictum) (doubting the right of the individual subject of the investigation to challenge subpoena of a third party); United States V. Miller, 425 U.S. 435 (1976) (repudiating due process or Fourth Amendment right to challenge subpoena of bank for defendant's check records). Where an activity is classified as investigatory, third parties have little if any\ncognizable right to agency enforcement of that activity. See Heckler v. Chaney, 470 U.S. 821 (1985) (prosecutorial discretion renders an agency's decision not to investigate presumptively immune from judicial review).\n378See Building & Constr. Trades' Dept. v. Donovan, 712 F.2d 611 (D.C. Cir. 1983), cert. denied, 464 U.S. 1069 (1984).\n319See International Bhd. of Teamsters v. United States, 735 F.2d 1525 (D.C. Cir. 1984).\n380See Stewart, The Reformation ofAmerican AdministrativeLaw, 88 HARV. L. REV. 1667, 1711-56 (1975); Stewart & Sunstein, Public Programs and Private Rights, 95\nHARV. L. REV. 1193 (1982). See also Sunstein, Deregulationand the Hard-Look Doc-\ntrine, 1983 Sup. CT. REV. 177, 179-89, 209-13 (1984).\n\nHarvardJournalon Legislation [Vol. 24:1\nof benefit to others. Because the APA, as construed by the courts, recognizes that beneficiaries of regulatory schemes may have legally cognizable interests in these schemes, such beneficiaries can ensure that any deregulation will be justified under law. Reporting requirements, however, except to the extent that they are reflected in regulations, are not similarly protected.\nThe Paperwork Reduction Act is dedicated to the reduction of reporting requirements. The clearance procedures are oriented toward ensuring that unnecessary reports are eliminated. Agencies are encouraged to focus on the burdens imposed by information collection and to review critically the need for information.3 8' OMB is placed in the role of monitor, with authority to disapprove only unnecessary or overburdensome collections, and without authority to disapprove failure to collect necessary information. 382 The weight of the clearance process is directed against collection. While this focus may be necessary in order to overcome institutional biases which prevent agencies from objectively determining their need for information, 383 the process loses sight of the fact that reporting requirements can serve important public interests. Given this bias in the Paperwork Reduction Act, the APA's failure to afford protection to the public values found in non-regulation reporting requirements is especially disturbing and could seriously jeopardize the public interest which these various reporting requirements serve.\nVI. THE PUBLIC PROTECTION PROVISION\nA. Its Limitations\nThe Federal Reports Act contained many of the substantive requirements found in the Paperwork Reduction Act.384 The Public Protection provision, 385 however, did not exist in the Federal Reports Act. Its inclusion in the Paperwork Reduction Act was the result of the perceived failure of agencies to comply even with the relatively easy requirements of the Federal Reports Act. In some cases the failure derived from legitimate\n381See 44 U.S.C. \u00a7 3507(a)(I) (1982). 38 See id. \u00a7 3508. 381See FINAL REPORT, supra note 12, at 16, 56, 63. -84 See 44 U.S.C. \u00a7\u00a7 3501-12 (1976). 5 44 U.S.C. \u00a7 3512 (1982).\n\n,1987]\n\nPaperwork Reduction Act\n\ndifferences in interpretation between agencies and OMB and\nGAO over the coverage of the Act, but simple non-compliance was also a factor.3 86 There was no specific mechanism in the Federal Reports Act to penalize agencies which failed to comply.387 Nor did the Act provide any means by which the public\nmight determine if an agency had complied with the Act in adopting a reporting requirement.3 88\nIn light of the history of non-compliance under the Federal Reports Act, the drafters of the Paperwork Reduction Act doubted the ability or will of OMB over time to police the agencies' compliance with the new act's requirements.3 89 Therefore, the idea of using the public as a means of enforcement appeared attractive. Such public enforcement would create an incentive for both the agencies and OMB to comply with the Act.\nTo enable this public enforcement, Congress first required that \"[a]n agency shall not engage in a collection of information without obtaining from [OMB] a control number to be displayed upon the information collection request.\"3 90 Next, it provided that\n[n]otwithstanding any other provision of law, no person shall be subject to any penalty for failing to maintain or provide information to any agency if the information collection request involved ...does not display a current control number assigned by [OMB], or fails to state that such request is not subject to this chapter. 39'\nThis requirement for the display of a \"current\" control number was to provide public enforcement of the time limitations ap-\n\n116Senator Percy (R-Ill.) reported that GAO had found that the Department of Agri-\nculture's Food Safety and Quality Service alone had issued 1100 forms that were not approved under the Federal Reports Act. The explanation for this non-compliance was that it was difficult to inform program people of the requirements and that top management did not take paperwork control seriously. S. REP. No. 930, 96th Cong., 2d Sess. 75, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEws 6312, 6313.\n117In the litigation over the Federal Trade Commission's Line-of-Business reports,\nthe courts entertained claims that the reporting requirement was invalid because of a failure to comply with the Federal Reports Act, but the courts consistently found compliance. See supra text accompanying notes 78-111. There is no reported case of a reporting requirement being overturned for failure to comply with the Federal Reports Act.\n31 In implementing the Federal Reports Act, OMB's Circular A-40 had required numbers to be displayed on approved forms. The circular itself, however, was neither published nor publicized.\n31I9nterview with Robert Coakley, supra note 248. 19404 U.S.C. \u00a7 3507(f) (1982).\n191Id. \u00a7 3512.\n\nHarvardJournalon Legislation [Vol. 24:1\nplicable to OMB's approval of information collection requests. 392 By these provisions, Congress thought that persons could determine whether a reporting or recordkeeping requirement needed to be approved, had been approved, and was still valid. 393 If approval was necessary but no current control number was displayed, persons could ignore the requirement with impunity. This threat of legalized non-compliance, it was hoped, would create a strong incentive for the agencies to follow the requirements of the Act.394\nSeveral factors, however, have severely limited the effectiveness of the Public Protection provision as a means of enforcement. Most importantly, respondents face considerable difficulty in determining which reporting or recordkeeping requirements they may safely ignore. First, some reporting requirements are not subject to the Act at all, so that the Public Protection provision does not insulate one from liability for failing to respond to them. 395\nSecond, OMB's interpretation of the term \"information\" not to include certain affidavits, oaths, affirmations, certifications, receipts, changes of address, consents, or acknowledgments396 increases the public's uncertainty as to which information request forms fall within the scope of the Public Protection provision. 397 To many people a governmental demand for a certifi-\n392See id. \u00a7\u00a7 3507(b)-(d). 393S. REP. No. 930, 96th Cong., 2d Sess. 52, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEWS 6292. 39S4ee S. REP. No. 930, 96th Cong., 2d Sess. 48, 52, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEWS 6288, 6292. 39S5ection 3518(c)(1) expressly exempts the collection of information from the application of the Act in five circumstances: in a criminal case; in a civil case to which an agency is a party; in an administrative case where an agency is proceeding against specific persons; in compulsory process under the Antitrust Civil Process Act; and in the conduct of intelligence activities. While the introductory phrase of section 3512, \"[n]otwithstanding any other provision of law,\" suggests universal applicability, the legislative history is explicit that the collections specified in section 3518(c)(1) are not subject to the Public Protection provision at all and thus need not state the inapplicability of the Act to them. See S. REP. No. 930, 96th Cong., 2d Sess. 52-53, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEWS 6292-93. These specified collections are not always immediately evident. For example, one uncertainty is whether the exemption applicable \"during the conduct of ...an administrative action or investigation involving an agency against specific individuals or entities,\" 44 U.S.C. \u00a7 3518(c)(I)(B)(ii) (1982), applies not only where the agency acts as plaintiff or prosecutor but also where the agency acts in a judicial capacity in a dispute between two private parties. 39See 5 C.F.R. \u00a7 1320.7(k)(1) (1986). 397OMB recognized that this exception could be misused, so it provided that the above exception applied only if it entailed \"no burden other than that necessary to\n\n1987]\n\nPaperworkReduction Act\n\n73\n\ncation as to some fact must be difficult to distinguish from normal government reporting requirements. Thus, the authorized absence of a control number again undermines the universality of control numbers, so that individuals cannot rely on the Public Protection provision.\nThird, the Public Protection provision only applies to information collection requests from an \"agency.\" However, two agencies are expressly excluded from the defined term, while others may be excluded by their particular legislation, further diluting the universality of the control number regime. 98\n\nidentify the respondent, the date, the respondent's address, and the nature of the instrument.\" 5 C.F.R. \u00a7 1320.7(k)(1) (1986).\nNotwithstanding this safeguard, it appears clear that \"certifications\" imposing substantial burdens have not been submitted to OMB and have not been assigned control numbers. For example, the IRS Form W-9 is entitled the Payer's Request for Taxpayer Identification Number and Certification. Failure to file the Taxpayer Identification Number subjects a person to a fifty dollar penalty; failure to certify that one is not subject to \"backup withholding\" results in financial institutions withholding twenty percent of the person's dividends, interest, and certain other payments. To fill out the form requires reading the instructions, which comprise the equivalent of one and a half pages of the Federal Register. In 1984 the IRS required that the W-9 form be filled out by every person and entity in the nation with a brokerage or financial account. This tremendous\nburden would have been recognized had it been submitted to OMB. Because it fell within the terms of OMB's certification exception, however, it was not submitted to OMB, was not considered in measuring paperwork burdens, and did not display a control number. To persons receiving it, however, it must have been perceived as\nanother government information requirement. The ability of agencies to avoid OMB clearance by styling their information collection as a certification creates an incentive for agencies to \"cheat,\" creating certifications which do not properly fit within OMB's\nlimited definition. There is some evidence that this occurs. See, e.g., 51 Fed. Reg. 312 (1986) (OSHA eliminates several recordkeeping requirements and replaces them with a \"certification\" requirement which goes beyond the limit of 5 C.F.R. \u00a7 1320.7(k)(1)\n(1986)). \"I The Federal Election Commission and the General Accounting Office are expressly\nexcluded, 44 U.S.C. \u00a7 3502(1) (1982), and recently the Second Circuit declared the\nUnited States Postal Service exempt. See Kuzma v. United States Postal Serv., 798 F.2d 29 (1986). The court's analysis in Kuzma, however, is not convincing. Ignoring the\nlegislative history of the Paperwork Reduction Act which indicates that one purpose of the Act was to eliminate exemptions of the Federal Reports Act, the court held that the Act did not evince an intent to supersede a section of the Postal Reorganization Act, Pub. L. No. 91-375, 84 Stat. 719 (1970), which exempts the USPS from any \"Federal Law dealing with public or Federal contracts, property, works, officers, employees, budgets, or funds, including the provisions of (the APA].\" 39 U.S.C. \u00a7 410(a) (1982).\nThe Paperwork Reduction Act on its face is not a law dealing with the listed subject areas for exclusion, and in Kuzma the challenged form did not relate to any of those subjects. The court also relied on the fact that the Paperwork Reduction Act specifically\nincludes the Postal Rate Commission, see 44 U.S.C. \u00a7 3502(10) (1982), but does not mention the USPS. This deduction is flawed because the Postal Rate Commission is one of the agencies included in the definition of an \"independent regulatory agency.\" Id. The USPS, however, is an agency in the executive branch, and the applicable definition which the USPS meets is by generic type rather than by a list of covered\nagencies.\n\nHarvardJournalon Legislation [Vol. 24:1\nFinally, the main cause of the lack of universal coverage for the Public Protection provision is the interpretation by the Office\nof Legal Counsel that a \"collection of information requirement\" is not an \"information collection request. ' 399 As a result of this interpretation, the Public Protection provision does not apply to reporting or recordkeeping requirements contained in regulations adopted after notice and comment, 400 probably the largest source of mandatory reporting and recordkeeping requirements. Moreover, this interpretation creates peculiar problems in identifying whether an information collection form may be safely ignored. For example, a businessman might find that a particular form sent to him by an agency requiring information concerning his firm's transactions does not display a control number or state that it is not subject to the Paperwork Reduction Act. If he ignores this form, he may find himself in legal difficulty because the form itself had been adopted after notice and comment as a regulation, 40' or because some or all of the substantive information demanded in the form was required by regulation, even though the particular form was not. If the latter was the case and he submitted the required information, but not on the particular form, he would be protected by the Public Protection provision, but if he ignored the form because it did not display a control number, he would be fully liable for failing to submit the information required by the regulation. 402\n399See supra notes 235-65 and accompanying text.\n4 In its implementing regulations, OMB required the assignment and display of control numbers even on collection of information requirements-i.e., reporting or recordkeeping requirements contained in regulations adopted after notice and comment. See 5 C.F.R. \u00a7\u00a7 1320.130), 1320.14(c), (e) (1986). This was done, however, pursuant to OMB's general rulemaking power to manage Federal information collection. Therefore, the public protection provision still does not apply. See 5 C.F.R. \u00a7 1320.5(b) (1986). As OMB stated, \"a member of the public confronting a collection of information requirement with no control number will be able to make further inquires and may find that the requirement is unenforceable.\" 48 Fed. Reg. 13,671 (1983).\n401This is not normal practice by agencies, but there are instances where it is done. See, e.g., Superior Oil Co. v. FERC, 563 F.2d 191 (5th Cir. 1977) (FPC adopted report order through rulemaking).\n401The OLC opinion on the Public Protection provision caused the members of Congress who viewed this provision as central to the effectiveness of the Act to attempt to amend the Act in 1984 to overturn the OLC opinion. See supra text accompanying notes 317-33, 343-44. While there were undoubtedly numerous reasons why that movement to amend the Act failed, at least two factors undercut the continued viability of the Public Protection provision's rationale of including the public in the enforcement of the Act. First, for four years OMB had demonstrated both the ability and will to enforce the Act against the agencies, and there was little, if any, willful non-compliance with the Act by agencies. The claim that public enforcement of the Act was necessary was unavailing in light of this experience.\nSecond, upon closer scrutiny, even the experience under the Federal Reports Act did\n\n19871\n\nPaperworkReduction Act\n\nAs a result of these various statutory, regulatory, and interpretive exclusions from the Public Protection provision, the public is virtually unable to determine which forms it can safely ignore. Given this uncertainty, an individual's safest course is to complete all information forms, whether or not they display a current control number. In most cases, the risk involved in not responding far outweighs the burden of the response\navoided. Despite the impacts on the effectiveness of the Public Protec-\ntion provision caused by the OLC opinion, there are substantial reasons why the provision should not be extended to cover information collections imposed by regulation. The Public Protection provision would make a reporting or recordkeeping requirement in a regulation unenforceable for failure to display a current control number regardless of the reason for the absence of the number, since the provision does not distinguish between omissions of control numbers resulting from the absence of OMB approval and omissions because of errors unrelated to the Paperwork Reduction Act. The potential consequences could be severe. For example, a typographical error omitting a control number from a Treasury Department regulation could have a devastating effect on revenue collection if seized upon by the universe of persons subject to that regulation.\nA requirement to display a \"current\" control number on regulations could create further problems. OMB has interpreted this requirement to mean that forms, questionnaires, and similar instruments must display their expiration dates, except in unusual circumstances approved by OMB. 40 3 If a similar require-\nment were applied to regulations, amendments to the regulations would be necessary whenever a regulation was reapproved. This\n\nnot support the notion that OMB would lack the will or ability to enforce the Act. That is, much of the so-called non-compliance under the Federal Reports Act was the result of ambiguities which were eliminated by the Paperwork Reduction Act. Also, much of the non-compliance involved non-mandatory reporting requirements, which the Public Protection provision would not affect. See infra text accompanying notes 406-07. Furthermore, far from evidencing a lack of will or ability, OMB had historically played an active role in trying to reduce agencies' paperwork, so active in fact that in 1973 Congress took away OMB's authority over independent regulatory agencies. See supra text accompanying notes 69-76.\n403 See 5 C.F.R. \u00a7 1320.7(f)(1) (1986). See also 48 Fed. Reg. 13,666, 13,676 (1983)\n(preamble explaining \u00a7 1320.7(f)). This requirement might cause some waste. For example, agencies must estimate the number of forms required to be printed. In the past the forms could then be used until replaced. Now, however, the form cannot be used\nafter the expiration date printed on the form, even though the same form is re-approved. The form must instead be reprinted with a new expiration date displayed.\n\nHarvardJournalon Legislation [Vol. 24:1\nwould increase the opportunities for clerical error and, because the Code of Federal Regulations is updated only annually, the date reflected in the annual volume would not necessarily be up-to-date.\nIf the Public Protection provision were the only possible way of assuring compliance with the Act, the problems of extending it to regulations might be justified. As it is, however, jud*icial protection of persons subject to the reporting requirements should be available. If an agency adopts a regulation with a reporting or recordkeeping requirement \"without observance of procedure required by law,\" a person subject to that regulation can challenge the validity of the requirement under the APA itself.404 There is nothing in the Paperwork Reduction Act to suggest that its procedures for adoption of collection of information requirements would not be enforceable through the APA. One difference between review under the APA and the Public Protection provision, however, is that under the APA a court can take into account harmless error, so that procedural or clerical errors not prejudicial to the respondent would not render the paperwork requirement unenforceable. 40 5\nAnother factor that limits the effectiveness of the Public Protection provision in trying to enforce the Paperork Reduction Act is that it can only be used defensively. It only protects against penalties. 40 6 While such a mechanism might be helpful for mandatory paperwork requirements which carry penalties for failure to comply, it obviously has no effect on voluntary reporting requests. More importantly, it has little effect on information required to be filed with the government in order to obtain a benefit. 407 A person seeking a benefit from the government, especially where the government can exercise discretion in determining who will receive it, is not likely to claim the right not to use the government's forms or not to submit the information requested. Inasmuch as one-third of the paperwork burden is currently in the area of procurement and application for\n4 See 5 U.S.C. \u00a7 706(2)(D) (1982). 4wId. \u00a7 706. - 44 U.S.C. \u00a7 3512 (1982). 407This is not to say that the Public Protection provision does not apply to such reporting requirements. Were an agency to deny a contract, grant, or welfare to an applicant solely because the applicant had failed to submit the information on the form required, this could be interpreted as a \"penalty.\" Cf. 5 C.F.R. 1320.5(c) (1986).\n\n1987]\n\nPaperworkReduction Act\n\n77\n\ngrants, contracts, and benefits, 4 8 the Public Protection provision\nwould be of little or no use in a large class of cases.\nFurther evidence of the lack of significance of the Public Protection provision is the fact that it has been invoked in only a handful of court cases. 40 9 In some of these cases the Act was invoked by tax protestors as but one of many claims, 410 and in\nnone of the cases did the Public Protection provision supply the protection sought.41' Because the provision has never been suc-\n\n403 U.S. OFFICE OF MANAGEMENT AND BUDGET, INFORMATION COLLECTION BUD-\nGET OF THE UNITED STATES GOVERNMENT: FISCAL YEAR 1986 18 (1986). 4 See Kuzma v. United States Postal Serv., 798 F.2d 29 (2d Cir. 1986); Navel Orange\nAdmin. Comm. v. Exeter Orange Co., 722 F.2d 449 (9th Cir. 1983); United States v. Particle Data, Inc., 634 F. Supp. 272 (N.D. Ill. 1986); Snyder v. IRS, 596 F. Supp. 240 (N.D. Ind. 1984); Cameron v. IRS, 593 F. Supp. 1540 (N.D. Ind. 1984), aff'd, 773 F.2d\n126 (7th Cir. 1985). 410 Indeed, in two of these cases all of the protestors' claims were frivolous and the\ncourt imposed sanctions on the tax protester-plaintiffs in the form of attorney fees for the government. See Snyder v. IRS, 596 F. Supp. 240; Cameron v. IRS, 593 F. Supp.\n1540. 411In Cameron, the plaintiff claimed that he did not have to file a tax return because\nthe IRS forms did not display an OMB control number. Cameron, 593 F. Supp. at 1556.\nThe court rejected this argument, claiming that the Paperwork Reduction Act does not apply to \"[t]he process of assessment and collection of taxes,\" id., citing section\n3518(c)(1)(B)(ii), which exempts collections of information during the conduct of an\nadministrative action or investigation involving an agency against specific individuals. Although the court's result is correct, its reasoning is flawed. The cited exemption would apply in the course of an IRS investigation and enforcement, but it clearly would\nnot apply to income tax returns required to be filed generally. A more satisfactory line of reasoning is that the requirement to file an annual income tax return stems from statute and regulation, not from the form the IRS adopts. See 26 U.S.C. \u00a7\u00a7 6011, 6012 (1982); 26 C.F.R. \u00a7\u00a7 1.6011, 1.6012 (1986).\nIn Snyder, the plaintiff alleged that he could not be fined for filing false W-4 tax forms\nbecause the forms did not display an OMB control number. Snyder, 596 F. Supp. at 250. In dismissing this argument, the court merely cited the Cameron case for the proposition that \"IRS documents do not need to carry OMB numbers to be valid.\" Id. Again, the court's result is correct, but its reasoning wrong. The Public Protection\nprovision at most only protects a person from penalties forfailing to file information. It does not protect one who files information which is false. Cf. 18 U.S.C. \u00a7 1001 (1982).\nTwo cases involved the IRS's use of IRS Form 2039, which does not display an OMB control number, as a summons. In ParticleData, the respondent defended his lack of\nresponse to the summons on the basis of the Public Protection provision. The court correctly noted, and chided respondent's attorney for not noting, that section 3518(c)(l)(B)(ii) exempts from the Act just such particularized investigations by an agency. ParticleData, 634 F. Supp. at 275-76. Prasch v. United States, 84-2 U.S. Tax Cas. (CCH) 9676 (E.D. Cal. 1984), also should have used this analysis. There the IRS had issued a summons to a savings and loan to submit plaintiff's financial records. Pursuant to 26 U.S.C. \u00a7 7609(b)(2, (1982) the plaintiff moved to quash an IRS summons\non the ground, among others, that the summons did not have an OMB control number. The court stated that, even assuming that a control number was required for the summons, its absence was irrelevant because the IRS had followed all its own internal procedures. The simpler and more accurate reasoning would have been that the sum-\nmons was exempt from the Act pursuant to section 3518(c)(1)(B)(ii). Moreover, the Public Protection provision probably does not, given its wording, provide a basis for a person to attack a collection of information request addressed to another. Therefore,\neven were the summons in the case not exempted by section 3518(c)(1)(B)(ii), the court\n\nHarvardJournalon Legislation [Vol. 24:1\ncessfully invoked to render an information collection request unenforceable, there is little reason to believe that the provision has had much of an impact on agency compliance. 412\nB. Other Bases for JudicialReview\nThe Public Protection provision's lack of effectiveness may be particularly troublesome because at least some of the judicial review available under the Federal Reports Act has been precluded by the Paperwork Reduction Act. Under the Federal Reports Act, with no Public Protection provision, persons were able to challenge the approval of a collection of information on\nwould have been correct in not using the Public Protection provision to deny enforcement, although not for the reason it gave.\nIn Navel Orange, the court granted the government's request to enjoin the defendant to file reports required pursuant to the Agricultural Marketing Agreement Act. The court held that, even if the defendant's claim that the forms failed to display control numbers were true, the defendant could not raise such a claim as an affirmative defense to an enforcement action, but only in a subsequent administrative review. Navel Orange, 722 F.2d at 453-54. Moreover, the court stated, the government sought injunctive relief only for the period when the report forms had control numbers. Id. at 454. This latter rationale, of course, would be sufficient, but the former is suspect. Unlike traditional challenges to marketing orders which can be vindicated in subsequent administrative reviews by the return of monies paid, once the respondent has suffered the burden of responding to an invalid information collection request, a subsequent administrative proceeding can do little to make him whole. A better line of reasoning for the court would have been that the Public Protection provision only protected the defendant from any penalty for not reporting. In this action, the defendant was not being penalized. Instead, the government was engaging in a civil action to substitute a judicially ordered collection of information for the collection imposed by the allegedly defective form. Just as a reporting requirement imposed directly by regulation or statute would not be subject to the Public Protection provision, so also a judicially imposed reporting requirement would not be subject to it.\nIn Kuzma, a private mail receiving and forwarding agency challenged the Postal Service's Form 1538 because it displayed no control number. The court held that the Postal Reorganization Act, Pub. L. No. 91-375, 84 Stat. 719 (1970), exempted the USPS from the requirements of acts such as the Paperwork Reduction Act. See Kuzmna, 798 F.2d at 31-32. But see supra note 398.\n412 There is evidence that public awareness of the control number regime has aided OMB in discovering some unapproved collections. Of the thirty violations that OMB has reported to Congress, more than half were identified and reported to OMB by persons in the private or public sector who noted the absence of a displayed control number. See U.S. OFFICE OF MANAGEMENT AND BUDGET, MANAGING FEDERAL INFORMATION RESOURCES (OMB's annual reports under the Paperwork Reduction Act contain a list of violators, how they were discovered, and what action was taken, in the second appendix of each report). This impact is less than what the provision's authors had hoped for. Senator Danforth, for instance, stated that the Public Protection provision \"should serve as [an] important deterrent ... and it is an important protection.\" 126 CONG. REC. 30,192 (1980). Nevertheless, it appears to be the most that can be expected.\n\n1987]\n\nPaperworkReduction Act\n\nthe merits. 4 3 Under the Paperwork Reduction Act, however, at\nleast where the approval is of a collection of information requirement contained in a rule adopted after notice and comment, this procedure is no longer possible. Section 3504(h)(9) expressly provides that \"[t]here shall be no judicial review of any kind of the Director's decision to approve or not to act upon\" a requirement contained in a regulation adopted after notice and comment. 414\nThe justification for this exemption is not entirely clear. On\nthe one hand, section 3504(h) might recognize that the volume of paperwork requirements and requests sent to OMB for review\nis too large to allow OMB to scrutinize each sufficiently to assemble an adequate record. Given modern notions of judicial review of agency action, were the Director's decision subject to review, the decision would have to be supported by some record. 41 5 This record would have to include evidence supporting a determination of the necessity for the information, including its practical utility.41 6 The Act, by recognizing inaction as the equivalent of approval, 4 7 indicates that lack of scrutiny by OMB should not bar a collection directly. Therefore, lack of scrutiny should not bar a collection indirectly by making it susceptible to judicial invalidation for failure to assemble an adequate record.\nOn the other hand, section 3504(h)(9) may simply reflect Congress' desire to make OMB's approval determination para-\nmount. After all, Congress was aware of the litigation under the Federal Reports Act, and it may have wished to preclude the courts from substantively reviewing OMB's approvals.4 8 One\n\n411 See Superior Oil Co. v. FERC, 563 F.2d 191 (5th Cir. 1977); Shell Oil Co. v. DOE, 477 F. Supp. 413, 428-31 (D. Del. 1979); In re FTC Corporate Patterns Report Litig., 432 F. Supp. 291, 307-08 (D.D.C. 1977). See also Union Oil Co. v. FPC, 542 F.2d 1036, 1039-44 (9th Cir. 1976) (reporting requirement in rule reviewed under APA \u00a7 706(b)(2)).\n41444 U.S.C. \u00a7 3504(h)(9) (1982). This provision was added as part of the Kennedy Amendment on the Senate floor. In a post-enactment statement explaining his amendment, Senator Kennedy made clear his intent that this provision had no effect on an OMB disapproval of a collection of information requirement. See 126 CONG. REC. 34,237 (1980). This explanation was made to counteract a statement made on the floor of the House by Rep. Horton that none of OMB's decisions under section 3504(h), even decisions to disapprove collection of information requirements, would be reviewable in court. See 126 CONG. REC. 31,228 (1980).\n415See Pedersen, FormalRecords andInformalRudemaking, 85 YALE L.J. 38 (1975). 416 See 44 U.S.C. \u00a7 3504(c)(2) (1982).\n417 See id. \u00a7\u00a7 3504(h)(3), (4). 4Is As Sen. D'nforth said, \"[w]e are not seeking to reduce paperwork by creating\njudicial remedies for people who want to challenge paperwork requests they receive from the Federal Government.\" 126 CONG. REC. 30,192 (1982).\n\nHarvardJournalon Legislation [Vol. 24:1\nweakness of this justification is its implication that OMB determinations resulting in a disapproval should likewise be immune from judicial review. Given the language of section 3504(h)(9), however, it seems impossible to conclude that disapprovals are so immune, despite Representative Horton's interpretation of the section as barring judicial review of OMB disapprovals. 419\nThis provision does not mean, however, that the rule, or the collection of information requirement, is itself immune from review. To the contrary, the substantive requirement is fully subject to review, but the review would be of the action or decision of the agency which adopted the rule, not of OMB's approval of the rule. Thus, parties may challenge the rule as if the Paperwork Reduction Act did not exist; the Director's approval is irrelevant to the judicial review. 420\nThere is no comparable provision to section 3504(h)(9) with respect to information collection requests. There is, however, explicit legislative history indicating an intent to preclude judicial review on the merits of an OMB approval. 421 Nevertheless, it is not clear how this intent is manifested in the absence of any preclusion provision similar to section 3504(h)(9).\nIn this context, general principles of administrative law would apply, and the issues litigated under the Federal Reports Act would reappear. One of those issues was whether the action of the Director of OMB is committed to agency discretion by law, so that under section 701(a)(2) of the APA the action would not be reviewable. Under the Federal Reports Act, that question was generally answered in the negative. Pursuant to the standard enunciated in Citizens to Preserve Overton Park v. Volpe,422 courts found that there was law to apply in the requirement that\n419See supra note 414. 42S0ee, e.g., International Bhd. of Teamsters v. United States, 735 F.2d 1525 (D.C. Cir. 1984) (court rejected Federal Highway Administration's amendment of reporting and recordkeeping requirements without regard to OMB's approval). Indeed, in this case the court acknowledged that \"the agency was in large part following OMB instructions to reduce the burdens,\" id. at 1529, yet the court rejected the agency's determination that some of the information deleted from the old regulation was \"unnecessary for law enforcement purposes.\" Id. at 1532. 421See S. REP. No. 930, 96th Cong., 2d Sess. 52, reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6292 (\"[i]f an information collection request displays a current control number or states that the request is not subject to this Act, it is valid for the purposes of this Act\"); 126 CONG. REC. 30,192 (1980) (remarks of Sen. Danforth) (\"[i]awsuits which seek to challenge the necessity or burden of information collection requests cannot therefore be grounded on the provisions of this act\"). 422401 U.S. 402 (1971).\n\n1987]\n\nPaperworkReduction Act\n\nthe information be necessary to the functions of the agency.423 Certainly, that is still the case, and section 3508 would seem to\npresent a perfectly adequate law to apply. Unlike the Federal Reports Act, however, the Paperwork\nReduction Act suggests that OMB's inaction, its failure affirmatively to approve or disapprove an information collection request, constitutes a form of approval. There is no standard in the Act governing when OMB must affirmatively act to approve. 424 In this circumstance, it seems that there is no law to apply and thus no judicial review when OMB approves by inaction. One can argue from this that there should be no judicial review even when OMB affirmatively acts to approve an infor-\nmation collection request. To subject approvals to judicial review only when done affirmatively would encourage OMB inaction rather than action.425\nAn alternative argument for insulating OMB approvals of in-\nformation collection requests from judicial review is that such an approval is not a final agency action and not ripe for judicial review, because the actual decision to impose an approved information collection request lies with the agency, not OMB .426 This argument is supported by the policies underlying section\n3504(h)(9)'s preclusion of judicial review of OMB's approval of collection of information requirements. There seems little reason to preclude judicial review of OMB's approvals of regulations imposing reporting or recordkeeping requirements, and yet allow review of approvals of information collection requests.\n\n43 See cases cited supra note 413. Where GAO, rather than OMB, was responsible for the paperwork review function, it was not to review for the necessity of the information. Section 3508's predecessor, 44 U.S.C. \u00a7 3506 (Supp. V 1975), only applied to OMB review. GAO's review was limited to avoiding duplication and minimizing compliance burdens, although a reference to determining \"the appropriateness of the forms\" suggested a further scrutiny. See 44 U.S.C. \u00a7\u00a7 3512(b), (d) (Supp. V 1975). See also Appeal of FTC Line of Business Report Litig., 595 F.2d at 708-10. Two courts in dictum stated that GAO's review was committed to its discretion by law because its review was characterized in the statute as \"advice.\" See General Elec. Co. v. FTC, 411 F. Supp. 1004, 1006 (N.D.N.Y. 1976); Westinghouse Elec. Corp. v. FTC, 1976-1 Trade Cas. (CCH) 60,871 (S.D. Ohio April 15, 1976).\n424 See 44 U.S.C. \u00a7 3507(b) (1982). There is, however, one distinction between an\naffirmative approval and an approval by inaction-the maximum time limit of the\napproval. When OMB affirmatively approves an information collection request, the approval may extend for up to three years. Id. \u00a7 3507(d). If the approval derives from inaction, however, the approval cannot last for more than one year. Id. \u00a7 3507(b).\n45 Were a court in fact to review and reverse an OMB affirmative approval, OMB could respond by merely approving by inaction; this response would then be immune\nfrom review. 416 Cf. Bethlehem Steel Corp. v. EPA, 536 F.2d 156 (7th Cir. 1976) (EPA regulations\nnot \"final\" action, because impact on plaintiffs will not arise until state has taken further\nindependent action).\n\nHarvardJournalon Legislation [Vol. 24:1\nAs with regulations, the information collection request could still be challenged on the merits, but OMB's approval would not be an issue. Also as with regulations, OMB's decisions to disapprove a collection of information request could be separately challenged. 427 There is a clear statutory standard applicable to all disapprovals, and at least with respect to non-independent regulatory agencies, OMB's action is final.\nVII. CENTRALIZED OVERSIGHT\nA. Background\nFor those who studied the paperwork problem in the late 1970's, one recommendation stood out-centralized oversight of all agencies' paperwork, 428 meaning both the elimination of exemptions from oversight and the unification of oversight in one office. 429 The Paperwork Reduction Act accordingly eliminated the exemption Treasury had enjoyed430 and unified in OMB the oversight functions which had previously been split between OMB and GAO. 431 Moreover, OMB's oversight under the Act is not merely procedural. It includes the power to determine whether information sought by an agency is necessary for the proper performance of that agency's functions and whether the information will have practical utility for the agency. 432 While this power is not completely new to OMB, 433 its restatement in the context of OMB's expanded jurisdiction contained the potential for significant change.\nOn its face, OMB's power to determine an agency's information needs is breathtaking. Rarely is one agency given the power to overrule another agency's determination of the proper scope of its functions. The legislative history confirms that this unique power was intended: the Senate Report emphasizes the test of information necessity and the responsibility of the Direc-\n427 But see supra text accompanying notes 377-83. 428See, e.g., FINAL REPORT, supra note 12, at 19-20. 429 Id. 430 See supra notes 213-65 and accompanying text. 43\"For a short period, the Secretary of Education had separate responsibility for overseeing information collection activities related to federal education programs. Control of Paperwork Amendments of 1978, Pub. L. No. 95-561, \u00a7 1212(b), 92 Stat. 2143, 2338-39 (1978) (codified as amended at 20 U.S.C. \u00a7 1221e (1982)). 43244 U.S.C. \u00a7 3508 (1982). 43 The Federal Reports Act contained similar language. See 44 U.S.C. \u00a7 3506 (1976).\n\n1987]\n\nPaperworkReduction Act\n\ntor of OMB in determining that necessity.4 4 Only if the collection of information is specifically required by statute is the Director's determination not final, 4 5 except for those independent regulatory agencies which are authorized to overrule the Director.436 On the floor of the Senate, Senator Danforth stressed both the necessity requirement, which means that the information is \"truly needed to achieve the agency's objectives, ' 437 and the role of OMB as the ultimate decision maker.438 Nothing in the House Report suggests the contrary.439\nThe change from the language used in the Federal Reports\nAct reinforces the inference that OMB is to apply the provision strictly. Under the Federal Reports Act, OMB determined whether information was necessary for the proper performance of an agency's functions orfor any otherproperpurpose.440 The latter basis was not included in the Paperwork Reduction Act, indicating a stricter standard. The specification in the Paperwork Reduction Act that the information have practical utility likewise indicates a strict standard.44'\nIn light of the breadth and depth of power given to OMB,\nsurprisingly little concern over its possible misuse exists in the legislative history. What little concern was expressed involved independent regulatory agencies. 442 In response, the Act pro-\ntects independent regulatory agencies by providing them with the power to overrule OMB's determinations. 443 With respect\nto executive agencies, however, the legislative history reveals virtually no concern. 444 Both the House and Senate reports, in response to concerns expressed regarding independent agencies,\nindicated that regulatory agencies in the executive branch had\n\n434S. REP. No. 930, 96th Cong., 2d Sess. 49, reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6289.\n435Id. 436 44 U.S.C. \u00a7 3507(c) (1982) (stating the conditions for authorization to overrule the\nDirector of OMB). 437126 CONG. REC. 30,190 (1980). 438 Id.\n419H.R. REP. No. 835, supra note 150.\n-0 44 U.S.C. \u00a7 3506 (1976). 44 The Act defines practical utility as \"the ability of an agency to use information it collects, particularly the capability to process such information in a timely and useful\nfashion.\" 44 U.S.C. \u00a7 3502(15) (1982). 44 S. REP. No. 930, 96th Cong., 2d Sess. 14-16, reprintedin 1980 U.S. CODE CONG.\n& ADMIN. NEws 6241, 6254-56; H.R. REP. No. 835, 96th Cong., 2d Sess. 21-23 (1980). 443 S. REP. No. 930, supra note 442, at 14-16.\n4 See id. at 15.\n\nHarvardJournalon Legislation (Vol. 24:1\nfunctioned without undue interference under the Federal Reports Act.445\nTwo further provisions in the Paperwork Reduction Act were viewed as protecting against interference from OMB. First, section 3504(a) requires that OMB's authority \"be exercised consistent with applicable law.' '446 Of course, this limitation provides little protection because the Act gives OMB authority to determine the applicable law regarding the necessity of information.447 Second, section 3518(e) states that nothing in the Act \"shall be interpreted as increasing or decreasing the authority\nof the President, the Office of Management and Budget or the Director thereof, under the laws of the United States, with respect to the substantive policies and programs of departments, agencies and offices .... \",448 This provision was adopted to guard against undue interference by drawing a distinction between paperwork requirements and substantive decisions. 449 Several agencies suggested the impossiblity of such a distinction, 450 and Senator Jacob Javits (R-N.Y.) expressed his doubts in the Senate, 45' but these warnings were brushed aside.\nCongress was more concerned that regulatory reform activities by OMB might dilute the attention given paperwork reduction and information management. 52 At the time of debate, paperwork activities were the responsibility of OMB's Office of Regulatory and Information Policy.453 That office also had responsibility for overseeing agency activity under President Carter's Executive Order 12,044 (E.O. 12,044), entitled \"Improving Government Regulations. '454 The version of the paperwork bill passed by the House created an Office of Federal Information Policy, which the House Report made fairly clear would consist of the information management and paperwork elements of the existing OMB office, but would not be assigned regulatory reform activities. 455\n45 See id. 446 44 U.S.C. \u00a7 3504(a) (1982).\n\"I See, e.g., id. at \u00a7 3504(d)(3). 448Id. at \u00a7 3518(e).\n449S. REP. No. 930, 96th Cong., 2d Sess. 56, reprintedin 1980 U.S. CODE CONG. &\nADMIN. NEWS 6241, 6296. 410H.R. REP. No. 835, 96th Cong., 2d Sess. 22 (1980). 411126 CONG. REc. 30,192 (1980). 452 Id.\n- 44 U.S.C. \u00a7 3503 (1976). 454E.O. 12,044, supra note 17. 4--H.R. REP. No. 835, 96th Cong., 2d Sess. 9 (1980).\n\n1987]\n\nPaperwork Reduction Act\n\n85\n\nIn the Senate, similar concerns were voiced, 456 but the Administration pressed strongly the view that the existing office's activities under E.O. 12,044 complemented its paperwork activities.45 7 As a result, the Senate committee's bill provided for an Office of Information and Regulatory Affairs, which would essentially continue the existing office's functions. 458 Nevertheless, the committee warned that it did \"not intend that 'regulatory reform' issues which go beyond the scope of information\nmanagement and burden be assigned to the Office by the Director.\"459\nIn order to limit OMB in its assignment of regulatory reform issues to the Office, the bill specified that the sums appropriated\nwere \"to carry out the provisions of the bill,\" and for no other purpose. 460 Senator Charles Percy (R-Ill.) was not convinced. He noted that general regulatory reform legislation then being considered envisioned broad oversight powers in OMB, powers\nwhich would likely be assigned to the Office of Information and Regulatory Affairs and which would likely bury paperwork concerns.46' His, however, was the sole dissenting voice, and the Senate committee's version became law.\n\nB. Executive Order 12,291\nImplementation of the Act lay in the hands of the new Reagan Administration, for which regulatory reform had an even higher priority than that assigned it by the prior administration. Executive Order 12,291 (E.O. 12,291),462 issued on February 17, 1981, expressed that priority. 463 Building upon President Carter's\n456See 126 CONG. REC. 30,192 (1980). 417The Comptroller General's approval of the relationship aided its acceptance. See S. REP. No. 930, 96th Cong., 2d Sess. 8, reprintedin 1980 U.S. CODE CONG. & ADMIN.\nNE4N58vsId6.241, 6248.\n419S. REP. No. 930, 96th Cong., 2d Sess. 9, reprintedin 1980 U.S. CODE CONG. & ADMIN. NEws 6241, 6249.\n460 S. REP. No. 930, 96th Cong., 2d Sess. 11, reprinted in 1980 U.S. CODE CONG. & ADMIN. NEws 6241, 6251; see also 44 U.S.C. \u00a7 3520 (1982). The 1986 reauthorization contained a similar limitation. See supra note 345.\n461S. REP. No. 930, 96th Cong., 2d Sess. 74-75, reprintedin 1980 U.S. CODE CONG. & ADMIN. NEWS 6241, 6312-13.\n462E.O. 12,291, supra note 17. 46 Actually, the first manifestations of general regulatory reform were in the President's creation of the Presidential Task Force on Regulatory Relief. See Remarks Announcing the Establishment of the Presidential Task Force on Regulatory Relief, PuB. PAPERS 30 (Jan. 22, 1981); Memorandum Postponing Pending Federal Regulations, PUB. PAPERS 63 (Jan. 29, 1981). The Task Force, chaired by the Vice President,\n\n86\n\nHarvardJournalon Legislation [Vol. 24:1\n\nregulatory reform order, the new order went significantly further both procedurally and substantively. Unlike E.O. 12,044, 464\nhowever, the new Order did not mention paperwork at all. Instead, OMB was instructed merely to coordinate 465 the provisions of the new Order with the requirements of the Regulatory Flexibility Act 466 and the Paperwork Reduction Act.\nSubstantively, the Order created a new general requirement that executive agencies not undertake regulatory action unless\nthe benefits to the public outweigh the costs, and that executive agencies select regulatory alternatives which maximize benefits.4 67\nProcedurally, the Order continued the previous Order's re-\nquirement that agencies prepare regulatory impact analyses of major rules, 468 and added the requirement that agencies send all\nproposed and final rules to OMB for review before publication.4 69 OMB would review first the proposed rule and later the final rule for compliance with the Executive Order.4 70 The Order\ndoes not purport to give OMB authority to override any determinations made by an agency,471 but does require agencies to\ndelay publication of proposed rules pending the completion of\nOMB's review and to delay publication of final rules until the agency has responded to OMB's views. 472 Moreover, OMB is\n\ngenerally oversaw executive branch regulatory relief activities. See infra text accompanying note 475. The memorandum provided that no current rulemakings could be completed for a sixty-day period and that rules not yet in effect because of a delayed\neffective date should have their effective dates further delayed. Memorandum Postponing Pending Federal Regulations, PuB. PAPERS 63 (Jan. 29, 1981); see also National Resources Defense Council v. EPA, 683 F.2d 752 (3d Cir. 1982).\n464E.O. 12,044, supra note 17.\n465E.O. 12,291, supranote 17, at \u00a7 6(b). 4 5 U.S.C. \u00a7\u00a7 601-05 (1982). In general, the Regulatory Flexibility Act requires agencies to prepare economic analyses of the effects of agency regulations which are likely to have a \"significant economic impact on a substantial number of small entities,\" 5 U.S.C. \u00a7 605 (1982). OMB has no role under the Regulatory Flexibility Act. See generally Verkuil, A Critical Guide to the Regulatory Flexibility Act, 2 DugE L.J. 213\n(1982).\n467 See E.O. 12,291, supra note 17, at \u00a7 2. The Order recognized that cost-benefit analyses might not always be permissible under the governing statute, including a caveat\nthat the Order's requirements applied only \"to the extent permitted by law.\" Id. 46 Id. at \u00a7 3; compare id. at \u00a7 l(b) (definition of major rule) with E.O. 12,044, supra\nnote 17, at \u00a7 3(a) (criteria for rules requiring regulatory analyses). 469E.O. 12,291, supra note 17, at \u00a7 3(c).\n470Id. at \u00a7 3(c)(3).\n471The Order explicitly provides that nothing in the subsection relating to OMB review shall be construed as displacing the agencies' responsibilities delegated by law.\nE.O. 12,291, supra note 17, at \u00a7 3(f)(3). 412 Id. at \u00a7 3(f)(1)-(2).\n\n19871\n\nPaperworkReduction Act\n\nauthorized to require the agency to consider additional relevant data from any appropriate source. 473\nThese standards and procedures applied to both pending and future rulemakings.4 Existing regulations did not escape review either. The Presidential Task Force on Regulatory Relief ultimately focused its attention on 119 existing regulations.475 The Office of Information and Regulatory Affairs (OIRA), created by the Paperwork Reduction Act,476 was the office in OMB responsible for implementing E.O. 12,291.477 Thus, Senator Percy's fear that paperwork issues would be buried under regulatory reform activities was apparently realized.\nNevertheless, the focus on regulatory relief was hardly at odds with paperwork reduction. Indeed, a number of the regulations identified for review by the Task Force were regulations\nthat directly created a paperwork burden. For example, the driver's log requirement imposed by the Department of Transportation 478 was one of the regulations on the Task Force's hit list. This regulation required truck drivers to make detailed records concerning their status for each hour of the day as well as to record date, total mileage, vehicle identification, name of co-driver, and home terminal. 479 The Carter Administration had identified this regulation as imposing the seventh largest burden of any government paperwork requirement and the second largest among non-tax paperwork requirements. 480 With the Task Force's backing and the Administration's commitment to dere-\ngulation, OIRA was able to convince the Department of Trans-\nportation to reduce significantly the regulation's requirements and paperwork burden. 48t\nThe Task Force's focus on deregulation often had paperwork\nreduction results, even where the regulation itself did not impose\n\n473 Id. at \u00a7 6(a)(3).\n114 Id. at \u00a7 7.\n475 See PRESIDENTIAL TASK FORCE ON REGULATORY RELIEF, REAGAN ADMINISTRA-\nTION REGULATORY ACHIEVEMENTS (August 11, 1983) [hereinafter REAGAN ADMINIS-\nTRATION REGULATORY ACHIEVEMENTS].\n47644 U.S.C. \u00a7 3503(a) (1982). 477See supra notes 458-59 and accompanying text. 47849 C.F.R. \u00a7 395 (1980). 479 Id. 4101981 ICB, supra note 122, at 10. 481Changes made by DOT reduced the burden hours from 31 million to 9.4 million. 1982 ICB, supra note 5, at 33; U.S. OFFICE OF MANAGEMENT AND BUDGET, INFOR-\nMATION COLLECTION BUDGET OF THE UNITED STATES GOVERNMENT: FISCAL YEAR\n1983 42 (1983) [hereinafter 1983 ICB]. Some of these changes, however, were invalidated\non judicial review. Sec, e.g., International Bhd. of Teamsters v. United States, 735 F.2d 1525 (D.C. Cir. 1984).\n\nHarvardJournalon Legislation [Vol. 24:1\na paperwork requirement. For example, when the Department of Health and Human Services combined approximately thirty grant programs into seven block grants, states were able to save fifty-two million dollars in paperwork costs alone. 482 By combining twenty-eight programs into one block grant, the Department of Education reduced paperwork requirements by over eleven million burden hours. 483\nDeregulation itself invariably resulted in reduced paperwork requirements. For example, the Department of Labor made major changes to the Davis-Bacon Act regulations,4 84 one of which was to reduce the recordkeeping requirements. 48 5 In addition, when President Reagan ended the price and allocation controls on crude oil and petroleum products, 486 a large body of reporting requirements was eliminated. 487 Thus, while the focus of OIRA was regulatory relief rather than paperwork issues, the result of that focus still led to significant reductions in paperwork burdens .488\nWhile OIRA focuses its efforts on regulatory relief, its organization and practice tend to blur distinctions between activities under the Paperwork Reduction Act and E.O. 12,291. The office has two deputy administrators, 48 9 one of whom supervises the three branches of desk officers: the Regulatory Policy Branch, the Reports Management Branch, and the Information Policy Branch. 490 Agencies are assigned to desk officers in these branches, so that both regulations under E.O. 12,291 and requests for paperwork clearances under the Paperwork Reduction Act go to that agency's desk officer or officers. 491\n482 REAGAN ADMINISTRATION REGULATORY ACHIEVEMENTS, supra note 475, at 82. 483 Id.\n44 See 29 C.F.R. \u00a7\u00a7 1.1-6.57 (1986).\n485 Id. at \u00a7\u00a7 3.3-3.4. The change in the paperwork requirements was invalidated,\nhowever, in Building & Constr. Trades' Dep't v. Donovan, 712 F.2d 611 (D.C. Cir. 1983), cert. denied, 464 U.S. 1069 (1984).\n49 Exec. Order No. 12,287, 3 C.F.R. \u00a7 124 (1981), reprinted in 15 U.S.C. \u00a7 757 (1982). 4 1982 ICB, supra note 5, at 25. 41 The GAO and certain congressional committees criticized the failure of OIRA to devote sufficient attention to paperwork issues in their own right, and especially to the information management and paperwork responsibilities not directly related to burden reduction, such as the creation of a Federal Information Locator System. See generally IMPLEMENTING THE PAPERWORK REDUCTION ACT, supra note 173. Dissatisfaction with OIRA's priorities was one of the catalysts for the unsuccessful attempts to amend the Paperwork Reduction Act in 1983 and 1984. See S. REP. No. 576, 98th Cong., 2d Sess. 6 (1984); H.R. REP. No. 147, 98th Cong., Ist Sess. 2 (1983). \"9 See 1983 Senate Oversight Hearings, supra note 2, at 72. 490Id. 491 Id.\n\n1987]\n\nPaperworkReduction Act\n\nThe Regulatory Policy agencies which generate\n\nBranch the most\n\nhrauslersesrpeognuslaibtiinligtycofonrdutchto.s4e92\n\nThe Reports Management Branch is responsible for those agen-\n\ncies with major paperwork burdens that are not generally regulatory agencies. 493 The Information Policy Branch is responsi-\n\nble for those agencies with the most automatic data processing equipment. 494 Despite the purported functional organization of\n\nthe branches, the breakdown into three branches seems less the\n\nresult of a need to differentiate among subject matters than a\n\npractice compelled by the number of desk officers and the need\n\nto provide adequate supervision. The orientation of the desk\n\nofficer to the agency for which he or she is responsible results\n\nin a program focus, rather than a focus on paperwork issues per\n\nse.\n\nFurthering the lack of distinction between paperwork review\n\nunder the Act and regulatory review under E.O. 12,291, OIRA\n\nchose to use the same form for regulatory review under\n\nE.O. 12,291 that had been used under the Federal Reports Act. 49 This form was then also adopted for use under the Pa-\n\n492 These include the Departments of Agriculture, Energy, Interior, and Transporta-\ntion, the Environmental Protection Agency, and several independent commissions: the Nuclear Regulatory Commission, the Federal Trade Commission, the Interstate Commerce Commission, the Consumer Product Safety Commission, and the Federal Energy Regulatory Commission. These independent commissions are only subject to OIRA for purposes of the Paperwork Reduction Act, not E.O. 12,291. E.O. 12,291, supra note 17, at \u00a7 1(a).\n493These include the Departments of Treasury, Education, Health and Human Services, Labor, and Housing and Urban Development, as well as the Veterans Administration, the Railroad Retirement Board, and the Bank Supervisory agencies.\nThe Reports Management Branch is described by OMB as the Branch concerned with the agencies imposing the most paperwork burdens. Interview with Nell Minow, Special Assistant to the Administrator of OIRA, in Washington, D.C. (June 17, 1985); see also 1983 Senate Oversight Hearings, supra note 2, at 72. Such a description, however, does not accurately reflect the paperwork burdens as they were perceived either in 1981 or at present. For example, the fiscal year 1980 paperwork burdens of the Departments of Agriculture, Transportation and Commerce, the Securities and Exchange Commission, and the Federal Communications Commission all were greater than those of the Departments of Housing and Urban Development, Labor, and Education. 1983 ICB, supra note 481, at 14.\n494These include the Departments of Defense and Commerce, the General Services Administration, the Federal Communications Commission, the Securities and Exchange Commission, the Small Business Administration, the Federal Emergency Management Agency, the National Credit Union Administration, the National Aeronautics and Space\nAdministration, the State Department, the Agency for International Development, the International Trade Commission, the Office of Personnel Management, the Department of Justice, and the Commodity Futures Trading Commission.\n495Memorandum from Louis Kincannon, Assistant Administrator of OMB to Clearance Officers of Departments and Agencies 1 (March 27, 1981) (on file at HARV. J. ON LEGIS.).\n\nHarvardJournalon Legislation [Vol. 24:1\nperwork Reduction Act. 496 OIRA quickly modified the form to indicate whether it was being used for an E.O. 12,291 review or for a paperwork clearance review. 497 Agencies thus submit two\nseparate copies of the form if a regulation is being reviewed under both E.O. 12,291 and the Paperwork Reduction Act. 498\nFor the desk officer, review under both authorities is essentially the same: to determine whether the benefits of the agency action\njustify the action and, if so, to determine whether the burdens\nof the action have been minimized. The respective procedures of E.O. 12,291 and paperwork\nclearance review are likewise indistinguishable. The desk officer reviews the materials submitted by the agency.49 9 If the submission is a rule, the submission must include the rule and its preamble, as well as any regulatory impact analysis. 0\u00b00 If the submission is a request for paperwork clearance, the submission must include a narrative justification,50 1 which serves the same purpose as a regulation's preamble. The justification should include a full description of the burden to be imposed by the reporting or recordkeeping requirement and the reasons for imposing that burden, including efforts to achieve the same or similar ends through alternative means such as using or modifying already existing collections. 0 2\nIf the desk officer has any questions regarding either a regu-\nlation or paperwork requirement, he or she may contact the\n49 Id. 497 Id. 491Memorandum from Nat Scurry, Chief of the Reports Management Branch of OIRA to Department and Agency Regulatory Contacts and Clearance Officers 1 (May 20, 1982) (on file at HARV. J. ON LEGIS.). 49 The officer within OIRA authorized to approve agency submissions depends on the submission. For example, at one time, no one beneath a branch chief could approve a regulation under either E.O. 12,291 or Paperwork Reduction Act review. Interview with Nell Minow, Special Assistant to the Administrator of OIRA, in Washington, D.C. (June 21, 1985). Desk officers, on the other hand, could approve paperwork requirements not contained in regulations and imposing less than 50,000 annual burden hours. Paperwork requirements imposing over 1 million burden hours went to the Deputy Administrator. Id. It has become common to refer to agencies as \"appealing\" OMB's disapproval of a paperwork requirement. See, e.g., OMB WATCH, INFORMATION COLLECTION REPORT: OMB CONTROL OF PROGRAMS 10 (Nov. 18, 1985). OMB's regulations allow for reconsideration of a disapproval only if the agency head or designated senior official, 44 U.S.C. \u00a7 3506(b) (1982), requests it and submits significant new or additional information. 5 C.F.R. \u00a7 1320.11 (i) (1986). In effect, an \"appeal\" means only that the requesting agency has elevated the issue to a higher bureaucratic level.\n'0O U.S. OFFICE OF MANAGEMENT AND BUDGET, INTERIM INSTRUCTIONS FOR REQUESTING OMB REVIEW UNDER THE PAPERWORK REDUCTION ACT AND EXECUTIVE\nORDER 12,291 (March 27, 1981) (on file at HARV. J. ON LEGIS.).\n501 Id.\n502Id.\n\n1987]\n\nPaperworkReduction Act\n\nagency for further information. Invariably this is done orally.503 Sometimes the discussion becomes a forum for low-level negotiation for changes to the agency's proposal.50 4 Often an\nagency will withdraw a submission rather than have it disapproved, especially where the agency plans to resubmit the proposal in altered form.5 0 5 If a paperwork submission is disapproved, OIRA places a one-sentence formal explanation in the public file. 50 6 In the case of a regulation reviewed under\nE.O. 12,291, apparently not even a summary statement is made publicly available.\nE.O. 12,291 provides no mechanism for public comments to OMB on proposed or final rules under consideration. Indeed,\nno formal mechanism exists by which the public even knows when a proposed or final rule is submitted to OMB. Interested\npersons, however, often are aware of the submission either from their sources within the agency, from the trade press, or from requests for their views from OMB.5 0 7 A significant volume of\ncomments to OMB on draft rules submitted by agencies may result.508 OMB places such materials in OIRA's public reading\nroom. Moreover, OMB advises persons communicating with OMB to communicate also with the agency involved.509\nSpecific provisions of the Paperwork Reduction Act affect\npublic comment on agency submissions under the Act. For example, section 3507(a)(2) requires an agency submitting a proposed \"information collection request\" to OMB to publish a notice of that fact in the Federal Register5.10 The notice pro-\n\n10S1ee Olson, The Quiet Shift ofPower: Office of Management & Budget Supervision\nof Environmnental Protection Agency Rulemaking Under Executive Order 12,291, 4 VA. J. NAT. RESOURCES L. 1, 56 (1984) [hereinafter Olson]. This article is noteworthy in its\ndiscussion of the realities of OMB review under E.O. 12,291. Its author obtained access to a substantial number of internal OMB documents involved in E.O. 12,291 reviews, something few have accomplished.\n.04 Id. 50 MB WATCH, MONTHLY REVIEW: EYE ON PAPERWORK 2 (Sept. 22, 1986). 50 See SUBCOMM. ON OVERSIGHT AND INVESTIGATIONS OF HOUSE COMM. ON EN-\nERGY AND COMMERCE, 99TH CONG., 2D SESS., OMB REVIEW OF CDC RESEARCH: IMPACT OF THE PAPERWORK REDUCTION ACT 24, 32, 34, 36, 37 (Comm. Print 1986)\n[hereinafter CDC REPORT]. 101Olson, supra note 503, at 56. 5' Id. .09Memorandum from Wendy L. Gramm, Administrator of OIRA to the Heads of\nDepartments and Agencies Subject to Exec. Order Nos. 12,291 and 12,498 (June 13, 1986), reprinted in 5 INSIDE THE ADMIN. 4 (August 14, 1986) [hereinafter Gramm Disclosure Memorandum].\n-'1044 U.S.C. \u00a7 3507(a)(2) (1982); see also 5 C.F.R. \u00a7 1320.12(a) (1986). The latter regulation requires an agency to send its notice to the Federal Register on or before the day the submission is made to OMB. This is not always done. OMB WATCH,\n\nHarvardJournalon Legislation [Vol. 24:1\nvided, however, has often consisted of little more than the title of the collection and information as to where further information can be obtained. 51' Thus, only the most sophisticated observers have taken advantage of the public notice. In practice, trade press and direct OMB solicitation are more likely sources of notice. While nothing in the Act addresses such a requirement, OMB has acquiesced to public demand for access to paperwork docket files, 51 2 which supposedly contain all public comment on agency paperwork submissions. The completeness of the files, however, is doubtful. 5 3 They generally have not contained copies of correspondence between OMB and the agency concerning\nthe agency's submission. The bulk of communication by OMB with agencies and third\nparties under either E.O. 12,291 or the Paperwork Reduction Act is oral. 514 Neither the Order nor the Act require the reduction of these communications to writing, and OMB is loath to do so. 515 Such a requirement would create a significant added burden for its staff, which might in turn create a disincentive to oral discussion. OMB has also opposed attempts by agencies to make records of oral communications with OMB. 51 6 OMB's explanation seems to reflect more truly its overall reluctance to memorialize oral communications: \"If everything is to be shared [with the public], then advice is not candid and to the point and straightforward.\" 517\nThe near identity in treatment of regulations reviewed under E.O. 12,291 and paperwork collections under the Paperwork\nINFORMATION COLLECTION REPORT: OMB CONTROL OF PROGRAMS 5 (Oct. 28, 1985). Indeed, the Department of Treasury has established a practice of not publishing any notice in the Federal Register when it sends to OMB for clearance an information collection request which is a regulation. See supra note 346. The justification may be that because these regulations do not require prior notice and comment under the Administrative Procedure Act, they should not require prior notice under the Paperwork Reduction Act. While there is a certain logic to this justification, it is not sufficient to overturn the plain language and intent of both the Act and the OMB regulation. The fact that OMB condones the practice does not make it lawful. Id.\n-'\" Olson, supra note 503, at 60. 512 See, e.g., id. at 64 n.324 (author receives docket files in response to Freedom of Information Act request). The 1986 amendments to the Paperwork Reduction Act, see supra note 345, have created a requirement for public access to agency and public written comments to OMB. 5,3 Olson, supra note 503, at 64. This author's experience with the paperwork docket files has been similar to that of Olson. 514 Id. at 55-57. -15Id. at 58-59. 56 Id. at 59. 5I7 The Role of OMB in Regulation: Hearings of the Oversight and Investigations\nSubcomm. of the House Comm. on Energy & Commerce, 97th Cong., 1st Sess. 57 (1981) (testimony of James Miller, III, former OIRA Administrator).\n\n1987]\n\nPaperworkReduction Act\n\nReduction Act provokes inquiry in three areas: the process by which regulations containing paperwork requirements are reviewed; the difference in authority OIRA possesses under the Order and under the Act with respect to an agency's submission; and the possibility of OMB misuse of its authority under the Paperwork Reduction Act to achieve policy goals unrelated to paperwork.\n\nC. Review of PaperworkRegulations\nUnder the Paperwork Reduction Act, as interpreted by the Office of Legal Counsel opinion,5 18 OMB may comment publicly on a collection of information requirement within sixty days after an agency publishes a proposed rule containing the requirement.519 If OMB fails to make any such comment, the Act bars OMB from disapproving the requirement. 20 If OMB comments and the agency's response in the final rule is unreasonable, OMB may disapprove the requirement. 521 In practice, however, the process has operated differently.\nFor executive agencies, E.O. 12,291 requires submission of all proposed rules to OMB at least ten days before their initial publication.5 22 Major rules must be submitted at least sixty days before publication.5 23 OMB may also extend these periods indefinitely at its discretion.5 24 These periods enable OMB to review and comment on proposed rules and final rules before publication. OMB's comments pursuant to the Executive Order are not placed in the rulemaking file. 525 If OMB does not object\nI'8 See supra notes 232-65 and accompanying text.\n51944 U.S.C. \u00a7 3504(h)(2) (1982). 520Id. \u00a7 3504(h)(4). If the agency fails to provide the required notice of rulemaking to\nOMB or if the agency substantially changes the proposal in the final rule without giving OMB a further opportunity to comment, OMB may disapprove the requirement at its discretion. Id. \u00a7 3504(h)(5).\n.121Id. \u00a7 3504(h)(5)(C). 55223-EId.O. . 12,291, supra note 17, at \u00a7 3(c). 524Id. at \u00a7 3(f). '5 During the summer of 1986, OMB came under substantial pressure from Congress regarding the secrecy of its regulatory review process under both E.O. 12,291 and E.O. 12,498. OIRA reached an agreement with Senators Levin (D-Mich.) and Durenberger (R-Minn.) whereby OIRA committed itself to an after-the-facr disclosure of review documents. Specifically, OIRA agreed to make public, upon written request, copies of the drafts of advance notices of proposed rulemaking, notices of proposed rulemaking, and final rules submitted by agencies to OMB for review under E.O. 12,291,\n\nHarvardJournalon Legislation [Vol. 24:1\nto the proposed rule, then the agency is so informed. 526 If OMB does object, informal negotiation between OMB and the agency occurs until OMB withdraws its objection.5 27 Then the proposed rule, perhaps significantly altered as a result of OMB's objections, is published for comment. A similar process is followed for the final rule.5 28\nObviously, this procedure differs from the process envisioned in section 3504(h) for proposed rules containing collection of information requirements. 529 OMB need not, and does not, follow the procedure in that section in its review of executive branch regulations containing collection of information requirements. Rather than make public comments on the proposed rule during the normal comment period, OMB makes private comments to the agency pursuant to its E.O. 12,291 review prior to publication of the proposed rule and then again between the end of the public comment period and publication of the final rule. 530 Moreover, OMB's ability to delay indefinitely publication of either a proposed or final executive branch rule amounts to the substantive power to deny approval on any basis OMB chooses. OMB withholds both the fact of denial and the bases for it from the public.53'\nOMB thus avoids the Kennedy Amendment when reviewing collection of information requirements for proposed regulations. This avoidance of the Kennedy Amendment does not mean that OMB disregards the Amendment. The Kennedy Amendment was written to limit substantively the powers given to OMB by the Paperwork Reduction Act with respect to collection of information requirements and to require OMB's exercise of these powers to be carried out pursuant to the procedures of section 3504(h). 532 The Kennedy Amendment did not address OMB's\nafter their publication in the FederalRegister. Gramm Disclosure Memorandum, supra note 509.\nAlso, OIRA will make available correspondence between OIRA and the agency head concerning the draft documents. Id. While the agreement with the Senators limits the disclosure of correspondence to that involving the agency head, OIRA will also apparently disclose correspondence sent from OIRA to any agency personnel regarding a proposed or final rule under review. See OMB Allows Disclosure of Correspondence from OIRA to Agency Personnel,5 INSIDE THE ADMIN. I (Sept. 4, 1986).\n526 Olson, supra note 503, at 56-59. 527Id. 528 Id. 529See supra notes 245-61 and accompanying text. 530 See supra notes 525-28 and accompanying text. 531 But see supra note 525.\n'31See supranotes 159-60 and accompanying text.\n\n19871\n\nPaperworkReduction Act\n\nexercise of power over regulations pursuant to authority other than the Paperwork Reduction Act. Under E.O. 12,291, OMB\nis able to influence executive branch regulations generally without public scrutiny of its influence. Regulations imposing paperwork requirements are only a small subset of the regulations covered by the Executive Order, and the review of that subset under the Order does not require the exercise of any powers granted to OMB by the Paperwork Reduction Act.5 33 Consequently, OMB's use of E.O. 12,291's procedures for executive branch regulations imposing reporting or recordkeeping requirements raises no more questions than its use of those procedures generally for executive branch regulations. 34\nOMB's use of E.O. 12,291 rather than section 3504(h) to influence or control regulations containing reporting or record-\nkeeping requirements should be disturbing to those who saw section 3504(h) as a means of assuring the possibility of meaningful public participation in decisions regarding collection of information requirements 35 Equally disturbing is the fact that\nOMB's penchant for non-public decision making appears to extend beyond collection of information requirements. The Kennedy Amendment's attempt to ensure public participation was not limited to collection of information requirements and section 3504(h). The amendment also built upon the requirement already\nestablished in section 3507(a)(2) that agencies publish in the FederalRegister a notice of submission to OMB of any information collection request.5 36 The Kennedy Amendment required that, in reviewing information collection requests, OMB provide\n\"interested agencies and persons early and meaningful opportunity to comment.' 537 Moreover, the Amendment required that OMB's decision to approve or disapprove an agency's information collection request be made publicly available.5 38\nTogether these provisions suggest, if not mandate, a significant opportunity for involvement by interested persons and\n\n\"I In order to affect an independent agency's regulation imposing a reporting or recordkeeping requirement, OMB must utilize the procedure of \u00a7 3504(h), because E.O. 12,291 would not be applicable. E.O. 12,291, supra note 17, at \u00a7 l(d).\n114There is, of course, active scholarly, political, and judicial debate over the broader\nquestions of OMB's procedures and powers under E.O. 12,291. See supra note 18; see\nalso Environmental Defense Fund v. Thomas, 627 F. Supp. 566, 571 (D.D.C. 1986) (judicial inquiry into OMB delay in promulgating regulations).\n531See supra text accompanying notes 156-59. 53644 U.S.C. \u00a7 3507(a)(2) (1982).\n-'11 Id. \u00a7 3517. -38Id. \u00a7 3507(b).\n\nHarvardJournalon Legislation [Vol. 24:1\nagencies in decisions concerning information collection requests. While the public procedures applicable to information collection requests are not as comprehensive as those available for collection of information requirements in notice-and-comment rulemakings, the procedure applicable to requests does not create a strictly two-part, relationship between the agency and OMB.\nOMB's regulations implementing the Act appear to reflect an acceptance of this public involvement, but in practice OMB has shown little interest in ensuring that agencies comply with the provisions regarding public participation in either its regulations or the Act. For example, OMB's regulations require that agencies send their notice of a submission of an information collection request to the FederalRegister not later than the date of the submission to OMB. 539 In this way, the public will be informed of a submission in time to have a meaningful opportunity to comment. OMB, however, is lax in enforcing this requirement, and it is often violated.5 40 It is not rare for a collection to be approved by OMB before the notice of its submission is even published. 54' While a means to police the requirement is readily available to OMB, OMB prefers to wait to receive complaints before acting.5 42\nEven a timely notice in the FederalRegisteris of little use if the notice is uninformative as to the information collection request involved. There was no uniform format prescribed for notices, and each agency determined on its own what notice it believed to be sufficient. While OMB could in its regulations prescribe a minimum amount of information, it has refrained from doing SO,543 and almost any issue of the Federal Register contains agency notices that contain information insufficient to allow meaningful comment by the public. 544 Moreover, at least\n539 5 C.F.R. \u00a7 1320.12(a) (1986). 540OMB Watch has begun a regular compilation of such violations. See, e.g., OMB\nWATCH, supra note 499.\n541 Id.\n542 Id.\n53 Cf. 5 C.F.R. \u00a7 1320.18(b) (1986) (minimum information required to be included on the information collection request itself).\n54 See, e.g., 51 Fed. Reg. 37,070 (1986) (Federal Communications Commission notice). In the 1986 amendments to the Paperwork Reduction Act, see supra note 345, Congress took matters into its own hands and specified the minimum information that agencies must publish in their Federal Register notices: the title of the request, a brief description of the need for the information and its proposed use, a description of the likely respondents and the number of times they will be asked to respond, and an estimate of the burden that will result from the information collection request. See Pub. L. No. 99-500, Title VIII, 100 Stat. - (1986).\n\n1987]\n\nPaperworkReduction Act\n\none agency, the Treasury Department, has decided to omit the\nFederal Register notice altogether where the information collection request is a regulation adopted without notice and comment. 545 OMB is aware of Treasury's failure to provide notice546\nbut has implicitly condoned it. In short, OMB is not enforcing\nthe public participation provisions of the Act or OMB's own regulations. Rather, OMB's actions suggest that it views the process as taking place strictly between OMB and the agency.\nThis does not mean that OMB is not interested in the views of certain members of the public. OMB desk officers on occasion will even initiate public participation by calling a particular lobbying group to ask for comments. 547 More frequently, desk officers accept calls from groups they view favorably. 548 Such oral communication, moreover, is not memorialized for the public\nfile. Indeed, some affected agencies assert that even they are sometimes not apprised of the origins of OMB comments which, because of their nature, the agencies believe are unlikely to have originated with OMB. 549 This selective and non-public pub-\nlic participation is not what was intended by the Act.\n\nD. OMB's Power To Determine the Need for Information\nExecutive Order 12,291 by its terms gives OMB no substantive power to disapprove or overrule an agency's determination. Rather, OMB must rely on its power to convince an agency to change its position. Under the Paperwork Reduction Act, however, OMB has the express power to overrule an agency's determination of its need for information, if the collection is not directly required by a statute or regulation.\nSection 3504(c)(2) provides that one of the Director's functions is to determine whether an agency's collection of information is \"necessary for the proper performance of the functions of the agency, including whether the information will have practical utility for the agency. '550 Section 3508 states that \"[b]efore approving a proposed information collection request, the Director shall determine whether the collection of information by an\n\n-- See supra note 346.\n5.4I6d.\n'47 See Olson, supra note 503, at 56. 58 Id. 49 Id. at 57. 55044 U.S.C. \u00a7 3504(c)(2) (1982).\n\nHarvardJournal on Legislation [Vol. 24:1\nagency is necessary for the proper performance of the functions of the agency, including whether the information will have practical utility. '551 That section concludes with the statement that \"[tio the extent, if any, that the Director determines that the collection of information by an agency is unnecessary, for any reason, the agency may not engage in the collection of the information. '552 This power extends, at least initially, even to the independent agencies, though they may overrule the Director's determination by a majority vote. 553 Despite this difference in power granted by the Executive Order and the Act, there is little indication that OMB's practice under the two sources differs.\nThere have been a number of criticisms of the way OMB has influenced agency regulations pursuant to E.O. 12,291.514 These criticisms are generally either that OMB is acting in excess of its authority or that its procedures subvert the public rulemaking procedure. 555 In assessing the authority granted to OMB by the Paperwork Reduction Act, it is natural to compare OMB's practice under the Executive Order with its practice under the Act. No one suggests that OMB should have more power over executive agency regulations. 556 Moreover, those supportive of OMB's oversight and deregulatory powers do not suggest that OMB's powers over executive agencies are insufficient for the task.557\nIn reality, OMB's power to persuade agencies is formidable. That power, however, derives not from legal arrangements as much as political ones. Despite OMB's substantial authority\n5 Id. \u00a7 3508.\n552Id. 553Id. \u00a7 3507(c). 5'4See, e.g., Olson, supra note 503; see also Regulatory Reform Act: HearingsBefore the Subcomm. on Admin. Law and GovernmentalRelations of the House Comm. on\nthe Judiciary,98th Cong., Ist Sess. 880 (1983). -'- Olson, supranote 503; see also Fix & Eads, The Prospectsfor RegulatoryReform:\nThe Legacy ofReagan's FirstTerm, 2 YALE J.ON REG. 923 (1985). 556 The American Bar Association's Section on Administrative Law has, however,\nrecommended the extension of the order to independent regulatory agencies. ABA Midyear Meeting, Legal Times 2, 4 (Feb. 17, 1986). The waning academic and judicial support for the constitutionality of the independence of independent regulatory agencies suggests that the future, or at least the nature, of that independence is clouded. See, e.g., Strauss, The Place of Agencies in Government: Separation of Powers and the FourthBranch, 84 COLUM. L. REV. 573, 662-66 (1984); see alsoSynar v. United States, 626 F. Supp. 1374 (D.D.C. 1986), aff'd sub nom. Bowsher v. Synar, 54 U.S.L.W. 5064 (1986).\n\"I See, e.g., DeMuth, The Reagan Record: A Strategy for Regulatory Reform, 7 REGULATION 25 (March/Apr. 1984).\n\n1987]\n\nPaperworkReduction Act\n\nover agencies (primarily the budget and personnel ceilings), absent presidential support and congressional acquiescence, OMB has little institutional clout. The power of OMB in the Reagan Administration in fact derives primarily from strong presidential support and a generally supportive Senate, as well as public sentiment generally favoring deregulation.\nOf equal importance to OMB's power to persuade is the desire by agency heads to support the President's program, rather than the program defended by the career bureaucracy. The career\nbureaucracy defends not so much a statutory program as the gloss which that bureaucracy has placed upon the statute. A new administration, especially one succeeding that of a different party, views that gloss not as a neutral and technocratically correct result, but as the political result that flows from the premises of the prior administration. New agency heads inherit their program infrastructure, yet they cannot rely upon it to reflect the new administration's political goals. The OIRA staff, however, is believed to be politically reliable and more likely to reflect the President's program than the agencies' staffs. 558 Thus, an agency head desirous of furthering the President's program may be easily persuaded by OIRA that an agency's proposed action in fact is not as well supported on the merits as the agency\nstaff suggests. If OMB's current power to persuade is in reality virtually the\npower to compel, it may not be surprising that OMB's substantive actions under the Paperwork Reduction Act do not differ significantly from its actions under the Executive Order. What is interesting is that, despite OMB's clear authority to overrule agency determinations of the need for information, OMB has generally proceeded by negotiation and persuasion rather than by fiat.55 9 Despite administration rhetoric suggesting that it is drastically cutting paperwork burdens, 560 in fact it has proceeded cautiously, picking its targets rather carefully and withdrawing\n\n..8 It is interesting that, in fact, most of the original staff of OIRA was inherited from\nthe Carter Administration's Office of Information and Regulatory Policy as well as its Council on Wage and Price Stability. Even the eminence grise of the initial Reagan deregulatory activities, Deputy Administrator James Tozzi, was a Carter holdover. This suggests that under the Carter Administration OMB's program was not far removed from that under President Reagan, as evidenced by the substantive similarity between E.O. 12,044 and E.O. 12,291. One major difference, however, is the lack of support given by the Carter Administration to regulatory reform. R. LITAN & W. NORDHAUS,\nREFORMING FEDERAL REGULATION 68-69 (1983).\n\"' Interview with Nell Minow, supra note 499. 1601982 ICB, supra note 5, at 33.\n\n",
        "hash_id": "76c5d9676c5b92e843b3c9e22f67e449"
    },
    {
        "key": "VPHAX48D",
        "version": 20,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/VPHAX48D",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/VPHAX48D",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/7MWIW2YW",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 7513848
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Funk",
            "parsedDate": "1987",
            "numChildren": 1
        },
        "citation": "<span>William F. Funk, <i>The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law</i>, 24 <span style=\"font-variant:small-caps;\">Harv. J. on Legis.</span> 1\u2013116 (1987), https://heinonline.org/HOL/P?h=hein.journals/hjl24&#38;i=7 (last visited Nov 14, 2022).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "CMD3RCGU",
        "version": 18,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/CMD3RCGU",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/CMD3RCGU",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/86M9HQNX",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/CMD3RCGU/file/view",
                "title": "Shapiro - 2013 - The Paperwork Reduction Act Benefits, costs and d.pdf",
                "length": 201988
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>ScienceDirect Full Text PDF, https://www.sciencedirect.com/science/article/pii/S0740624X13000087/pdfft?md5=262e4b4783827d53cbbc99a25f669c97&#38;pid=1-s2.0-S0740624X13000087-main.pdf&#38;isDTMRedir=Y (last visited Nov 14, 2022).</span>",
        "fulltext": "Government Information Quarterly 30 (2013) 204\u2013210\nContents lists available at SciVerse ScienceDirect\nGovernment Information Quarterly\njournal homepage: www.elsevier.com/locate/govinf\n\nThe Paperwork Reduction Act: Bene\ufb01ts, costs and directions for reform\u2606\nStuart Shapiro \u204e\nBloustein School of Planning and Policy, Rutgers University, 33 Livingston Ave #538, New Brunswick NJ 08901, USA\n\narticle info\nAvailable online 16 February 2013\nKeywords: Paperwork Reduction Act Cost\u2013bene\ufb01t analysis Information resource management Information collection\n\nabstract\nCongress passed the Paperwork Reduction Act (PRA) in 1980. Intended to ensure that the federal government carefully managed information and to reduce the burden of information collection on the American public, it has arguably failed to do either. This article uses a simple analysis of the bene\ufb01ts and costs of the Act to evaluate possible directions for reform. The implementation of the PRA has resulted in the misallocation of government resources. Far too much time is spent at the Of\ufb01ce of Management and Budget and at agencies reviewing collections and soliciting input on thousands of information collections that are routine and unchanging. If this time was cut back, both OMB and agencies could devote more time to new information collections that have methodological issues and signi\ufb01cant policy impacts. Agencies and OMB could also devote more time to tying information collection to information management as the authors of the PRA initially intended.\n\u00a9 2013 Elsevier Inc. All rights reserved.\n\n1. Introduction\nThe Paperwork Reduction Act (PRA) is a statute that is often derided and rarely praised. Agencies forced to comply with it argue that it increases paperwork. Advocates of public health argue that it deters agencies from collecting information that will support new environmental and health regulations. Even the supporters of the statute continually point to the growing burden of information collections on the American public and a lack of compliance with the Act. All parties agree that the statute, last revised in 1995, is ill-equipped to deal with the massive changes in the ways that information can be collected and utilized in the 21st Century.\nThe PRA does serve a worthwhile goal, ensuring that the American public is burdened by information collection only when the information collected provides suf\ufb01cient practical utility to the government. Attempts to reauthorize the statute have been bogged down. The Administrative Conference of the United States (ACUS)1 recently took up questions of how to reform the statute. I was the consultant\n\u2606 I would like to thank Jeffrey Lubbers, Emily Bremer, David Plocher, David Rostker, Anne Gowen, and members of the Administrative Conference of the United States Committee on Administration and Management for helpful comments on earlier versions of this paper. I would like to thank interview subjects for their generous allocation of time to this project. All errors are my responsibility.\n\u204e Corresponding author. E-mail address: stuartsh@rutgers.edu.\n1 See http://www.acus.gov/research/the-conference-current-projects/paperworkreduction-act/ (last viewed June 27, 2012). (Note: this is the ACUS project site for the Paperwork Reduction Act and includes the request for proposals, the consultant report, and all subsequent public comments. It also contains the \ufb01nal ACUS recommendation on the PRA.)\n0740-624X/$ \u2013 see front matter \u00a9 2013 Elsevier Inc. All rights reserved. http://dx.doi.org/10.1016/j.giq.2012.09.002\n\non the project and this article is an attempt to capture my views on the principles that should be followed as reform is considered.\nThese principles come from examining the PRA using a bene\ufb01t\u2013 cost framework. Bene\ufb01t\u2013cost analysis is used in the United States and Europe to evaluate regulatory initiatives. But it can also be used to evaluate measures that affect government operations such as the PRA. In this paper, I use a rough bene\ufb01t\u2013cost analysis to evaluate how the PRA functions and to point the way toward reforms. This framework clearly points toward focusing the energies of the Of\ufb01ce of Information and Regulatory Affairs (OIRA) and agencies toward the most critical information collections and to reducing the scrutiny it gives to the less important ones.\nThe \ufb01rst goal of this paper is to use bene\ufb01t\u2013cost analysis to suggest reforms that would improve the Paperwork Reduction Act. The second is to use the Paperwork Reduction Act to demonstrate the utility of bene\ufb01t\u2013cost analysis (BCA) for evaluating reforms to government practice. The use of BCA to evaluate regulations has been controversial since its inception in the 1970s (Tozzi, 2011). However, it is possible that the use of BCA to evaluate reforms in public administration will be considerably less controversial because the impacts on policy are indirect. The potential to improve the effectiveness of government (and to demonstrate areas for potential improvement) argues for further adoption of BCA beyond the regulatory sphere.\nThis article proceeds as follows. The next section is a review of the relatively scant literature on the PRA and on the use of BCA to evaluate regulatory reforms. I then proceed to detail my research methodology, and tabulate the bene\ufb01ts and costs of the PRA in the subsequent three sections. In Section 6, I use the results of the preceding analysis to discuss reforms to the PRA. I conclude with observations both on the PRA and on the use of BCA.\n\nS. Shapiro / Government Information Quarterly 30 (2013) 204\u2013210\n\n205\n\n2. The Paperwork Reduction Act and bene\ufb01t\u2013cost analysis\nThe Paperwork Reduction Act has gotten far more attention inside the government than outside it. Most of the academic work on the PRA has focused on the history of attempts to reduce the burden of information collections on the American public. Probably the most detailed history was written in 1987, by William Funk. Funk details the history of the Federal Reports Act (FRA), which was passed in 1942, and \ufb01rst required that federal agencies clear their information collections with a centrally located of\ufb01ce (then the Bureau of the Budget). He also details the dissatisfactions with the FRA and the resulting Commission on Paperwork Reduction from the 1970s. The work of this Commission led directly to the passage of the PRA in 1980 (Funk, 1987).\nLubbers (1996) discusses the wholesale revisions to the PRA in 1995. These revisions set up the current information collection clearance process. Agencies wishing to collect information from ten or more individuals must \ufb01rst publish a notice in the Federal Register and give the public sixty days to comment on the collection. Once the comment period is closed, the agency publishes a second notice that announces that it is sending the collection to OMB for approval and informing the public that it may submit comments to OMB within 30 days. OMB then has 30 additional days from the end of the comment period to approve or disapprove the collection. Agencies must submit the responses to 18 questions to OMB along with the proposed instrument2 (there are 23 questions for information collections with a statistical purpose).\nInformation collections within proposed regulations follow a slightly different process with the proposed rule taking the place of the \ufb01rst Federal Register notice and the \ufb01nal rule taking the place of the second notice (Lubbers, 1996). The information collection provisions of a rule do not take effect until they are approved by OMB, regardless of the effective date of the rule.\nThere are also a number of exemptions to the Act, including collections from government employees, minor changes to approved collections,3 and emergency collections which only require one Federal Register notice, a shorter comment period, and OMB approval. Finally, OMB has developed a process for \u201cgeneric clearances\u201d whereby an agency goes through the full process for a type of information collection and then is allowed to solicit OMB approval on individual collections within this type on an expedited basis. The Obama Administration has highlighted this tool which is often used for customer satisfactions surveys4 as a way to streamline the application of the PRA.\nBeyond descriptions and histories of the Act such as those by Lubbers and Funk, the only academic articles on the PRA have been discussions of the failure to implement information resource management (IRM). IRM is one of the central goals of the PRA (Relyea, 2000) and re\ufb02ects the idea that information has a life-cycle (its collection, its use, its dissemination, its storage, and its disposal) and agencies should pay attention to all aspects of that life-cycle when deciding to collect information. Relyea (2007) and Plocher (1996) both bemoaned the focus of government resources on the information collection process at the expense of a more holistic IRM perspective.\nThe Government Accountability Of\ufb01ce (GAO) has issued numerous reports on both the clearance process for information collections and on IRM. The reports on the clearance process sound several consistent themes. They highlight the increasing burden of information collections on the American public, the dominance of a small number of collections by the Internal Revenue Service (IRS) in making up the total burden, violations of the Act by agencies, and the lack of resources at OIRA to\n2 The questions and cover sheet can be found here: http://www.whitehouse.gov/ sites/default/\ufb01les/omb/inforeg/83i-\ufb01ll.pdf (last viewed January 5, 2012).\n3 These must still be approved by OMB but do not require Federal Register notices. 4 See http://www.whitehouse.gov/sites/default/\ufb01les/omb/assets/inforeg/PRA_Gen_ ICRs_5-28-2010.pdf (last viewed January 12, 2012) for the memo from OIRA Administrator Cass Sunstein on generic clearances.\n\nexercise more effective oversight. Similarly the reports on IRM tend to focus on how the executive branch has focused its attention on the acquisition of information technology rather than on information management.\nFinally, OIRA issues a statutorily required annual report to Congress entitled \u201cThe Information Collection Budget of the United States.\u201d This report typically has four sections of potential use to researchers. OIRA details the total burden imposed by federal agencies in the previous years and discusses all signi\ufb01cant changes in burden from the previous year. OIRA also lists all of the violations of the PRA that OIRA has discovered and lists the ones that are unresolved. Finally, OIRA typically includes in the report some narrative sections on important issues regarding information collection practices.\nNone of these sources attempts to systematically analyze the bene\ufb01ts and costs of the PRA. In fact, while there have been much discussion and analysis of the bene\ufb01ts and costs of regulations, there have been comparatively little on the bene\ufb01ts and costs of regulatory reforms such as the PRA. The earliest piece calling for an analysis of the costs and bene\ufb01ts of regulatory reforms is Portney's (1984) examination bene\ufb01t\u2013cost analysis itself.\nPortney calculated the direct costs of requiring agencies to conduct bene\ufb01t\u2013cost analysis of their regulations but (by his own admission) omitted the more dif\ufb01cult to calculate indirect costs. He also declined to attempt to calculate the bene\ufb01ts of a BCA requirement, but argues convincingly that the bene\ufb01ts were likely to outweigh the direct costs. The crux of this argument is that the direct costs were small ($25 million in 1984 dollars) and hence a 1% savings as a result of BCA (since rules regularly impose costs of more than $2.5 billion annually) would signify that the bene\ufb01ts of the requirement were greater than the costs.\nShapiro (2008) called for more examination of regulatory reforms using bene\ufb01t\u2013cost analysis. He applied a rudimentary analysis to a requirement that agencies conduct peer reviews of scienti\ufb01c documents supporting signi\ufb01cant regulations. Like Portney, Shapiro was unable to produce a de\ufb01nitive result but argued that, in this case, the bene\ufb01ts of the reform (regulatory peer review) were unlikely to outweigh the costs.\nOther attempts to assess regulatory reforms have been examinations of effectiveness rather than comprehensive analyses of costs and bene\ufb01ts. Again, BCA has been the reform most closely examined with studies on its effectiveness. OMB summarized 47 such studies in its 2005 report to Congress on the costs and bene\ufb01ts of regulations. Scholars have also evaluated participation requirements such as notice and comment rulemaking (Golden, 1998; West, 2004; Yackee, 2006) and requirements for regulatory negotiation (Coglianese, 1997). And of course, as described above, GAO has done numerous studies on the effectiveness of the Paperwork Reduction Act.\nStudies of effectiveness are useful. Showing that a regulatory procedure is ineffective at achieving its goals indicates that it is unlikely to pass a cost\u2013bene\ufb01t test. However, subjecting a procedural constraint on agency decision-making to a bene\ufb01t\u2013cost analysis, even an imperfect one, is a more comprehensive way of evaluating its effectiveness. The remainder of this paper describes my attempt to do so for the Paperwork Reduction Act.\n3. Research data and methodology\nI originally conducted much of the research for this article as part of a research project for the Administrative Conference of the United States (ACUS). While ACUS had several speci\ufb01c questions it wanted to see answered as part of the project, they also asked for an assessment of the bene\ufb01ts and the costs of the Act. Toward that end, I began my research by gathering data on the information collection process. I used data from the annual Information Collection Budgets and the website reginfo.gov to ascertain the number of collections reviewed each year, which types of collections (voluntary vs. non-voluntary, statistical vs. non-statistical, collections from individuals vs. collections from\n\n206\n\nS. Shapiro / Government Information Quarterly 30 (2013) 204\u2013210\n\nbusinesses etc.). OMB reviewed, the total burden hours, and the disposition by OMB of different types of information collection requests. I also gathered data on the comments received for information collection requests.\nThe data on the number of information collections shows little change (except for an increase in 2011) (Table 1).\nUntil 2011 (which may have shown an increase in collections because of the passage in 2010 of the Affordable Care Act and the Dodd-Frank Financial Wall Street Reform and Consumer Protection Act) there were regularly approximately 4000 active information collections and 700\u2013800 new collections each year.\nHow many burden hours do these collections impose on the American public? As I discuss below, the measure of burden hours is an imprecise tool that is often criticized. Still, the overall trend in burden hours is illustrative. While differences in burden hours in the thousands are likely to be inaccurate, differences in the billions are likely to signify real changes.\nBefore anyone gets too excited about the one billion hour decrease in 2010, it results from a correction of a previous error, rather than an effort by the government to reduce burden.\nSixty-three percent of collections are aimed at collecting data from businesses and twenty-seven percent, from individuals. This has also been largely consistent over time; the current data nearly mirror results reported by GAO (2002). In that same report, GAO also found that 95% of collections were for regulatory compliance (including tax compliance) and fewer than 5% of collections were for bene\ufb01t applications (the remainder (less than 1%) were for \u201cother categories\u201d such as surveys).\nThe data presents a stark picture of the increase in burden since the passage of the Paperwork Reduction Act. However, data alone cannot \ufb02esh out the picture of how the information collection approval process operates. Understanding the day-to-day mechanics of the PRA process is critical to an analysis of bene\ufb01ts and costs of the PRA. It is also critical to evaluating reforms to the PRA. In order to acquire this understanding, I conducted 21 interviews of people familiar with the PRA.\nAll of the interview subjects were promised con\ufb01dentiality so they would speak freely. Such guarantees of con\ufb01dentiality are standard in good qualitative research (Rubin & Rubin, 2005). Twenty of the interviews were with individuals and one was with a group of OIRA of\ufb01cials. The individual interviews broke down as follows: \ufb01ve were with former OIRA of\ufb01cials, six were with current or former agency of\ufb01cials familiar with the PRA, four were with current or former GAO examiners who had contributed to the many reports on the PRA, and \ufb01ve were with representatives of outside interests who had long histories of involvement with PRA-related issues.\nBecause the ACUS charge was broader than just assessing the bene\ufb01ts and costs of the PRA, the interviews included questions on other subjects including information resource management and reforms to the Act. Interviews were also tailored to the particular expertise of the interview subject. However interviews had a consistent theme of meaningfully assessing the bene\ufb01ts and the costs of the Act.\n4. Bene\ufb01ts of the Paperwork Reduction Act\nIronically, an assessment of the bene\ufb01ts of the PRA reveals that it includes few of the bene\ufb01ts intended by the framers of the statute.\n\nTable 1 Information collections per year.\n\nYear\n\nNumber of information collection requests\n\n2011 2006 2001 1996 1991 1986\n\n4805 4076 3849 3484 3744 4176\n\nNumber of new requests\n970 705 691 644 846 799\n\nTable 2 Annual burden hours imposed by information collections.\n\nFiscal year\n1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010\n\nAnnual burden hours (in millions)\n6970 6967 7183 7361 7651 8223 8099 7971 8240 8924 9642 9711 9795 8783\n\nAs Table 2 reveals, the information collection burden on the public has increased steadily since the most recent incarnation of the PRA.5 The nearly two billion hour increase belies the argument that the Paperwork Reduction Act has reduced paperwork.\nIt is certainly possible that the burden imposed on the public by government collection of information would have increased more if the PRA had not been in place. However, the evidence that this is the case is very limited. While the Act may have deterred some information collections, these largely seem to be small collections, and federal agencies seem to modify their information collections to get around the requirements of the Act at least as often as they decide not to collect the information. Some observers maintain that violations of the Act (collections by agencies conducted without OMB approval) are rampant (Belzer, 2012). This would argue that the increase in burden since the passage of the PRA is even higher than reported in the Information Collection Budget.\nIn addition, it is unlikely that the PRA has led to better estimates of burden. The PRA helps us understand trends in government information collection, as discussed above. However, my interviews with those inside and outside of government make clear that agencies' PRA-mandated estimates of burden are highly questionable at best and random numbers at worst. Particularly challenging is the fact that some believe the numbers are signi\ufb01cant overestimates, while others believe the opposite.\nOMB best described the problem itself back in 1999:\n\u201cDespite public input and certain common methodological techniques, agency estimation methodologies can produce imprecise and inconsistent burden estimates. Many agencies simply rely on program analysts to generate burden estimates based on their individual consideration of, for example, the number and types of questions asked, what records will need to be created and maintained, how long it will take people to complete these and other tasks, and how many people will be performing the tasks. While these of\ufb01cials are often experts in their areas of responsibility and are usually familiar with the public's experience with responding to information collections they oversee, in many cases their estimates are not based on objective, rigorous, or internally consistent methodologies.\u201d\nThis problem of agencies estimating burden in Washington without testing their collections on the affected populations causes a general lack of faith in the burden numbers produced. One former OIRA of\ufb01cial said, \u201cI have no con\ufb01dence in burden hours.\u201d An agency of\ufb01cial said, \u201cI think the tabulating and counting of burden hours is an arti\ufb01cial\n\n5 The Information Collection Budget produced by OIRA which reports annual burden numbers was \ufb01rst issued in 1997.\n\nS. Shapiro / Government Information Quarterly 30 (2013) 204\u2013210\n\n207\n\nexercise that has no use in the real world.\u201d Finally, an outside expert called the process of calculating burden-hours \u201cpseudo science.\u201d\nFinally, one of the primary goals of the three iterations of the PRA (1980, 1986, 1995) was to improve information resource management. Indeed, the information collection approval process was intended to be merely a part of a broader information management process at the agencies. Yet the PRA has come to be identi\ufb01ed largely with the information collection clearance process and the other aspects of information resource management have faded to the background.\nThe original act provided \u201crelatively content-free process requirements leaving it to OIRA and agency initiative to give meaning to that IRM interconnectivity (Plocher, 1996)\u201d. Yet, in each reauthorization of the Act, failures in information management were cited as reasons for the reauthorization (Copeland and Burrows, 2009). And since the 1995 reauthorization of the PRA, agency Chief Information Of\ufb01cers have focused more on the purchase of information technology and less on information management (GAO, 2005).\nThis is unfortunate as a number of my interview subjects bemoaned the loss of focus on information management. IRM advocates talk about the \u201clife-cycle\u201d approach to information management. When information is collected from the public, thought must be given to how the information will be used by agencies, whether it will be disseminated by them (and if so, what privacy concerns apply), how long it will be stored, and how and when it will be disposed of. These issues become more complicated with electronic processing of information than they were with paper. Information collected via the Internet can be stored inde\ufb01nitely or easily be lost forever. A systematic government-wide approach to IRM would have tremendous bene\ufb01ts.\nSo, if burden reduction, burden accuracy, and information management are not among the bene\ufb01ts of the PRA, does the Act have any bene\ufb01ts? My interviews and examination of PRA-related data reveal two important categories of bene\ufb01ts. The \ufb01rst of these is the improvement of a small number of information collections, usually statistical in nature that agencies often use to gather data to evaluate the need for new regulations. The second category is the public participation provisions. While participation in information collection review is limited to a small percentage of collections, it occurs frequently enough that it should be recorded as a bene\ufb01t.\n4.1. Improvement of information collections\nWhile my interview subjects were nearly unanimous in agreement that the PRA does not reduce burden, many (though not all) cited the improvement of agency information collections as the chief bene\ufb01t of the PRA. What is the nature of these improvements? They tend to come in two categories. The \ufb01rst is preventing the government from asking the public questions that are ineffective, overly intrusive, offensive, or otherwise inappropriate. As one agency of\ufb01cial said,\n\u201cI've seen some of the things people put out there and they could get us in trouble. Too many people do these collections without thinking.\u201d\nOIRA of\ufb01cials cited examples of inappropriate questions that they had stopped and examples of collections that they had improved by, for example, asking an agency to develop a Spanish language version of a form (in a case where respondents were likely to be primarily Spanish speaking).\nThe second way that OIRA review improves information collections is through the work of the statistical policy branch in OIRA. There are different views on the extent of this type of improvement. Former OIRA of\ufb01cials cited this as the chief accomplishment of OIRA review. One said that\n\u201cWhere there is an improvement is in statistical policy[,] where OIRA can make sure that agencies' survey designs make sense[,]\n\nand they aren't doing phone surveys getting a 10% response rate. There is bene\ufb01t in technical expertise at OMB rather than having someone in the agency who only took one statistics course design the survey.\u201d\nAnother said \u201cThe methods [of surveys when they arrive in OIRA] are not often well thought out and impose burdens on the economy that are huge.\u201d\nOften these surveys are used to examine the need for new regulations. They also include major statistical collections including the Census, the Current Population Survey, and major public health surveys such as the National Health and Nutrition Examination Survey (NHANES). In other words, these are information collections that have major policy implications. Improving the quality of these collections and their usefulness to the government has the potential to have signi\ufb01cant bene\ufb01ts.\nHow many collections are improved by OIRA review? From reginfo.gov, new information collections are changed during OIRA review roughly half of the time. Some of these changes are likely trivial or merely changes in the burden estimates, changes that have a negligible social bene\ufb01t. If there are 800 new collections per year, 400 are changed. If we assume that between a quarter and a half of those where a change occurs during review receive meaningful changes, between 100 and 200 collections each year are improved via OMB review. In addition, a small number of existing collections are probably improved (92% of existing information collections are approved by OMB with no changes). So a reasonable estimate is that between 150 and 250 information collections each year are improved.\n4.2. Public participation\nWhen the PRA was passed in 1980, public participation was viewed as a key to its success. The hope was that the public would both improve information collections by commenting on them, and would call attention to collections that agencies were conducting in violation of the Act. Instead the public has commented on a limited number of collections as shown in Table 3. The public is signi\ufb01cantly more likely to comment on new collections or revisions than on extensions. Even for new and revised collections, the percentage that receive public comments is fewer than 15%.\nStill, 15% is not a trivial amount if the comments are helpful and lead to meaningful changes in agency collections. We looked at a small sample of collections from the agencies that received the most public comments to determine if the comments were substantive.\nWhile the number of collections that received a comment was small, more than 50% of the comments that those collections received were substantive. Several interview subjects from agencies claimed that about 50% of comments led to changes (although this includes changes in the estimate of the burden imposed by the collection) (Table 4).\nCompared with the regulatory process, the public comments far less frequently (8.7% here, versus one study's estimate of 63% for regulations Shapiro, 2007) but the proportion of comments that resulted in changes in regulations was similar to the off-the-cuff estimate\n\nTable 3 Comments by type of information collection.\n\nNew collectiona Revisionsb Extensionsc\n\nReceived comments Total Percentage of collections that\nreceived comments\n\n164 1201\n13.70%\n\n379 2740\n13.80%\n\n291 5197\n5.60%\n\na Includes existing collections without an OMB # submitted for the \ufb01rst time. b Includes reinstatements of previously approved collections with changes. c Includes extensions with no change or no material change and reinstatements of\npreviously approved collections without changes.\n\n208\n\nS. Shapiro / Government Information Quarterly 30 (2013) 204\u2013210\n\nTable 4 Public comments on agency information collections.\n\nAgency\nAgriculture Commerce Education FDIC FERC Interior Labor State\n\n% of collections that received comments\n25 14 17 15 22 14 13 33\n\n# sampled\n11 8 6 3 3 5 5 5\n\n# substantive\n6 5 5 3 3 3 3 3\n\ngiven by the agency of\ufb01cial quoted above for information collections (\u201c50\u201350\u201d here, whereas in the regulatory context, 45% of rules on which agencies received comments underwent more than trivial changes Shapiro, 2007).\nThose who have examined the public comment process in the regulatory context, have argued that comments play several roles. These include substantive (leading to changes in agency policy), political (serving as \u201c\ufb01re alarms\u201d alerting political overseers of agencies to problems with agency policy), and symbolic (West, 2004). While the number of information collections affected by participation is likely smaller than envisioned by the PRA's authors, participation is playing a substantive and symbolic role (and possibly a political one) and therefore does lead to bene\ufb01ts at least for a subsection of information collections, particularly new collections or ones that agencies change signi\ufb01cantly.\n5. The costs of the Paperwork Reduction Act\nThe costs of the PRA include both direct and indirect costs. The direct costs are largely measurable as they include salaries of government employees charged with implementing the PRA and costs of printing the notices required by the PRA. The indirect costs are much harder to measure because they re\ufb02ect decisions not made because of the PRA. One enters the realm of the hypothetical in attempting to \ufb01gure out how many collections agencies would have conducted if the PRA were not present, and whether the collections would have net social bene\ufb01ts.\nThe largest component of the direct costs is the cost of the employees who are charged with implementing the Act. These employees include civil servants at OIRA and at the agencies. At the agencies, there are department or agency wide of\ufb01cials who coordinate the information collection approval process and interact with OIRA and for each collection (particularly new ones) there is at least one individual with substantive knowledge of the collection who must prepare the approval package.\nIn the report for ACUS, I present a range of estimates for the number of employees and their GS levels. I \ufb01nd that the range of annual employee costs attributable to the PRA is $5.3 million to $8.3 million. In addition I argued that the government spends approximately $1.7 million on printing public notices in the Federal Register leading to an estimate of the total annual direct costs of between $7 million and $10 million.\nThe indirect costs of the PRA are potentially much larger than the direct costs. There are two major categories of indirect costs. The \ufb01rst is the delay of socially bene\ufb01cial collections (information collections that will lead to government decisions that will improve social welfare). Of course not all collections that are delayed lead to net social bene\ufb01ts. However, given that OMB Reports to Congress have repeatedly shown that regulations have large net social bene\ufb01ts, it is reasonable to assume that information collections that may lead to the promulgation of such regulations will on average have net bene\ufb01ts.\n\nTherefore delay of such information collections has costs. The magnitude of these costs has a wide band of uncertainty however.\nThe best that can be done is to assess the delay imposed by the OIRA-review requirement and note that multiplying that delay by the total net bene\ufb01ts per year of the collections that were delayed would give a value for the cost of delay. The formal delay imposed by the PRA process is simple to calculate. An agency has to wait 60 days for the \ufb01rst public comment period, and then another 60 days for OMB review (which includes the second 30 day comment period).6 This would lead to a delay of four months or one third of a year.\nBut interview subjects made clear that this is a minimum estimate for the delay and that it usually takes longer \u2014 sometimes much longer. As noted above, preparing the OMB submission, including the supporting statement, takes time. The Federal Register does not publish notices for comment immediately upon receipt. If an agency receives comments during the 60 day comment period, it must decide how to respond to the comments (if they are substantive). Finally, questions from OIRA may extend the review period beyond sixty days while agencies negotiate a resolution with OIRA.\nAll of these delays lead to an estimate that was relatively consistent from agency personnel. The time that an information collection is developed until it is \ufb01nally approved is generally between six and nine months. As previously noted, though, this delay is impossible to quantify or monetize. If the 780 new collections per year lead to zero net social bene\ufb01ts, then obviously the delay has no costs. If they lead to $1 million in annual net bene\ufb01ts, then the delay costs between $500,000 and $750,000. If they lead to $1 billion in annual net bene\ufb01ts, then the delay costs between $500 million and $750 million.\nThe second category of indirect costs is the collections that agencies choose not to pursue or change. In order to avoid the information collection clearance process, agencies may decide not to collect information which could lead to poorer decisions by government agencies (In another ACUS report,7 Copeland argues that this often occurs with collections designed to produce better regulatory impact analyses). Agencies also modify their information collections, most frequently by reducing the number of respondents below the statutory threshold of ten, in order to avoid the PRA clearance process. This likely leads to inferior information collected by the government.\nIf a collection that is unlikely to produce social bene\ufb01ts is abandoned by a federal agency, then the abandonment has no costs (and may have bene\ufb01ts). However, if on balance, such abandoned and altered collections have net bene\ufb01ts then not pursuing them has costs. Social costs also arise from poorer information quality that results from attempts by agencies to modify their information collections to avoid the requirements of the PRA. The number of collections changed or abandoned is impossible to quantify but their existence adds to the overall costs of the PRA.\n6. Arguing for a more ef\ufb01cient PRA\nTwo themes emerge from an analysis of the bene\ufb01ts and costs of the PRA. The \ufb01rst is that while bene\ufb01ts do emerge from OMB review of information collections, a tremendous share of the total effort related to the PRA is spent on reviews that provide no bene\ufb01ts. The second is that one of the primary bene\ufb01ts envisioned by the authors of the Act, the integration of information collection review with information resource management has not occurred. The potential bene\ufb01ts of a focus on IRM are signi\ufb01cant and should be a focus of any reforms.\n6 OMB can complete the review in fewer than 60 days but must wait at least 30 until the \ufb01rst comment period is completed. For purposes of simplicity, I assume here that OMB will use the full 60 days.\n7 See http://www.acus.gov/wp-content/plugins/download-monitor/download.php? id = 586 (last viewed June 27, 2012) for Copeland's report on reforming regulatory analysis requirements.\n\nS. Shapiro / Government Information Quarterly 30 (2013) 204\u2013210\n\n209\n\n6.1. Inef\ufb01ciencies in the PRA\nThe bene\ufb01ts of OIRA review accrue to a relatively small number of information collections. The vast majority of collections that are submitted to OMB as renewals of existing approvals without changes are approved by OMB without any change during the review. OIRA devotes the most attention to those collections that are new collections or collections that raise methodological issues. OMB modi\ufb01es new collections with a frequency ten times as great as renewals of existing collections. Often times these are also collections that agencies may use to examine the need for a new regulation.\nThe pattern is similar for public participation in the information collection review process. While nearly 15% of new information collections elicit public comment a signi\ufb01cantly smaller percentage of renewals do so. It could be that agencies are not doing enough to solicit participation but in the case of renewals, where no change is made by the agency, it is more likely that the lack of comment is a rational public response to something that is not worth commenting on.\nSince it is likely that there is considerable overlap between the collections that bene\ufb01t from OIRA review and the collections that solicit public participation, the information collection review process should be streamlined to focus on these collections. There are several ways that this could be accomplished. OMB could be given authority to approve collections unlikely to bene\ufb01t from review for a longer period of time than the currently statutorily mandated three years. A GAO report (2005) estimated that for each year approval that was extended, the volume of information collections reviewed would decrease by 20%. These same collections could be subject to one comment period rather than two (or the second comment period could apply to only those collections in this category that receive comments in the \ufb01rst period). OIRA could delegate review of less critical collections to the agencies under Section 3507(i) of the PRA.\nEach of these changes would reduce the burden of information collection review on OIRA and on agencies. Doing so would allow both OIRA and agencies to spend more time on those collections that have the biggest impact on the public.8 Review of collections with a signi\ufb01cant public impact could become more stringent with more OIRA attention. Agencies could experiment with other methods of reaching out to the public through social media or other means to solicit input on these collections that should generate the most public interest and receive the greatest bene\ufb01t from public input. In short, these changes could increase the bene\ufb01ts of the PRA while simply reallocating the costs from the collections that few care about to the ones that many care about.\n6.2. Information resource management\nAs described above, information resource management was central to the construction of the PRA. The legislative history is replete with aspirational comments about the potential bene\ufb01ts of IRM (Relyea, 2000). At the ACUS committee meetings on the PRA, numerous comments were made about how it was dif\ufb01cult to evaluate the information collection review process outside of the process of information resource management.\nThe 1986 amendments de\ufb01ned IRM as:\nthe planning, budgeting, organizing, directing, training, promoting, controlling, and management activities associated with the burden collection, creation, use, and dissemination of information\n8 These would likely include all high burden collections as well as ones with methodological issues or regulatory implications. There have been numerous calls for OIRA to focus more on IRS collections which make up the bulk of the burden on the public. Reducing the time OIRA spends on 80% of collections with low burden and no changes, would allow OIRA to pay more attention to IRS as well as to other signi\ufb01cant information collections.\n\nby agencies and included the management of information and related resources such as automatic data processing equipment.\u201d9\nThe failure of the executive branch to focus on IRM was clear when the PRA was reauthorized in 1995:\n\u201cbillions of dollars lost due to faulty bene\ufb01t payments systems, unwitting or unauthorized release of sensitive personal and law enforcement information, inadequate systems to provide basic \ufb01nancial data on program operations and more.\u201d10\nThe 1995 PRA Amendments linked IRM to program performance and gave agencies more responsibilities. Agencies have the job of applying IRM principles to management.\nThe information collection review process was intended to be embedded in a broader information resource management system. Over time, to the extent that IRM has been implemented, it has been implemented separately from information collection review. In part this is due to the passage of subsequent statutes such as the Clinger Cohen Act, 11 and the E-Government Act.12 In part it is due to administrative decisions at OMB that separated the two functions (information collection review and information resource management) within OIRA.13 And \ufb01nally in part it is due to political circumstances as different constituencies are focused on information management and burden reduction.\nWhile a reintegration of information collection review into information resource management would not be costless (although the costs would mainly be administrative) doing so would potentially increase the bene\ufb01ts of the information collection review process. Coming up with a plan for such a reintegration is beyond the scope of this paper. Before resources are available to focus on IRM, the current information collection review process must be streamlined as suggested above. But, the payoff of doing so would be that issues such as information quality, privacy, information storage and information disposal could be planned before the information is collected and agencies would be more accountable for doing so. These bene\ufb01ts are hard to quantify but the attention given to them by the crafters of the PRA indicate that there is a strong belief that they are signi\ufb01cant.\n7. Conclusion\nEconomists have long argued for greater use of bene\ufb01t\u2013cost analysis in setting government policy (Hahn & Litan, 2005). This argument should not be limited to government policies that directly impact the private sector. Laws and regulations that govern the administration of public decision-making could also pro\ufb01t from an analysis of bene\ufb01ts and costs. In this paper, I have discussed the bene\ufb01ts and costs of the Paperwork Reduction Act and found that there is signi\ufb01cant room for improvement in the administration of the PRA.\nThe Paperwork Reduction Act is a frequent target of criticism. From agencies and supporters of government policy, the PRA is accused of sti\ufb02ing information collections that could lead to large social bene\ufb01ts.14 From opponents of government action, the PRA is considered a failure that the executive branch has poorly implemented\n9 PL 99\u2013351. 10 CRS report RL30590 January 4, 2007 \u201cPaperwork Reduction Act Reauthorization and Government Information Management Issues\u201d by Harold Relyea. 11 110 Stat 680. 12 116 Stat 2899. 13 Within OIRA there is an information policy branch that is separate from the \u201cdesk of\ufb01cers\u201d who review information collections. In addition there is an entirely separate of\ufb01ce from OIRA, the \u201cE-Government\u201d of\ufb01ce that deals with many information technology issues. Several of my interview subjects blamed this fragmentation for diminished attention to IRM. Interview subjects also attributed this diminished attention to the fact that small businesses supported information collection review and there was no signi\ufb01cant constituency for IRM. 14 See for example, comment 2009-0020-0007 at regulations.gov.\n\n210\n\nS. Shapiro / Government Information Quarterly 30 (2013) 204\u2013210\n\nleading to a vast increase in the burden on the public, the very problem the statute was designed to solve.15\nA careful reading of the bene\ufb01ts and the costs of the Act reveals that both sides have merit. Agencies and OMB spend far too much time on following a burdensome procedural requirement for information collections that are well-established and unlikely to change even with careful review. In an age when resources are scarce, using tax dollars to print notices about such collections and to pay government employees to manage the review is a clear waste. Even if little else is done, streamlining the information collection review process for at least 80% of the information collections would save tax dollars, and have little impact on the public.\nBut the resources saved here could be put to bene\ufb01cial use. Information collection review by OMB adds value and better ensures that the information collected is useful and meaningful. This is particularly true for new information collections, collections intended to justify new regulations, and collections that employ statistical methodology. Increasing public participation on the development of these collections and increasing the scrutiny of these collections are likely to have net social bene\ufb01ts. Better integrating new collections into a broader information resource management system is also likely to have net social bene\ufb01ts.\nTaking these relatively simple steps has the potential to change the PRA from a statute that is frequently mocked (critics often argue that the PRA increases paperwork16) to one that better ensures that the information that the government collects is useful and justi\ufb01es the burden imposed on the public.\nFinally, regulatory reform is a constant priority of legislators, particularly during the current challenging economic times. We should examine changes to government processes with the same scrutiny that we apply to government regulations. Analyzing the bene\ufb01ts and costs of such processes, like this article has done for the Paperwork Reduction Act holds the promise of ensuring that government works better.\nReferences\nBelzer, R. (2012). Comments submitted to the Administrative Conference of the United States. available at: http://www.acus.gov/wp-content/plugins/download-monitor/ download.php?id=531 (last viewed June 21, 2012)\nCoglianese, C. (1997). Assessing consensus: The promise and performance of regulatory negotiation. Duke Law Journal, 46, 1255\u20131349.\n\nCopeland, C., & Burrows, V. K. (2009). Paperwork Reduction Act: OMB and agency responsibilities and burden estimates. (Congressional Research Service, R40636 2009). Washington D.C.: Government Printing Of\ufb01ce.\nFunk, W. (1987). The Paperwork Reduction Act: Paperwork reduction meets administrative law. Harvard Journal on Legislation, 24, 1\u201385.\nGolden, M. M. (1998). Interest groups in the rule-making process: Who participates? Whose voices get heard? Journal Of Public Administration Research And Theory, 8(2), 245\u2013270.\nGovernment Accountability Of\ufb01ce (2002). Paperwork Reduction Act. Burden increases and violations persist. GAO Report 02-598T. Washington D.C.: Government Printing Of\ufb01ce.\nGovernment Accountability Of\ufb01ce (2005). Paperwork Reduction Act: A new approach may be needed to reduce government burden on the public. GAO report 05\u2013424. Washington D.C.: Government Printing Of\ufb01ce.\nHahn, R., & Litan, R. (2005). Counting regulatory bene\ufb01ts and costs: Lessons from the US and Europe. Journal of International Economic Law, 8(2), 473\u2013508.\nLubbers, J. (1996). Paperwork redux: The (Stronger) Paperwork Reduction Act Of 1995. Administrative Law Review, 49, 111\u2013120.\nOf\ufb01ce of Management and Budget: Annual Information Collection Budget of the United States 1997\u20132011. (found at). http://www.whitehouse.gov/omb/inforeg_infocoll#icbusg (last viewed June 27, 2012)\nPlocher, D. (1996). The Paperwork Reduction Act of 1995: A second chance for information resources management. Government Information Quality, 13(1), 35\u201350.\nPortney, P. (1984). The bene\ufb01ts and costs of regulatory analysis. In V. K. Smith (Ed.), Environmental policy under Reagan's executive order. Chapel Hill: University Of North Carolina Press.\nRelyea, H. (2000). Paperwork Reduction Act reauthorization and government information management issues. Government Information Quarterly, 17(4), 367\u2013393.\nRelyea, H. (2007). Paperwork Reduction Act reauthorization and government information management issues. Congressional Research Service Report: RL30590. Washington D.C.: Government Printing Of\ufb01ce.\nRubin, H. J., & Rubin, I. S. (2005). Qualitative interviewing: The art of hearing data. Thousand Oaks CA: Sage Publication.\nShapiro, S. (2007). Presidents and process: A comparison of the regulatory process under the Clinton and Bush (43) administrations. Journal of Law and Politics, 32, 393\u2013418.\nShapiro, S. (2008). Evaluating the bene\ufb01ts and costs of regulatory reforms: What questions need to be asked? Evaluation and Program Planning, 31(3), 223\u2013230.\nTozzi, J. (2011). OIRA's formative years: The historical record of centralized regulatory review preceding OIRA's founding. Administrative Law Review, 63(SE), 37\u201371.\nWest, W. (2004). Formal procedures, informal processes, accountability, and responsiveness in bureaucratic policy making: An institutional policy analysis. Public Administration Review, 64(1), 66\u201380.\nYackee, S. W. (2006). Sweet-talking the fourth branch: Assessing the in\ufb02uence of interest group comments on federal agency rulemaking. Journal of Public Administration Research and Theory, 16(1), 103\u2013124.\nStuart Shapiro is an Associate Professor and Director of the Public Policy Program at the Bloustein School at Rutgers University. Prior to coming to Rutgers, he worked at the Of\ufb01ce of Information and Regulatory Affairs as a policy analyst and manager for \ufb01ve years. His research focuses on the regulatory process and the role of factors such as public participation and cost\u2013bene\ufb01t analysis in regulatory decision-making. He recently served as the research consultant on the Paperwork Reduction Act for the Administrative Conference of the United States.\n\n15 See for example, comment 2009-0020-0082 at regulations.gov. 16 See for example: http://www.hhs.gov/ocio/policy/collection/infocollectfaq.html#6 (last viewed January 14, 2012) where HHS states: \u201cQ. Doesn't the PRA cause more paperwork, not less? A. For government employees, yes. The objective of the law is to reduce the paperwork burden on the public. The process of trying to do that adds to government paperwork.\u201d\n\n",
        "hash_id": "1c08963a78a9fb46de287ff339c71aca"
    },
    {
        "key": "86M9HQNX",
        "version": 17,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/86M9HQNX",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/86M9HQNX",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/CMD3RCGU",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 201988
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Shapiro",
            "parsedDate": "2013-04-01",
            "numChildren": 1
        },
        "citation": "<span>Stuart Shapiro, <i>The Paperwork Reduction Act: Benefits, costs and directions for reform</i>, 30 <span style=\"font-variant:small-caps;\">Government Information Quarterly</span> 204\u2013210 (2013), https://www.sciencedirect.com/science/article/pii/S0740624X13000087 (last visited Nov 14, 2022).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "885YWXMU",
        "version": 16,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/885YWXMU",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/885YWXMU",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/SZ8VI8G9",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/885YWXMU/file/view",
                "title": "Shapiro - 2019 - The Case for Reinvigorating the Paperwork Reductio.pdf",
                "length": 227913
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID3351953_code396314.pdf?abstractid=3351953&#38;mirid=1 (last visited Nov 14, 2022).</span>",
        "fulltext": "The Case for Reinvigorating the Paperwork Reduction Act Stuart Shapiro Associate Dean of Faculty Bloustein School of Planning and Public Policy Abstract The Paperwork Reduction Act (PRA) has not been modified since 1995. In that time, the burden of providing information to the government as required by law, and government-mandated recordkeeping, has continually increased. Paperwork requirements can have deleterious impacts on businesses, universities, and individuals trying to secure government benefits to which they are legally entitled. Although the PRA seeks to reduce these burdens, it itself sets up a bureaucratically cumbersome process that likely deters useful small-scale information collections. This article makes the case for reinvigorating the PRA. A reboot of the PRA should contain three elements: a mechanism for better understanding the impact of paperwork, including the cumulative burden; a revised system that allows greater scrutiny of more burdensome and important information collection requirements and less time spent on more routine ones; and additional flexibility for agencies seeking to comply with the PRA and for the Office of Information and Regulatory Affairs seeking to enforce it.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nIntroduction Congress passed the Paperwork Reduction Act (PRA) in 1980.1 While the PRA was revised twice in subsequent years, it has not been amended since 1995. Meanwhile, the world has changed considerably. Most obviously, much of what used to be on paper is now online (though in the case of government information collections, there are laggards).2 Further, the number of areas in which collections are required has increased. Pre-1995 collections like the Census and IRS 1040 still exist; to those, the government has added mandatory collections from the Department of Homeland Security (created in 2002) and the agencies charged with implementing Obamacare (2010), among many others. Also since 1995, scholars have taken more seriously the cumulative burden of regulation.3 We better understand how information collections can have a particularly pernicious and regressive effect in deterring people from applying for benefits to which they are entitled.4 And while privacy has always been a public concern, it has taken on new dimensions in an era of Facebook, Cambridge Analytica, and Google.\nOn the surface, the PRA appears to be a failure. After all, it is called the Paperwork Reduction Act -- but the number of hours spent by each citizen every year submitting information to the government or retaining records required by the government has increased since 1997 from 25.6\n1 Paperwork Reduction Act, Pub. L. No. 96\u2010511, 94 Stat. 2812 (1980) (codified at 44 U.S.C. \u00a7\u00a7 3501\u201321). 2 See for example https://www.fcc.gov/licensing\u2010databases/forms (last viewed July 23, 2018) which lists many forms which are available on line and numerous ones that are not. 3 Mandel, Michael, and Carew, Diana G. \"Regulatory Improvement Commission: A Politically\u2010Viable Approach to US Regulatory Reform.\" Progressive Policy Institute, May 2013. 4 Infra notes 72\u201075.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nhours5 to 30.7 hours.6 Both the Government Accountability Office (GAO) 7 and the Administrative Conference of the United States (ACUS)8 have criticized the PRA\u2019s information management provisions, and agencies hoping to collect information have long complained about the PRA\u2019s burdensome requirements and argued that it deters them from collecting useful information.9\nThe truth, of course, is more complicated. Perhaps burdens would have been higher if there had been no PRA. The Office of Information and Regulatory Affairs (OIRA), the agency charged with enforcing the PRA, has doubtlessly improved hundreds of information collections through its review process. OIRA has also strived to make the review process less burdensome where possible, within the confines of the statute.10\nBut few people who have dealt with the PRA would argue that it could not be improved. In this article, drawing on research I have done as both as a consultant for ACUS, and other work on the reaction of businesses to paperwork, as well as on research by others on the impact of paperwork in other policy areas, I discuss the need for rebooting the PRA and examining how the statute could be changed to become more effective. There are opportunities to make the PRA work better. By that I mean there are ways to ensure that it both reduces burden on the public without creating meaningless hoops for agencies to jump through. The act should be re-envisioned as\n5 \u201cFinal Paperwork Reduction Act Report.\u201d Stuart Shapiro for the Administrative Conference of the United States, https://www.acus.gov/report/final\u2010draft\u2010paperwork\u2010reduction\u2010act\u2010report 6 \u201cInformation Collection Budget of the United States\u201d Office of Management and Budget 2016. https://obamawhitehouse.archives.gov/sites/default/files/omb/inforeg/icb/icb_2016.pdf (last viewed July 24, 2018). 7 See e.g. \u201cInformation Resources Management: Comprehensive Strategic Plan Needed to Address Mounting Challenges.\u201d GAO report 02\u2010292, February 2, 2002. 8 I am the author of this report for ACUS. Supra note 5. 9 Id. 10 \u201cPaperwork Reduction Act Efficiencies\u201d Stuart Shapiro for the Administrative Conference of the United States https://www.acus.gov/report/paperwork\u2010reduction\u2010act\u2010efficiencies\u2010final\u2010report\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nwell to deal with complicated issues of the cumulative burden of information collection on businesses, on schools, on state and local governments, and on individuals.\nThis article proceeds as follows. In the next section, I discuss the history of the Paperwork Reduction Act. Then I turn to the problems with the PRA. The section on the problems is divided into two parts. The first describes research by myself and others on the pernicious and growing effects of paperwork. The second describes the failures of the PRA itself to sufficiently curb these problems while creating a bureaucratic process for agencies that may deter useful work. I then discuss how the PRA could be reformed to better address the problems with paperwork while curbing needless burdens on the agencies. I recommend three guiding principles in PRA reform, better understanding paperwork burden, better focusing the PRA on important and burdensome information collections, and adding flexibility to OMB implementation of the PRA. Finally, I offer concluding thoughts.\nThe History of the Paperwork Reduction Act11\nThe first serious attempt to manage government information came with the Federal Reports Act (FRA) of 1942.12 The FRA was prompted by concerns from businesses, and a large increase in paperwork from the World War II agencies, the Office of Price Administration, and the War Production Board.13 Implemented by the Bureau of Budget (BOB) (the predecessor agency to the Office of Management and Budget (OMB)), the FRA required agencies to submit collections of information for approval to the BOB. Much of the Department of the Treasury was exempted\n11 Portions of this section are copied from my report for the Administrative Conference of the United States Surpa note 5. 1256 Stat 1078 13 William Funk \"The Paperwork Reduction Act: Paperwork Reduction Meets Administrative Law\" Harvard Journal on Legislation 1987 24:7\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nfrom the requirements of the FRA.14 Many skeptics such as Senator Arthur Vandenberg (R-MI) thought the bill did not, \"remotely touch the magnitude of the problem.\"15\nAgencies complained about BOB interference in their statutory missions and the length of time required to secure BOB approval for information collections. In 1973, Congress exempted independent regulatory agencies from the requirements of the FRA.16 The Office of Management and Budget was also criticized by the Senate Select Committee on Small Business for an indifference \u201ctowards their basic responsibilities. Since only a relative handful (between one and five percent) of forms were disapproved the committee could only conclude that hundreds of duplicative forms were being imposed on the public.\u201d17 OMB lacked initiative in pursuing the goals of the FRA.18 The FRA contained numerous other provisions but the clearance process received the most attention and the most criticism.19 OMB thus was criticized both for being too intrusive with federal agencies, and not sufficiently curbing agency information collection20 (criticisms that, as is detailed below, are echoed in today\u2019s debates about the PRA).\nAs a result of these criticisms, Congress created a \u201cCommission on Federal Paperwork\" in 1974. The Commission completed its work in 1977 and argued that the FRA was flawed. The flaws\n14. For a history of the operation of the FRA, see Funk, Id. 15Supra Note 13. 16 Public Law 93\u2010153. 17Harold Relyea \u201cPaperwork Reduction Act Reauthorization and Government Information Management Issues\u201d Government Information Quarterly 2000 17:367. 18 S REP No. 125 93rd Congress 1st Session 25 (1973) at 34. 19 Supra Note 13 p. 13 20 Id.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nincluded the exemption of Internal Revenue Service (IRS), insufficient funding for FRA supervision, and a placement in the decision-making process that was too late to make a difference in the content of information collections.21 The Commission also roundly criticized unnecessary paperwork defined as duplicative, excessive, costly contradictory, intimidating, or confusing.22 After GAO reported that the commission's recommendations were being carried out too slowly, legislators began work on the Paperwork Reduction Act (PRA).\nAn earlier version of the PRA was introduced in the House as H.R. 3570, the Paperwork and Red Tape Reduction Act of 1979, accompanied by the companion bill, S. 1411, in the Senate.23 While the business community and state and local governments most ardently supported the effort, it enjoyed bipartisan support.24 Non-partisan groups such as the Citizens Committee on Paperwork Reduction sprang up to support the act as well.25\nFederal agencies were concerned that the PRA would curb their authority and allow the Office of Management and Budget (OMB) to intrude upon their policy-making authority. Senator Lawton Chiles (D-FL), who was a key sponsor of the legislation that eventually became the PRA, assured critics that while transparency and accountability for agency information collection was\n21 Commission of Federal Paperwork, Final Summary Report (October 3, 1977). 22 Id at p. 12.\n23 The House bill was sponsored by Rep. Frank Horton (R-NY) and Rep. Jack Brooks (D-TX), while the Senate bill was sponsored by Sen. Lawton Chiles (D-FL), Sen. Lloyd Bentsen (D-TX), and Sen. John Danforth (R-MO). One year later, Horton\u2014who previously acted as the chairman of the Commission on Federal Paperwork\u2014and Brooks reintroduced their bill as H.R. 6410 as a new companion to the Senate bill. 126 CONG. REC. S14,687 (daily ed. Nov. 19, 1980). 24 Shapiro, Stuart, and Deanna Moran. \"The Checkered History of Regulatory Reform since the APA.\" NYUJ Legis. & Pub. Pol'y 19 (2016): 141.\n25 Id.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\na clear goal of the act,26 nothing in the act affected the balance of authority between the agencies and OMB.27\nThe bill passed overwhelmingly in the House and unanimously in the Senate and signed into law by President Carter on December 11, 1980.28 While the bill contained provisions on information management, government dissemination of information,29 and maximizing the usefulness of information provided to the government, President Carter\u2019s statement upon signing the legislation made no mistake about the primary purpose of the Act.\nThis legislation, which is known as the Paperwork Reduction Act of 1980, is the latest and one of the most important steps that we have taken to eliminate wasteful and unnecessary Federal paperwork and also to eliminate unnecessary Federal regulations. . . . This legislation is another important step in our efforts to trim waste from the Federal Government and to see to it that the Government operates more efficiently for all our citizens.30\nCongress has amended the PRA twice since being enacted. The first time was in 1986. The 1986 amendments occurred amidst much controversy about the role of the Office of Information and Regulatory Affairs (OIRA), the office within OMB charged with implementing the PRA.\n26 See, e.g., Paperwork and Redtape Reduction Act of 1979: Hearing on S. 1411 Before the Subcomm. on Federal Spending Practices and Open Government of the Comm. on Governmental Affairs, 96th Cong. 3\u20136 (1980) (statement of Sen. Lawton Chiles) 27 Comm. on Gov\u2019t Affairs, Report on Paperwork Reduction Act of 1980, S.1411 Rep. No. 4. (1980) page 47 28 See https://en.wikipedia.org/wiki/Paperwork_Reduction_Act (last viewed April 20, 2018). 29 The dissemination of information was further emphasized in the Data Access Act and the Information Quality Act (statutory cites) 30 Remarks on Signing H.R. 6410 into Law, 3 PUB. PAPERS 2795 (Dec. 11, 1980).\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nPresident Reagan had given OIRA authority over regulatory review in Executive Order 12291.31 This role of OIRA thrust it into the national spotlight and into the center of political controversy.32 The 1986 amendments made the OIRA Administrator subject to confirmation by the Senate (a key demand of opponents of OIRA\u2019s regulatory review function), emphasized information resource management (IRM) as a goal of the act, and set paperwork reduction goals.33\nThe 1995 Amendments took place amidst the flurry of regulatory reform efforts engaged in by the 104th Congress (which included passage of the Unfunded Mandates Reform Act34 and the Small Business Regulatory Enforcement Flexibility Act35). According to Jeffrey Lubbers, the 1995 amendments to the PRA are better described as an \u201centire recodification\u201d of the Act.36 They increased the scope of OIRA's oversight to include dissemination of information, maintenance of archives, acquisition of information technology, and numerous other functions,37 while maintaining OIRA's authority over information collection and setting revised goals for paperwork reduction. They also required that each agency establish an office, independent from program responsibilities, to conduct information collection clearance activities.38\n31 Executive Order 12291 Federal Register 46 FR 13193. 32 Morrison, Alan B. \"OMB interference with agency rulemaking: the wrong way to write a regulation.\" Harvard Law Review 99, no. 5 (1986): 1059\u20101074.\n33Supra Note 17 34 Unfunded Mandates Reform Act, Pub. L. No. 104\u20104, 109 Stat. 48 (1995) (codified at 2 U.S.C. \u00a7\u00a7 1501\u201303). 35 Small Business Regulatory Enforcement Fairness Act, Pub. L. No. 104\u2010121, 110 Stat. 857 (1996) (codified at 5 U.S.C. \u00a7\u00a7 601, 631, 648(c)(3), 657). 36Jeffrey Lubbers, \u201cPaperwork Redux: The (Stronger) Paperwork Reduction Act of 1995\u201d Administrative Law Review 1996 49:111 p. 112 3744 U.S.C.A. 3504 38Supra Note 36.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nThe PRA has remained unchanged since 1995. The process for securing an approval for an information collection from OIRA is also largely unchanged. If an agency wants to collect information from ten or more people, it must undertake the following steps: 39\n\uf06e Develop the information collection and supporting documentation. The supporting information consists of responses to eighteen questions (or 23 questions in the case of information to be used for statistical purposes)40 about the burden of the collection and how the information will be used, maintained and disposed of.41\n\uf06e Publish a notice in the Federal Register and allow the public up to sixty days to submit comments to the agency on the information collection.42\n\uf06e Submit the collection and supporting material to OIRA including any responses to comments received.43\n\uf06e Simultaneous with submission to OIRA, publish a second Federal Register notice with a request to submit comments to OIRA within 30 days.44\nOIRA then has 60 days from submission (or 30 days from the close of the second comment period) to approve or disapprove the collection. They may approve the collection for up to three years at which point, if the agency wishes to continue to use the collection, the agency must again go through the above steps.\nUnder the Obama Administration, OIRA highlighted several techniques which agencies could\n39 These requirements are outlined in 5 CFR 1320 et. seq. 40 OIRA developed these questions. 41 See https://www.opm.gov/about\u2010us/open\u2010government/digital\u2010government\u2010strategy/fitara/paperwork\u2010 reduction\u2010act\u2010guide.pdf (last viewed May 1, 2018). 42 This is a statutory requirement supra Note 1. 43 Id. 44 Id.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nuse to expedite clearance of information collections under the PRA in limited circumstances. These include the use of generic clearances,45 and fast-track clearances (which are a subset of generic clearances).46 Generic and fast track clearances allow an agency to group collections of a specific type and go through the process outlined above once every three years for the group of collections. Agencies can then submit individual collections to OIRA with a pledge by OIRA to review them in a very short time (five days for collections submitted under fast-track clearances and a negotiated time for collections submitted under generic clearances). Generics and fasttrack clearances are largely used for collections to measure agency performance such as customer satisfaction surveys.47 OIRA has made clear that the PRA prohibits their use for collections with policy implications.\nWhat is Wrong with the Paperwork Reduction Act?\nThe argument for the need for reinvigoration of the PRA48 can be broken down into two subarguments. The first is the fact that the burden of information collections is still a significant one with deleterious consequences and is arguably growing worse with each passing year. The second is that the act itself has done little to curb this problem while creating a cumbersome process that treats nearly all information collections identically, and therefore diverts attention\n45 Sunstein, Cass. \u201cPaperwork Reduction Act \u2013 Generic Clearances.\u201d May 28, 2010. https://www.whitehouse.gov/sites/whitehouse.gov/files/omb/assets/inforeg/PRA_Gen_ICRs_5\u201028\u20102010.pdf (last viewed March 7, 2018). 46 Sunstein, Cass. \u201cNew Fast\u2010Track Process for Collecting Service Delivery Feedback Under the Paperwork Reduction Act.\u201d June 15, 2011. https://www.whitehouse.gov/sites/whitehouse.gov/files/omb/memoranda/2011/m11\u201026.pdflast viewed March 7, 2018). 47 Supra note 10. 48 Adam Samaha calls the PRA, \u201ca provocative failure,\u201d Samaha, Adam M. \"Death and paperwork reduction.\" Duke LJ 65 (2015): 279. At p. 283.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nfrom both collections that impose the greatest amount of burden and issues such as the cumulative burden of government requirements. I discuss each of these in turn.\nThe Problem with Paperwork\nCongress passed the PRA to deal with a very real problem. The hearings preceding the passage of the Act were filled with testimony about the burden of paperwork.49 While the PRA is also intended to ensure that the information collected by the government produces practical utility50 and to improve the management and dissemination of information within the executive branch, the statute was not named the Paperwork Reduction Act by accident. Dating back to the Federal Reports Act, there has always been a perceived need to reduce paperwork burden.\nIs that perception accurate? Much of the impetus for paperwork reduction comes from the business community.51 As part of a broader project to study regulatory burden, Debra BorieHoltz and I conducted a survey and interviews of small business owners in the Midwest. We focused on the manufacturing sector because it is the locus of much of the rhetoric about regulatory burden. We surveyed 322 small businesses owners or high level managers about how they complied with regulations and how they formed their perceptions about regulation and government. After the survey was complete we conducted eight interviews, which included five site visits to probe in greater depth the relationship between these businesses and government\n49 Shapiro and Moran supra note 24 50 Supra Note 1 at 3502(11) 51 Shapiro and Moran supra note 24\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nregulation.52\nWhile we did not go into the survey intending to focus on the role of paperwork, it emerged as a major theme surrounding business compliance with regulation. It was both a major part of the actual burden experienced by small businesses and a significant force in shaping their perceptions of government.\nIn the survey, we asked business owners for an estimate of how many hours their firms spent filling out government paperwork (we did not differentiate between levels of government although we asked separately about which level of government was the greatest source of their compliance burden).53 Respondents reported that their employees spent an average of 2.8 hours per week filling out government forms. With 27.9 million small businesses (under 500 employees) in the United States,54 this translates to 4.1 billion hours per year for small businesses. 55\nWe also asked respondents whether particular types of regulation had a \u201csignificant impact on their business. The results are in Table 1.\n52 This work is described in more detail in Stuart Shapiro and Debra Borie\u2010Holtz \u201cWhy (and When) do Small Businesses Hate Regulation\u201d on file with author. 53 In response to this question, 29% of respondents believed it was the cumulative effect of all levels of government, 26% said federal regulations were the largest burden, 18% said state level regulations, and 17% said they didn\u2019t know the difference between levels of government. Only 9% of respondents faulted local regulations as being a primary source of burden. 54 See https://www.sba.gov/sites/default/files/FAQ_Sept_2012.pdf (last viewed November 6, 2017). 55 Given that the sample size had a median number of employees of six and a mean of 12.2, this paperwork estimate is likely low as larger firms are likely to have greater paperwork burdens.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nTable 1 Do the following types of regulations have a significant impact on your business?\n\nLicensing Employee Worker Recordkeeping/Reporting Environment The\n\nBenefit Safety\n\noverall\n\nvolume of\n\nregulations\n\nYes 25%\n\n42%\n\n29% 43%\n\n32%\n\n47%\n\nNo 75%\n\n58%\n\n71% 57%\n\n68%\n\n53%\n\nAs the reader can see, aside from a concern about the overall volume of regulation (which may\n\ninclude concerns about the cumulative burden of paperwork requirements), small business\n\nowners see reporting and recordkeeping regulations as having the most significant impact on\n\ntheir businesses. They see the impact as greater than even traditional regulatory bogeymen,\n\nenvironmental and worker safety regulations.\n\nThe interviews furthered our understanding of the outsized role that paperwork plays in people\u2019s perceptions of regulatory burden. Respondents complained about overlapping reporting requirements from different levels of government. They were upset about regular reporting requirements (monthly or annual requirements), for which no one explained the purpose to them. The requirement to keep records for long periods of time were also seen as a particular burden. As one respondent put it, \u201cAll the government crap that is duplicate of each other. You take the same numbers fill it out in five different directions, city, county, state, federal, another federal agency.\u201d\n\nAs I walked into one interview, the owner of the business had prepared a visual aid for me. On one side of the desk was one binder. On the other was a pile of eight binders. Before I started the interview, he pointed to the one binder and said, \u201cthis is how we make {our product},\u201d then he pointed to the pile of binders and said, \u201cthese are the records we have to keep because of regulations.\u201d He later said, \u201cthere is lots of duplicative information. There are never ending\n\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nrequests for information and you never have any idea where it goes, who reads it, what are the results.\u201d\nThis was in contrast to sentiments about other regulatory requirements where there was a onetime obligation (buying particular equipment or changing a production process). Respondents admitted that these were difficult when they had to be implemented but quickly became part of their regular routines and hence were no longer seen as an obligation. Our interview subjects also understood the purpose of those one time requirements even if they didn\u2019t love having to implement them. For reporting and recordkeeping requirements, they seemed to feel they were spending their time doing things that had no purpose and would never be seen by anyone.\nPaperwork requirements thus seem to be a significant contributor to both the burden imposed on businesses by regulation56 and particularly to the anti-regulation animus that has spiked in recent years. While the literature on regulation is voluminous, very little of it deals with the role of paperwork. There is a strain of the public administration literature that discusses \u201cred tape\u201d that comes closest to grappling with the problem of paperwork.\nBozeman defines \u201cred tape\u201d as \u201crules, regulations, and procedures that remain in force and entail a compliance burden but do not advance the legitimate purposes the rules were intended to serve.\u201d57 He uses red tape to describe both the requirements for the internal management of government agencies, and the requirements that those agencies impose on external parties, usually in the form of paperwork. Bozeman identifies a vicious cycle, whereby distrust in government often leads to more layers of government, which in turn leads to more red tape and\n56 In addition to the perceptions of these small business owners, please see the discussion below on increasing burden infra notes 78\u201082. 57 Bozeman, Barry. Bureaucracy and red tape. Prentice Hall, 2000, p. 12.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nmore resentment.58 Moynihan et. al. discuss three components of administrative burden on program beneficiaries, learning costs, psychological costs, and compliance costs59 (typically it is only compliance burdens that are accounted for by the PRA).\nSimilarly Rosenfeld, studying Community Development Block Grants (CDBG), identifies red tape as, \u201cguidelines, procedures, forms, and government intervention that are perceived as excessive, unwieldy, or pointless in relationship to decision-making or implementation of decisions.\u201d60 Rosenfeld summarizes different theories as to the origins of red tape.61 These include Kaufman\u2019s argument that paperwork/red tape comes from the push and pull of wanting government to perform myriad functions but also not trusting government.62 Other theories cited by Rosenfeld are the role of federalism which leads to requirements from different levels of government, vague delegations to the bureaucracy by Congress, and risk aversion within the bureaucracy.63\nRosenfeld then examines these origins of red tape in an analysis of perceptions of the Community Development Block Grant program. He finds that the roles of Congress and the bureaucracy are particularly important in explaining the growth of red tape.64 His research also indicates that despite the best of intentions within the government, there is a tendency for\n58 Id. p. 87. 59 Moynihan, Donald, Pamela Herd, and Hope Harvey. \"Administrative burden: Learning, psychological, and compliance costs in citizen\u2010state interactions.\" Journal of Public Administration Research and Theory 25, no. 1 (2014): 43\u201069 60 Rosenfeld, Raymond A. \"An expansion and application of Kaufman's model of red tape: The case of community development block grants.\" Western Political Quarterly 37, no. 4 (1984): p. 603.\n61 Id. p. 604. 62 Kaufman, Herbert. Red tape: Its origins, uses, and abuses. Brookings Institution Press, 2015.\n63 Rosenfeld, supra note 59 pp. 604\u2010605 64 Id.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\npaperwork requirements to multiply and grow over time.65 Rosenfeld\u2019s work highlights the fact that paperwork is not just a problem for businesses trying to comply with regulation but also for those attempting to use government programs.\nExamples of paperwork as a problem for non-business entities are myriad. Administrative burden has been identified as a fundamental way that citizens experience their interaction with the state.66 The National Academy of Sciences issued a report in 201667 describing (among other things) the excessive recordkeeping and reporting requirements associated with the federal scientific grants process.68 These requirements include both the application process for grants itself, progress reporting requirements, and the numerous other mandates from agencies like the National Institute of Health, the National Science Foundation, and other grant-making agencies.69 The report indicates that the Paperwork Reduction Act has failed to curb these often duplicative burdensome requirements70 which in turn keep scientists from spending time on research.\nThe process of applying for financial aid for higher education presents another example of the problems of excessive paperwork having harmful impacts. The primary application for potential\n65 Id. 66Supra note 59. 67 The author was a member of the NAS panel that wrote the report. 68 National Academies of Sciences, Engineering, and Medicine. Optimizing the nation's investment in academic research: A new regulatory framework for the 21st century. National Academies Press, 2016.\n69 See also Bozeman, Barry, and Derrick M. Anderson. \"Public Policy and the Origins of Bureaucratic Red Tape: Implications of the Stanford Yacht Scandal.\" Administration & Society 48, no. 6 (2016): 736\u2010759 for a discussion of how universities have been incented to over comply with red tape requirements because of the embarrassment associated with one particular scandal.\n70 Id. p. 64.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\ncollege students is the Free Application for Federal Student Aid (FAFSA). The FAFSA has been widely criticized as excessively burdensome and complex.71 Furthermore, leading scholars on access to higher education have described the process of applying for aid as, \u201ca serious obstacle to both efficiency and equity in the distribution of student aid,\u201d and \u201cthat complexity disproportionately burdens those with the least ability to pay and undermines redistributive goals.\u201d72\nNumerous scholars have noted the regressive impact of administrative burden, particularly in social welfare programs. Brodkin and Majmundar coined the term, \u201cadministratively disadvantaged to describe those for whom red tape is a barrier to claiming benefits to which they are entitled.73 Moynihan and Herd note, \u201cBurdesome administrative rules can make citizens less trusting of the state and less confident of their own capacities as citizens.\u201d74 Behavioral factors which make it more likely that someone forgoes benefits because of paperwork burdens are particularly likely to afflict the disadvantaged.75\n71 Davidson, J. Cody. \"Improving the financial aid process for community college students: A literature review of FAFSA simplification, information, and verification.\" Community College Journal of Research and Practice 39, no. 5 (2015): 397\u2010408. 72 Dynarski, Susan M., and Judith E. Scott\u2010Clayton. \"The cost of complexity in federal student aid: lessons from optimal tax theory and behavioral economics.\" National Tax Journal 59, no. 2 (2006): p. 319. 73 Brodkin, Evelyn Z., and Malay Majmundar. \"Administrative exclusion: Organizations and the hidden costs of welfare claiming.\" Journal of Public Administration Research and Theory 20, no. 4 (2010): 827\u2010848. 74 Moynihan, Donald, and Pamela Herd. \"Red tape and democracy: How rules affect citizenship rights.\" The American Review of Public Administration 40, no. 6 (2010): 654\u2010670. P. 658 75 Sunstein, Cass R. \"Sludge and Ordeals.\" Duke Law Journal, Forthcoming (2018).\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nOne recent example of the regressive impact of paperwork is state requirements that Medicaid\napplicants demonstrate employment in order to secure eligibility for benefits. The Center for\nBudget and Policy Priorities argues that the burdens of completing these reporting requirements\ncould cost people (who are already the most likely individuals to have trouble finding time to fill\nout forms) eligibility for Medicaid even if they meet the standards by deterring them from even applying for benefits.76 Indeed, states themselves vary the paperwork requirements for Medicaid\nrecipients for a wide variety of reasons and those with already more burdensome requirements, have seen lower take-up rates from potential beneficiaries.77\nThis is not meant to be an exhaustive list of the problems that paperwork causes throughout the public policy universe.78 The examples of grant-making, student financial aid, and Medicaid\neligibility are meant to illustrate that paperwork is not just an issue that involves regulatory\ncompliance for businesses. Recent work in behavioral public administration has argued that red tape leads to negative emotional responses for anyone dealing with the state.79 Too often\npaperwork is framed as a problem for business which leads to those who support such regulation\nof industry into the default position of opposing paperwork reduction. This does not need to be\n76 https://www.cbpp.org/research/health/many\u2010working\u2010people\u2010could\u2010lose\u2010health\u2010coverage\u2010due\u2010to\u2010medicaid\u2010 work\u2010requirements (last viewed May 8, 2018). 77 Fox, Ashley, Wenhui Feng, and Edmund Stazyk. \u201cAdministrative Easing: The Diffusion of Red Tape Reduction in Medicaid Enrollment Procedures Across the States.\u201d Presented at George Washington University Behavioral Public Administration conference May 24, 2018, copy on file with author. See also Moynihan, Herd, and Harvey (supra Note 58) who show that reductions of burden in Wisconsin\u2019s Medicaid program increased enrollments. 78 See Burden, Barry C., David T. Canon, Kenneth R. Mayer and Donald P. Moynihan. 2012. \u2018\u2018The Effect of Administrative Burden on Bureaucratic Perception of Policies: Evidence from Election Administration.\u2019\u2019Public Administration Review72(5):741\u2014751 for a discussion of how local election officials who feel requirements are burdensome have negative sentiments toward the policies imposing those burdens. 79 Fabian Hattke, David Hensel, Janne Kalucza, Cordelia Muhlbach, and Judith Znanewitz. Emotional Responses to Bureaucratic Red Tape. Presented at Behavioral Public Administration Symposium George Washington University May 24, 2018. On file with author. See also Kaufmann, Wesley, and Lars Tummers. \"The negative effect of red tape on procedural satisfaction.\" Public Management Review 19, no. 9 (2017): 1311\u2010 1327.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nthe case as excessive paperwork can impose unnecessary and harmful burdens on all sectors of society.\n\nDoes the Paperwork Reduction Act Help?\n\nThe first test of whether the PRA is achieving its intended purpose is to examine how the burden of information collections has changed over the years. Since 1997, OIRA has submitted a statutorily required annual report to Congress entitled, The Information Collection Budget of the United States (ICB).80 In the report OIRA aggregates the total number of burden hours from the previous year. This information is collected in Table 2.\n\nTable 2: Annual Burden Hours Imposed by Information Collections81\n\nFiscal Year\n\nAnnual Burden Hours (in millions)\n\n1997\n\n6,970\n\n1998\n\n6,967\n\n1999\n\n7,183\n\n2000\n\n7,361\n\n2001\n\n7,651\n\n2002\n\n8,223\n\n2003\n\n8,099\n\n2004\n\n7,971\n\n2005\n\n8,240\n\n2006\n\n8,924\n\n2007\n\n9,642\n\n2008\n\n9,711\n\n2009\n\n9,795\n\n2010\n\n8,783\n\n2011\n\n9.14\n\n80 All of the Information Collection Budgets can be found at https://obamawhitehouse.archives.gov/omb/inforeg_infocoll#icbusg (last viewed July 24, 2018). 81 Id.\n\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\n2012\n\n9.43\n\n2013\n\n9.45\n\n2014\n\n9.43\n\n201582\n\n9.78\n\nAs the table shows, hours increased at an almost constant rate (with the exception of 2010)83 with a total increase of 40% from the beginning to the end of the period.84 While this is partly explained by a fifteen percent increase in the U.S. population over the period, the burden hours per capita have also increased, from 25.6 hours to 30.5 hours. Changes in the economy also result in burden changes (for example, during a recession, more people will fill out paperwork to apply for food stamps), but it is clear from the annual ICBs, they cannot on their own explain the change over the past decade and a half.\n\nOf course, declines in the paperwork burden, do not by themselves demonstrate that the PRA is\nnot working to reduce the burden of information collection. Burden could easily have grown at a\nfaster clip without the presence of the Paperwork Reduction Act. There is a significant reason to\nbe skeptical of the proposition that burden would have gone up substantially more without the\nPRA than it did with the PRA. The Information Collection Budgets regularly cite new statutes as\nthe primary cause of burden increases.85 Many of these statutes (e.g. the Affordable Care Act,\nthe Patriot Act) would not have been any different in the absence of the PRA because the PRA\napplies only to actions by agencies, not to actions by Congress. Meanwhile, the specific\n82 The Trump Administration has not yet issued an Information Collection Budget which makes 2015 the most recent year for which data is available. 83And the reduction in 2010 is largely due a change in the way that burden hours are computed by the Internal Revenue Service. 84 The relatively constant numbers from 2012\u20102014 are also due to adjustments downward in burden calculations for some collections offsetting increases due to new information collection requirements. 85 Supra note 79.\n\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\ninstances of paperwork burden having adverse policy consequences are numerous as outlined above.\nAnd, as described above, burden reduction is not the sole purpose of the PRA. The PRA also intended to improve the utility of the information received by the government and improve the management of this information with federal agencies. On this latter claim, information resource management has long been seen as neglected in the implementation of the PRA.86 In part, Congress dealt with this problem by passing other statutes such as the Clinger-Cohen Act,87 and the Government Paperwork Elimination Act.88 Despite these other statutes, advocates for a more efficient system of managing information within the government still feel as if this part of the PRA has been unduly neglected.89\nWhat about the role of the PRA in improving information collections? On this score, there is evidence that review by OIRA has served to improve information collections. As I wrote in my 2012 report to ACUS,90\nWhat is the nature of these improvements? They tend to come in two categories. The first is preventing the government from asking the public questions that are ineffective, overly intrusive, offensive, or otherwise inappropriate. . . The second way that OIRA\n86 David Plocher, \u201cThe Paperwork Reduction Act of 1995: A Second Chance for Information Resources Management\u201d Government Information Quality 13:35 1996 87 The Clinger Cohen act requires agencies to focus their information resource planning on supporting their strategic missions. 110 Stat 680. 88 GPEA requires agencies to make their information collection mechanisms available electronically. 112 Stat 2681. 89 Supra note 5. 90 Id.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nreview improves information collections is through the work of the statistical policy branch in OIRA.\nThe report also notes though that there is disagreement about these benefits and that in any case they are likely to be small compared to the increases in burden and the negative impacts of paperwork outlined above.91 The documented concerns about the complexity of information collections deterring individual from applying for benefits to which they are entitled, indicates that there are still major concerns about the practical utility of such collections.92\nStill, the potential of the PRA to stop intrusive collections should not be completely discounted. In 2017, lawsuits intended to stop the President\u2019s Advisory Commission on Election Integrity from collecting individual voter data from the states cited the PRA. The commission had asked states to turn over data on voters, and claimed that this information collection was exempt from the PRA. States refusing to turn over their information cited the PRA (as well as other statutes) as did advocacy groups that used the courts to overturn the commission\u2019s request.93 The President eventually disbanded the commission and abandoned the attempt to collect this information.94\nWhile the PRA has largely failed to create large burden reductions and has produced only some improvements in the practical utility of information given to the government, the process for approval of information collections has been widely criticized. Such criticisms include\n91 Id. 92 Supra notes 73\u201076. 93 See https://www.brennancenter.org/legal\u2010actions\u2010taken\u2010against\u2010trump%E2%80%99s\u2010 %E2%80%9Cfraud%E2%80%9D\u2010commission (last viewed May 11, 2018). 94 Michael Tackett and Michael Wines, \u201cTrump Disbands Commission on Voter Fraud\u201d New York Times January 3, 2018. https://www.nytimes.com/2018/01/03/us/politics/trump\u2010voter\u2010fraud\u2010commission.html (last viewed May 11, 2018).\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\narguments that the process is overly burdensome for federal agencies and that it deters agencies from collecting information that would be valuable.95\nIn the research for my 2012 report for the Administrative Conference of the United States (ACUS) on the Paperwork Reduction Act,96 I looked at both the aggregate data on information collection approvals and conducted interviews with 20 individuals inside and outside of government on their perceptions of the PRA approval process. In the report, I found that it took agencies from between six and nine months from the initial development of an information collection to finally securing OMB approval to field it. While this time frame was reasonable for collections imposing a significant burden on the public or intended to inform important policy decisions, it is excessive for many of the collections subject to the act.97\nMuch of the unnecessary time in the PRA process comes from two provisions in the PRA in particular. Agencies must review collections every three years.98 This is true even if there have been no changes to the information collection and there are no public complaints about it. The practical impact of this is that every three years an agency must go through the entire process for approval for every information collection it utilizes on a continuing basis. OMB must then dedicate resources to reviewing the renewal of approval.\nThe second provision in the PRA that creates unnecessary delay in the PRA approval process is the requirement for two comment periods. Regulations, most of which have much more profound impacts on the public generally only have one comment period. Furthermore, the public\n95 Supra note 5. 96 Id. 97 Id. 98 44 USC 3507(g)\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\ninfrequently uses the comment process to raise concerns about information collections.99 I found that 8.7% of information collections receive public comments and the number drops for collections that impose smaller numbers of burden hours.100\nOMB does have a way to reduce the burden of these provisions on agencies (and on itself). OMB may delegate authority for review of information collections, or subsets of those collections to individuals within agencies that are \u201csufficiently independent of program responsibility.\u201d101 OMB has only used this authority twice in 39 years (OMB has delegated review authority to the Federal Reserve Board, and the Managing Director of the Federal Communications Commission). Theoretically OMB could delegate review of some categories of information collections (renewals of existing low burden collections) that do not profit from OMB review or from two comment periods.102\nInstead the PRA creates an overly broad and bureaucratic process while largely failing to achieve the goal listed in its title. If paperwork was a meaningless issue, then this failure would be a matter that was solely about efficient government. But as discussed above, in areas ranging from business regulation to education policy to the distribution of benefits, paperwork has pernicious effects. It both undermines the purpose of government programs and alienates those who must submit data and maintain records from the mission of government. A Paperwork Reduction Act that meaningfully grapples with these problems could have tremendous benefits.\nHow to fix the Paperwork Reduction Act\n99 Supra note 5. 100 Id. 101 44 USC 3507(i)(1) 102 Supra note 5.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nLooking Abroad and Within\nConcern about paperwork is not exclusive to the United States. The OECD has analyzed paperwork (or \u201cred tape\u201d) reduction efforts among its members103 and found, \u201cpaperwork\u201d is usually identified by regulated subjects as the most annoying aspect of regulatory compliance and a negative symbol of bureaucracy.\u201d104 This comports with the discussion above about businesses in the Midwestern United States. The approaches to reducing red tape have varied across the developed world and several are outlined below.\nAustralia has utilized a service called, \u201cSmart Forms.\u201d Smart Forms is, \u201can online forms development, hosting and support service.\u201d105 The service manages online submission of information and streamlines the information collection process by automatically filling in fields that the respondent has provided elsewhere (minimizing duplication). For example, the Department of Education and Training in Australia used the service for its \u201cUnique Student Identifier\u201d registry system.106 The Smart Forms service reports burden reductions for training organizations using the system, and greater efficiency for the government.107\nAs part of its \u201cBetter Business Regulation\u201d initiative, Denmark has created a program entitled, \u201cBurden Hunters.\u201d108 Burden hunting involves seeking business input on the development of regulatory requirements in order to understand which requirements are most burdensome and could be scaled back. The Burden Hunter website notes, \u201cAn example of a small well defined\n103 Cutting Red Tape. \"Why is Administrative Simplification So Complicated?.\" OECD (2010). 104 Id. p. 17. 105 See https://www.business.gov.au/For\u2010Government/smart\u2010forms (last viewed May 16, 2018). 106 See https://www.usi.gov.au/ (last viewed June 7, 2018). 107 See https://www.business.gov.au/For\u2010Government/smart\u2010forms/Case\u2010study\u2010Dept\u2010of\u2010Education\u2010and\u2010Training (last viewed May 16, 2018). 108 See https://danishbusinessauthority.dk/burden\u2010hunter\u2010hunting\u2010administrative\u2010burdens\u2010and\u2010red\u2010tape (last viewed May 16, 2018).\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nproblem could be; how do we get more businesses to choose the right NACE109 code when they start up their business? Whereas a more complex problem could be: How do we develop a smart and efficient Consumer law?\u201d110\nSeveral countries have used systematic surveys to attempt to better understand red tape or paperwork burdens. In Belgium, the Federal Planning Bureau does biennial surveys on administrative burden.111 In the United Kingdom, the Department for Business Innovation and Skills has done a regular \u201cBusiness Perceptions Survey\u201d to attempt to better understand the issue of administrative burden. In the latest survey in 2014, more than half of respondents cited paperwork as a burden and 63% saw unnecessary duplication (needing to submit the same information to different offices) as a particular problem.112 These surveys may be the most easily replicable methods for better understanding burden although it should be noted that efforts to measure the effectiveness of these surveys have yet to occur.\nIn 2011, Canada issued a \u201cRed Tape Reduction Action Plan.\u201d113 Among other measures, the plan included a requirement for a \u201cone for one\u201d tradeoff whenever a government agency created a new administrative burden. Agencies would be required to reduce a burden on business equal to the amount of new burden they were imposing. This plan was cited when President Trump\n109 Nomenclature of Economic Activities (see http://www.dst.dk/Site/Dst/Udgivelser/GetPubFile.aspx?id=16251&sid=21dic (last viewed June 7, 2018). 110 Id. 111 See https://www.plan.be/databases/data\u201029\u2010en\u2010the+administrative+burden+in+belgium (last viewed May 16, 2018). 112 See https://www.nao.org.uk/wp\u2010content/uploads/2014/05/Business\u2010perceptions\u2010survey\u20102014.pdf p. 35 (last viewed May 16, 2018). 113 See https://www.canada.ca/en/treasury\u2010board\u2010secretariat/services/federal\u2010regulatory\u2010management/red\u2010tape\u2010 reduction\u2010action\u2010plan.html (last viewed May 16, 2018).\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nrolled out a similar \u201ctwo for one\u201d plan for regulatory relief (which focused on overall costs rather than just paperwork burden).114\nThere have been few meaningful evaluations of these programs. The efforts in Europe in particular point to the fact that there is considerable uncertainty regarding paperwork burden and a need for government to better understand it. Our work with Midwestern manufacturing businesses also highlights that concern in this country. Australia and Canada\u2019s efforts point toward possible solutions but without evaluations of them any recommendations must be tempered.\nThere may also be lessons to be learned from the U.S. States. In Wisconsin, Governors Thompson and Doyle reduced burdens for Medicaid enrollees through auto-enrollment, simplified application systems, and assistance for applicants.115 Practices that reduce burden, particularly by reducing learning costs and the psychological costs associated with completing stigmatizing paperwork may hold particular promise in the social policy sphere.116\nPrinciples for a new Paperwork Reduction Act\nWhen thinking about changes that are necessary to improve the functioning of the Paperwork Reduction Act, the discussion above points toward three principles. These principles derive from the literature on red tape and administrative burden, the problems with the current PRA outlined above, and the experience of other countries and the US states. The first such principle is the need to better understand the burden of information collections, including the cumulative burden. The second is to tailor the requirements of the PRA to focus on the most important and\n114 Federal Register, February 3, 2017, 82 FR 9339 115 Supra note 59. 116 Id.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nmost burdensome collections, as well as the cumulative burden. Finally, the third principle is to eliminate requirements that aren\u2019t improving the quality of information received by government or reducing burden. I detail these principles below and the steps necessary to operationalize them. Understand Burden In Europe, a key pillar of attempts to reduce red tape has been a continual effort to understand the perspective of those affected by government requirements. My work in talking to Midwestern small manufacturers has convinced me that a similar mechanism is needed in the United States. Paperwork requirements have a disproportionate impact on the way the public perceives government.117 Paperwork is ever present in the lives of those complying with regulations in a way that other requirements are not. This is also true for those trying to secure government benefits. While the problem of paperwork has been recognized in scholarship on improving regulation,118 this has not translated to action. The United States was far ahead of the rest of the world in passing the Paperwork Reduction Act in 1980. But Congress has not revised the PRA in 24 years. During that time, other countries have attempted to meaningfully grapple with the challenge of administrative burdens. A key part of these attempts has been the improved understanding of how burden affects the regulated community.119 Currently the United States relies only on the burden estimates (which are rarely\n117 Supra note 102. 118 Sunstein, Cass R. \"Empirically informed regulation.\" The University of Chicago Law Review 78, no. 4 (2011): 1349\u20101429. 119 Supra notes 107\u2010111.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nbased on empirical assessments)120 by federal agencies. The United States should follow the lead of other developed countries and initiate a similar effort to better understand burden.\nA revised Paperwork Reduction Act could task various agencies with (yes, I realize the irony) collecting information from various parties about the paperwork burden imposed by government. For businesses, Congress could delegate the task to the Department of Commerce or the Small Business Administration. Any such requirement should not be a one-time survey but rather a multipronged effort that is regularly repeated to detect changes over time. If burden reduction efforts are undertaken, such surveys would measure their impacts. In addition such surveys can ask about state and local requirements in addition to federal ones to better understand duplication and to allow the government to find opportunities to eliminate it. They could be complemented by more in depth interviews of members of the regulated community and regular efforts to assess the effectiveness of the effort to better understand burden.\nBut the focus of this effort should not be solely on businesses. From a policy perspective financial aid applicants,121 grant recipients,122 and those who receive government benefits123 all also suffer from information collection burdens. The same is almost certainly true of hospitals, schools, local governments, and other types of institutions. For too long, issues surrounding the PRA and regulation generally have generated a partisan divide because of the focus on businesses as the relevant regulated community. By requiring surveys on the burdens government imposes on communities besides businesses, support for a PRA reform is likely to become more bipartisan.\n120 Supra note 5. 121 Supra note 72 122 Supra note 68. 123 Supra note 74.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nSuch a survey will also highlight the issue of cumulative burdens. Our work speaking with Midwestern business owners confirms a significant concern with the cumulative burden of regulation that has been voiced in the academic literature.124 While advocates (and some of the businesses we spoke with) have bemoaned the weight of the many regulations they have to comply with, there has been little systematic research in measuring the impact of these burdens. How much overlap do businesses experience? How much of this overlap is avoidable if different agencies and different levels of government shared information (or made more use of a system like Australia\u2019s Smart Forms125)? How do cumulative burdens affect the hiring and expansion decisions made by businesses? How about for medical patients who must repeatedly provide medical histories, or grant recipients, or anyone who fills out government forms that could be populated with data they have already provided to the government?\nAnswers to these questions will also inform Congressional behavior. As noted above, the primary driver of increased paperwork burden according to the Information Collection Budgets is new statutes passed by Congress. With new work on the impact of burdensome information collection requirements, pressure on Congress to consider this aspect of potential new laws when crafting new statutes will increase.\nFocus on Major Existing and New Burdens \u2013 Tie in with Regulatory Review\nThere are important aspects of the Paperwork Reduction Act that work well. The statistical office within OIRA is seen as improving information collections with a statistical component.126 As described above, in 2017, the PRA was cited in a lawsuit against the Presidential Commission\n124 Supra note 3. 125 Supra note 104. 126 Supra note 5.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\non Election Integrity when the Commission attempted to collect voter data from states.127 Any reform should endeavor to maintain the most successful parts of the PRA.\nBecause of the magnitude of the potential problems caused by paperwork, a mechanism for holding agencies accountable is necessary. The PRA, particularly OIRA review and the requirement for public input on burdensome information collections is the correct mechanism to provide this accountability. OIRA should continue to review collections that impose large burdens or are likely to have significant policy impacts and they should still be subject to public comment.\nMany information collections, particularly burdensome ones, are required by agency regulations. Every president since Ronald Reagan, but particularly Presidents Obama and Trump have emphasized the need for retrospective review of regulations. President Obama\u2019s Executive Order 13563 was particularly detailed in outlining a process for review.128 Such a process involved agencies publicizing a plan for review of existing regulations and reporting regularly to OIRA and the public on the progress of such reviews.129\nUnfortunately, the executive order had at best mixed success.130 In part, this is because retrospective review of regulations is hard to do well.131 Attributing benefits and costs to existing regulations is an area where analysis has lagged. And agencies have political and institutional\n127 United to Protect Democracy et. al. v. Presidential Advisory Commission on Election Integrity motion filed October 11, 2017. See https://3coziq40vafz1kqd5812oc8r\u2010wpengine.netdna\u2010ssl.com/wp\u2010 content/uploads/2017/10/motion\u2010for\u2010preliminary\u2010injunction\u2010pra.pdf (last viewed May 30, 2018). 128 Barack Obama Executive Order 13563 \u201cImproving Regulation and Regulatory Review\u201d https://obamawhitehouse.archives.gov/the\u2010press\u2010office/2011/01/18/executive\u2010order\u201013563\u2010improving\u2010 regulation\u2010and\u2010regulatory\u2010review (last viewed May 30, 2018). 129 Id. 130 Sofie Miller and Susan Dudley, \u201cRegulatory Accretion: Causes and Possible Remedies\u201d Administrative Law Review Accord 67:98\u2010114. 131 Coglianese, Cary. \"Moving Forward with Regulatory Lookback.\" Yale Journal on Regulation 30, no. 57 (2013).\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nincentives to divert resources from retrospective review toward newer initiatives.132 There have been calls for institutionalizing retrospective review of regulation in legislation.133 These have largely been partisan bills however that focus on deregulation of industry and hence have made little progress through Congress.134\nA revised Paperwork Reduction Act may be able to more effectively institute retrospective review. If it is the reporting and recordkeeping requirements of regulations that most infuriate businesses and other regulated parties, perhaps retrospective review should focus on these requirements. A requirement for agencies to regularly reexamine their most burdensome and significant information collections and to report on both the burden imposed and the continued need for such requirements (by showing how the agency used the information) holds the potential for directing retrospective review where it can be most useful. It could also more explicitly bring the practice of cost-benefit analysis to information collection review.135\nThe PRA already attempts to do this of course by requiring renewal of approvals every three years.136 But because these renewals are required for every one of the thousands of information collections, the significant information collections get the same treatment as a requirement that affects 100 people and creates 10 hours of information collection burden. Agency estimates of burden and of explanations of how the information is used are frequently copied and pasted from\n132 Id. 133 The \u201cSearching for and Cutting Regulations that are Unnecessarily Burdensome Act\u201d the SCRUB Act. HR 998 115th Congress. 134 Phillip Wallach, \u201cAn Opportune Moment for Regulatory Reform\u201d Brookings Institute April 2014. https://www.brookings.edu/wp\u2010content/uploads/2016/06/Opportune\u2010Moment\u2010for\u2010Regulatory\u2010 Reform_Wallach.pdf (last viewed May 30, 2018). 135 Sunstein suggests that the PRA require cost\u2010benefit balancing as a part of information collection review. Like Executive Order 12866 for regulations, this could be required of the most burdensome information collections. Supra note 75. 136 Supra Note 1.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nthe last time OMB reviewed the collection. There is no doubt that OIRA tries to prioritize information collection reviews by their impact, but with thousands of such reviews (including customer satisfaction surveys that are given expedited treatments)137 their ability (and the ability of the agencies) to focus on the meaningful collections with a small staff is necessarily limited.\nThe PRA should be reoriented to focus on new information collections proposed by agencies and review of collections that either impose significant burdens or have significant policy impacts. It could require agencies to work with OIRA to come up with a plan for regularly reviewing such requirements. Right now, it is a statute that takes a \u201cone size fits all\u201d approach to information collection review. OIRA has tried to manage this by allowing certain collections to be reviewed using expedited processes and this has helped.138 But the constraints of the PRA do not allow OIRA to use this approach for very many information collections. Hence there are right now 9,375 active information collections (and some of these are generic collections which include many individual surveys).139\nOMB was criticized for its administration of the Federal Reports Act. These criticisms paradoxically included both over-interference in agency affairs and insufficient oversight.140 History is repeating itself and a changed PRA must contain solutions to both of these issues. If the PRA is to be used to more meaningfully review and improve the most burdensome and policy-significant collections, then it also needs to be changed to reduce the review of less significant collections.\nStreamlining the PRA Process\n137 Supra Note 10. 138 Id. 139 As of May 30, 2018 as seen on www.reginfo.gov 140 Supra Note 13.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nThe requirements of the PRA lead to agencies spending six to nine months141 securing approval for information collections regardless of the size or importance of the collection. This has both lead to the creation of bureaucratic structures at cabinet departments designed principally to manage this process and to the deterrence of smaller information collections at the agencies.142 The threshold of requiring PRA clearance for all collections from ten or more individuals has also led to the proliferation of pilot studies involving nine individuals.143\nA revision of the PRA designed to focus on more significant and burdensome collections should give OIRA more flexibility in approving smaller and less burdensome collections. This is particularly true for less significant information collections that have already been in place (as opposed to new collections of information \u2013 where the policy impacts may not be clear) or collections that are clearly necessary to maintain program integrity.144 Toward that end there are three reforms that would allow OIRA and agencies to spend less time on less critical collections. The first two require statutory changes and the third could be done under the current PRA, but a statutory revision could encourage it.\nThe first reform is a change to the limit the PRA currently puts on the duration of approval of information collections. The statute restricts OIRA from approving collections for more than three years.145 Given that it takes agencies six-nine months to navigate the PRA clearance process,146 agencies have approval for just over two years before they must focus on renewing that approval. For many information collections, there are no changes from year to year and\n141 Supra Note 5. 142 Id 143 Id. 144 Supra Note 75. 145 Supra Note 1 at 3507(g) 146 Supra Note 5.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nlittle controversy about them. Extending the amount of time that OIRA can grant approval for an information collection would relieve burden on both OIRA and the agencies by decreasing the number of collections requiring review in any given year. Allowing this longer approval time to be within OIRA\u2019s discretion will ensure that only those collections that OIRA feels are uncontroversial and not in need of more frequent review are approved for longer periods of time.\nThe second reform would reduce the time involved in securing OMB approval. Currently, as described above, there are two required comment periods.147 The first comment period is sixty days long and the second, concurrent with submission of the collection to OIRA is thirty days long.148 This is far more time for public comment than many regulations receive.149 And a smaller percentage of information collections receive public comment than regulations.150 Eliminating the first Federal Register notice (the one that allows sixty days for public comment) would reduce the time needed to secure approval by two months and save budgetary dollars by reducing printing costs for the Federal Register. Given the low comment rate on information collections, there is unlikely to be any adverse consequences from reducing the number of comment periods from two to one.151\nFinally, as described above, the PRA currently allows OMB to delegate information collection review to the agencies.152 OIRA has been reluctant to use this authority, perhaps because of risk\n147 Supra notes 42 and 44. 148 And of course each collection is subject to these public comment periods every three years. 149 The Administrative Procedure Act requires only one comment period, which is typically sixty days long. Occasionally agencies will issue an Advanced Notice of Proposed Rulemaking and have an additional comment period. 150 Supra Note 5. 151 If Congress decided that new collections should still have two comment periods, the PRA could also be modified to reduce the number of comment periods for renewals or modifications of existing collections. 152 Supra note 100.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\naversion typical of a bureaucratic agency.153 A revised PRA could encourage OIRA to explore delegation to agencies that have demonstrated a robust information collection review process. Delegation does not have to be for all information collections, it could be for a subset of collections, such as those below a burden hour threshold or those that are already approved and are not being significantly modified. Delegation would free up OIRA time for review of more significant information collections.\nNine thousand information collections are a huge number to review every three years. OIRA staff has been historically small and the agency also has numerous other functions to perform, most famously regulatory review.154 The reforms listed here could be complemented by increases in the size of OIRA. Such increases though are also needed for OIRA to improve its ability to review regulations and improve regulatory analysis.155 If OIRA were given more resources, it is reasonable to expect that political imperatives would first devote those resources to regulatory review and that information collection supervision would be secondary.156\nThe reforms described in this section, allowing longer approval times for information collections, eliminating a comment period, and encouraging delegation of review for some collections to agencies will free OIRA to spend more time on more significant information collections and on\n153 Downs, Anthony. Inside bureaucracy: A RAND Corporation research study. Waveland Press:, 1967.\n154 Bolton, Alexander, Rachel Augustine Potter, and Sharece Thrower. \"Organizational capacity, regulatory review, and the limits of political control.\" The Journal of Law, Economics, and Organization 32, no. 2 (2015): 242\u2010271. 155 Shapiro, Stuart, and John Morrall. \"Does haste make waste? How long does it take to do a good regulatory impact analysis?.\" Administration & Society 48, no. 3 (2016): 367\u2010389. 156 Samaha supra note 48 at p. 314, \u201cPaperwork burdens almost certainly receive little weight at in most decisions at OMB, OIRA, and the requesting agencies. These officials operate under serious resource constraints and paperwork burdens will usually strike them as low\u2010stakes issues.\u201d\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nits other functions, regardless of the size of OIRA. They will also save time and money at the agencies. And they are unlikely to do anything meaningful to affect the practical utility of information collected by the government.\nConclusions\nPaperwork has no constituency. And yet it proliferates across policy sectors and erodes public confidence in government.157 The recognition that the government should be more careful about imposing burdens on the public goes back to the Federal Reports Act passed early in the era of the administrative state.158 The Paperwork Reduction Act, first passed in 1980 and last updated in 1995, is the most serious attempt in the United States to grapple with this problem.159\nBut it has proven lacking. The burden of providing information to the government continues to increase. It continues to have adverse consequences such as increased hostility toward government regulation (even for causes that are accepted),160 deterrence from applying for financial aid161 for education and from grants for scientific research and making it harder for individuals to escape poverty.162 In an age when even the word, \u201cpaperwork\u201d appears antiquated, it is more than time to revisit the Paperwork Reduction Act.\nA new PRA (perhaps retitled!) would focus on three things. The first is making the understanding of how burden affects those who suffer from it, and how it possibly undermines other policy goals a regular and recurring objective of the act. The second is designing a process\n157 Moynihan, Herd and Harvey (supra note 58) argue that because administrative burden is a hidden cost, it is not debated publicly in the way that other aspects of policy are. They also argue that burden often fulfills the goals of policy\u2010makers by making beneficiaries less likely to take up benefits. 158 Supra Note 12. 159 Supra Note 1. 160 Infra notes 52\u201055. 161 Supra note 72. 162 Supra note 68.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\nthat incorporates this information and focuses on meaningful review (both prospective and retrospective) of information collections that impose the largest burdens and have the most significant policy impacts. The third is ensuring that scarce government resources are not used to support a process that has become excessively bureaucratic and (in many instances) adds little to the tradeoff between the burden on the public and the practical utility of information provided to the government. In an era where most issues appear to be hyper-partisan, a revised PRA, like the original statute in 1980, could be bipartisan. Unnecessary red tape erodes faith in government and democratic processes.163 The problems caused by duplicative, excessive, and poorly understood information collection requirements hurt constituencies that support both major parties. Agencies across the government express dissatisfaction with the current PRA process for approving information collections.164 There is a great deal of policy space for both reducing the burden on the public and creating a process that agencies prefer to the current one. Tackling red tape in a meaningful way by revising the Paperwork Reduction Act is a task that is long overdue.\n163 Supra Note 74. 164 Supra Note 5.\nElectronic copy available at: https://ssrn.com/abstract=3351953\n\n",
        "hash_id": "ea335a4501dae9a82560bf5298c375d0"
    },
    {
        "key": "SZ8VI8G9",
        "version": 15,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/SZ8VI8G9",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/SZ8VI8G9",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/885YWXMU",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 227913
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Shapiro",
            "parsedDate": "2019-03-13",
            "numChildren": 1
        },
        "citation": "<span>Stuart Shapiro, <i>The Case for Reinvigorating the Paperwork Reduction Act</i>, (2019), https://papers.ssrn.com/abstract=3351953 (last visited Nov 14, 2022).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "7TU5MTX4",
        "version": 13,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/7TU5MTX4",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/7TU5MTX4",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/QTGHMCDF",
                "type": "application/json"
            },
            "enclosure": {
                "type": "text/html",
                "href": "https://api.zotero.org/groups/4848934/items/7TU5MTX4/file/view"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            }
        },
        "citation": "<span>How to estimate burden | A Guide to the Paperwork Reduction Act, https://pra.digital.gov/burden/estimation/ (last visited Nov 9, 2022).</span>",
        "fulltext": " Skip to main content \nU.S. flag \n \nAn official website of the United States government \n \nHere\u2019s how you know \nHere's how you know \nDigital.gov Guide Explore \nA Guide to the Paperwork Reduction Act \nMenu \n \n    About the PRA \n    Do I need clearance? \n    PRA approval process \n    Clearance types \n    Estimating burden \n    Additional resources \n \n    Glossary \n    Get PRA Help \n \nSearch Search \nIn this section: \n \n    Estimating Burden \n    Burden activities \n    How to estimate burden \n \nHow to estimate burden \n \nOMB has consistently held that measuring burden is \u201coften difficult and imprecise in the absolute, but reliable and consistent measures of change are possible.\u201d These guidelines and resources are a good starting point for agencies to make the best estimates of burden for their collections. \n \nThe Department of Health and Human Services\u2019 Burden Calculator is a helpful resource for estimation. \nTime (burden hours) \n \nEstimates of burden hours need to include: \n \n    The number of respondents. \n    The frequency of response. \n    The total number of burden hours per year. \n \nBase your burden hour estimates on consultation with a representative sample of potential respondents; do not make a special survey to make an estimate unless directed. \n \nBurden hours may vary widely due to differences in activity, size, or complexity. If they do, include a frequency distribution of expected burden along with the factors that explain why. \n \nTo value and account for the full array of personnel required to plan, develop, prepare, and fulfill an information collection , burden hour estimates fall into four categories of labor: \n \n    Clerical and other unskilled workers \n    Skilled-labor, craft-labor, and other technical workers \n    Professionals and managers \n    Executives \n \nThis includes time spent by all employees, partners, and associates of the respondent. All wages need to be fully-loaded, meaning they reflect the full cost of labor, including benefits. The Bureau of Labor Statistics\u2019 wage data is a good resource to start with. \nFinancial costs and all other aspects of burden \n \nUsing market prices for time and effort, report burden costs that will be carried directly or indirectly by subordinates, associates, agents, or contractors for the respondent. \n \nIf you expect respondents to satisfy some part of a collection through outside consultants, contractors, legal and financial advisors, report the estimate for these services as a lump sum. \n \nThese types of burden need to be estimated in two separate categories: \n \n    Non-recurring or capital cost : one-time investments to fulfill a collection request \n    Recurring or annualized cost : ongoing costs, such as operating or maintaining a capital investment \n \nBurden calculation \n \nWhen submitting burden estimate to OIRA the agency should be prepared to include: \n \n    Estimated burden hours per respondent; \n    Estimated aggregate burden hours; \n    Estimated capital and other non-labor costs per respondent; and \n    Estimated aggregate capital and non-labor costs. \n \nReturn to top \nContact: Get PRA Help | Email us site feedback \nGSA logo Office of Management and Budget \n \npra.digital.gov \n \nAn official website of the U.S. General Services Administration and the Office of Management and Budget \n \n    About GSA \n    Accessibility support \n    FOIA requests \n    No FEAR Act data \n    Office of the Inspector General \n    Performance reports \n    Privacy policy \n \nLooking for U.S. government information and services? \nVisit USA.gov \nClose Hide glossary \nGlossary \nSearch for a PRA term: \n \n    annualized cost \n    Cost per year with one-off costs distributed over the duration of the information collection. \n    burden \n    Time, effort, or financial resources required to generate, maintain, or provide information for a collection. \n    capital cost \n    A one-time expense to buy something needed to answer an information collection request. \n    clearance \n    Permission to collect a specific set of data from the American public. \n    contractor \n    Federal contractors are considered members of the public under the PRA. \n    control number \n    Two four-digit numbers separated by a hyphen. The first four digits identify the sponsoring agency and bureau, and the last four identify the particular collection. \n    de minimis \n    Changes that are purely cosmetic in nature. \n    desk officer \n    The OIRA staff member reviewing an information collection request. \n    exempt \n    Status for collections that do not require OMB approval under specific statutory reasons, despite otherwise meeting the criteria. \n    fast-track \n    The process for approval for a collection already covered by an existing generic clearance. \n    Federal Register \n    The daily journal of the United States government. Its use is required by the PRA to gather comments from the public during the 60- and 30-day comment periods. https://www.federalregister.gov/ \n    generic clearance \n    A broad type of standard clearance issued in advance for a group of similar, low-burden collections. A good fit for surveys. \n    IC \n    A proposal and process for requesting information. PRA approval is granted to collections. \n    ICB \n    OIRA's annual report sent to Congress summarizing the major activies under PRA, and the total burden of information collection \n    ICR \n    Details about a proposed collection to be reviewed by the agency, public, and OIRA desk officers. \n    information collection \n    A proposal and process for requesting information. PRA approval is granted to collections. \n    information collection request \n    Details about a proposed collection to be reviewed by the agency, public, and OIRA desk officers. \n    non-substantive change \n    A change made to an information collection that requires OIRA review and approval but does not require public comment. \n    OIRA \n    Office of Information and Regulatory Affairs, part of the Office of Management and Budget \n    OMB \n    Office of Management and Budget \n    person \n    For purposes regarding the Paperwork Reduction Act, a person can be an individual, individuals representing a partnership, association, corporation, legal representative\u2014or any combination of these. Also includes state, territorial, tribal, or local governments. \n    PII \n    Personally Identifiable Information. Information that can be used to distinguish or trace an individual\u2019s identity, either alone or when combined with other information that is linked or linkable to a specific individual \n    PRA \n    The Paperwork Reduction Act. A law governing how federal agencies collect information from the American public. \n    recordkeeping \n    The practice and process of organizing and maintaining information, documents, or data as required by an agency's guidelines. \n    ROCIS \n    Regulatory Information Service Center (RISC) RISC/OIRA Combined Information System (ROCIS) is the digital platform used by agencies to submit PRA clearance requests to OIRA. \n    sponsor \n    To cause or trigger the collection of information. Agencies sponsor collections. \n    the public \n    People or groups outside of the federal government. Federal employees and military personnel are not considered the public under the PRA. \n \n",
        "hash_id": "847a78781eb97752579d53b9a4a2eb27"
    },
    {
        "key": "QTGHMCDF",
        "version": 11,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/QTGHMCDF",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/QTGHMCDF",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/7TU5MTX4",
                "type": "application/json",
                "attachmentType": "text/html"
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 1
        },
        "citation": "<span>How to estimate burden | A Guide to the Paperwork Reduction Act, https://pra.digital.gov/burden/estimation/ (last visited Nov 9, 2022).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "ESH4ALH5",
        "version": 10,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/ESH4ALH5",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/ESH4ALH5",
                "type": "text/html"
            },
            "attachment": {
                "href": "https://api.zotero.org/groups/4848934/items/XUZYMPAP",
                "type": "application/json",
                "attachmentType": "application/pdf",
                "attachmentSize": 593576
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "creatorSummary": "Fran\u00e7ois et al.",
            "parsedDate": "2020-12",
            "numChildren": 1
        },
        "citation": "<span>Thomas Fran\u00e7ois et al., <i>AMesure: A Web Platform to Assist the Clear Writing of Administrative Texts</i>, <i>in</i> <span style=\"font-variant:small-caps;\">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations</span> 1\u20137 (2020), https://aclanthology.org/2020.aacl-demo.1 (last visited Nov 9, 2022).</span>",
        "fulltext": "",
        "hash_id": null
    },
    {
        "key": "XUZYMPAP",
        "version": 9,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/XUZYMPAP",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/XUZYMPAP",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/ESH4ALH5",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/XUZYMPAP/file/view",
                "title": "Fran\u00e7ois et al. - 2020 - AMesure A Web Platform to Assist the Clear Writin.pdf",
                "length": 593576
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://aclanthology.org/2020.aacl-demo.1.pdf (last visited Nov 9, 2022).</span>",
        "fulltext": "AMesure: a web platform to assist the clear writing of administrative texts\n\nThomas Franc\u00b8ois\n\nAdeline Mu\u00a8 ller\n\nIL&C, CENTAL/TeAMM IL&C, CENTAL\n\nUCLouvain\n\nUCLouvain\n\nEva Rolin IL&C, CENTAL\nUCLouvain\n\nMagali Norre\u00b4 IL&C, CENTAL\nUCLouvain\n\nAbstract\nThis article presents the AMesure platform, which aims to assist writers of French administrative texts in simplifying their writing. This platform includes a readability formula specialized for administrative texts and it also uses various natural language processing (NLP) tools to analyze texts and highlight a number of linguistic phenomena considered dif\ufb01cult to read. Finally, based on the dif\ufb01culties identi\ufb01ed, it offers pieces of advice coming from of\ufb01cial plain language guides to users. This paper describes the different components of the system and reports an evaluation of these components.\n1 Introduction\nIn our current society, written documents play a central role as an information channel, especially in the context of communication between institutions and their target audiences (Madinier, 2009). Unfortunately, although efforts to raise the education level of the population worldwide have increased in recent decades, reports (OECD, 2016) point out that a signi\ufb01cant proportion of citizens still have general reading dif\ufb01culties. As regards administrative texts, various reading issues have been reported. For instance, Kimble (1992) reported that, in a survey carried out in the US, 58% of the respondents admitted to dropping out of an administrative process due to the reading dif\ufb01culty.\nAdministrations have been aware of this issue for decades and have launched various initiatives to address it, the most prominent of which is the Plain Language movement. Plain language aims to increase the accessibility of legal documents for a general audience and has been shown to both reduce costs and please readers (Kimble, 1996). It has not only been promoted through various campaigns (e.g. Plain English Campaign in the UK) and writing guides (Gouvernement du Que\u00b4bec,\n\n2006; Ministe`re de la Communaute\u00b4 franc\u00b8aise de Belgique, 2010; European Union, 2011; Plain Language Action and Information Network, 2011; Cutts, 2020), but also incorporated in some legal principles. However, its widespread application is still undermined due to, for example, the efforts required to train writers (Desbiens, 2008), or the necessity to persuade writers \u2013 especially legal ones \u2013 to abandon their \ufb02owery style, which is seen as a determinant of the image of expertise they project in the reader\u2019s mind (Adler, 2012). This second reason, however, falls beyond the scope of the current study, which aims to address the \ufb01rst reason, i.e. writers\u2019 training.\nRecent research by Nord (2018) revealed that although several plain language guides are available to assist writers of administrative texts in their work, the guidelines provided in these guides are not always followed by writers, mainly because they are too vague and too numerous. To relieve writers from the need to keep all these guidelines in mind, we have designed a web platform, AMesure1, aimed at automatically identifying clear writing issues in administrative texts and providing simple writing advice that is contextually relevant. In its current state, the platform offers the three following functionalities: (1) providing an overall readability score based on a formula specialized to administrative texts; (2) identifying, in a text, linguistic phenomena that are assumed to have a negative effect on the comprehension of the text; (3) for the phenomena detected in step 2, proposing simpli\ufb01cation advice found in plain language guides.\nIn the following sections, we \ufb01rst refer to some related work (Section 2), before describing the NLP analyses carried out to operate the system (Section 3.1). Then, we introduce the system and the way suggestions are provided (Section 3.2). The paper\n1The platform is freely available online at https:// cental.uclouvain.be/amesure/.\n\n1\nProceedings of the 1st Conference of the Asia-Paci\ufb01c Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 1\u20137\nDecember 4 - 7, 2020. c 2020 Association for Computational Linguistics\n\nconcludes with a report about the system performance (Section 4).\n2 Related work\nThis work stands at the intersection between two very different \ufb01elds: writing studies \u2013 \u201cthe interdisciplinary science that studies all the processes and knowledge involved in the production of professional writings and their appropriateness for the addressees\u201d (Labasse, 2001) \u2013 and automatic text simpli\ufb01cation (ATS), a branch of NLP that aims to automatically adapt dif\ufb01cult linguistic structures while preserving the meaning to enhance text accessibility.\nRelevant facts from writing studies have already been covered in the introduction. As regards ATS, the last few years have witnessed the publication of numerous interesting studies, reviewed by Shardlow (2014), Siddharthan (2014), and Saggion (2017). In brief, the \ufb01eld has mainly focused on developing algorithms to automatically simplify complex words (lexical simpli\ufb01cation) and/or complex syntactic structures (syntactic simpli\ufb01cation). It has \ufb01rst relied on rule-based approaches (Chandrasekar et al., 1996; Siddharthan, 2011) in which a text is automatically parsed before being applied simpli\ufb01cation rules de\ufb01ned by experts. Later, ATS has been assimilated to a translation task (the original version is translated into a simpli\ufb01ed version) and addressed with statistical translation systems (Specia, 2010; Zhu et al., 2010). As neural machine translation has emerged under the impulse of deep learning, the Seq2Seq model has become prevalent for ATS too (Nisioi et al., 2017; Zhang and Lapata, 2017).\nSome work has speci\ufb01cally focused on the issue of lexical simpli\ufb01cation, which involves different techniques. Lexical simpli\ufb01cation is generally operated in four steps, the \ufb01rst one being the identi\ufb01cation of complex words. Some systems choose to consider all words as candidates for substitution (Bott et al., 2012); others use a list of complex words or machine learning techniques for classi\ufb01cation of complex words (Alarcon et al., 2019). Once complex words have been identi\ufb01ed, the next step is the generation of simpler synonyms for substitution, either by relying on lexical resources (De Belder and Moens, 2010; Billami et al., 2018), getting candidates from corpora (Coster and Kauchak, 2011), producing them with embeddings (Glavas\u02c7 and S\u02c7 tajner, 2015; Paetzold\n\nand Specia, 2016) or, more recently, with BERT (Qiang et al., 2020). In a next step, the candidates are semantically \ufb01ltered to \ufb01t the context and are ranked according to their dif\ufb01culty by classi\ufb01ers using various word characteristics (e.g. frequency, embedding, morphemes, syllabic structures, etc.) (Paetzold and Specia, 2017; Billami et al., 2018; Qiang et al., 2020).\nAlthough numerous ATS systems are described in publications, we have found only four of them that made their way through a web platform tailored to writers\u2019 needs. Scarton et al. (2010) developed a simpli\ufb01cation web platform for Portuguese, in which the user is able to either accept or reject simpli\ufb01cations done by the system. Similarly, Lee et al. (2016)\u2019s system performs lexical and syntactic simpli\ufb01cations for English and supports human post-editing. More recently, Falkenjack et al. (2017) introduced TeCST, which is able to perform simpli\ufb01cation at different levels, depending on the user. Finally, Yimam and Biemann (2018) implemented a semantic writing aid tool able to suggest context-aware lexical paraphrases to writers. None of these tools, however, have focused on writers of administrative texts, nor on French.\nAMesure could also be related to the family of writing assistants, such as Word or LibreOf\ufb01ce. However, only a few of them provides writing advice based on speci\ufb01c criteria or plain language guides. There are some examples of these tools available for the general public in French: (1) Plainly2; (2) Lirec3 which relies on the FALC guidelines, an equivalent of the Easy-to-Read language in French, tailored to people with a cognitive disability; or (3) Antidote4, which offers various writing advice to be clearer and includes \ufb01ve readability indexes. These are however commercial tools, whose scienti\ufb01c foundations are dif\ufb01cult to know and to compare to.\n3 The platform\nAMesure aims to help writers to produce clear and simple administrative texts for a general audience5.\nFor this purpose, it offers various diagnoses about the reading dif\ufb01culty of a text as well as\n2https://www.labrador-company.fr/ outil-langage-clair/\n3http://sioux.univ-paris8.fr/lirec/ 4https://www.antidote.info/fr 5People with low reading levels require even more simpli\ufb01ed texts (with shorter sentences, no subordinated clauses, etc.). This \u201doversimpli\ufb01cation\u201d falls under the scope of the Easy Language domain.\n\n2\n\nadvice on simpler ways of writing. Before moving to the description of the platform in Section 3.2, we \ufb01rst introduce the various NLP processes used to analyse the text and annotate dif\ufb01culties in Section 3.1.\n3.1 The analysis of the text\nAs soon as a text is uploaded on the platform, it is processed through various NLP tools to get a rich representation of the text, on which further rulebased processes are then applied. In a \ufb01rst step, the text is split into sentences and POS-tagged with MElt (Denis and Sagot, 2012), before being syntactically parsed with the Berkeley parser adapted for French (Candito et al., 2010). As a result, each sentence is represented as a dependency tree, on which we apply a set of handcrafted rules expressed in the form of regular expressions using the Tregex (Levy and Andrew, 2006) syntax. The rules currently implemented (Franc\u00b8ois et al., 2018) are able to identify four classes of complex syntactic structures: passive clauses, relative clauses, object clauses, and adverbial clauses. Identifying these four classes is motivated by the characteristics of administrative texts. Passive clauses and in\ufb01nitive verbs are often used in administrative texts to conceal the presence of the writer (Cusin-Berche, 2003), while other types of clause are used to provide the reader with as many detailed information as possible (Catherine, 1968). Parentheticals are also identi\ufb01ed, as they are prone to hinder the reading process.\nIn a second step, the tagged text is further processed to carry out lexical analyses of the text. During this step, three types of lexical dif\ufb01culties are identi\ufb01ed. Firstly, rare words are detected relying on frequencies from Lexique3 (New et al., 2007), based on a threshold set empirically.\nSecondly, technical terms are detected with some heuristics able to detect both simple terms and multi-word terms \u2013 a task that remains a challenge for current fully automatic approaches (da Silva Conrado et al., 2014) \u2013 that are included in a database. The database has been compiled from three different sources: (1) the of\ufb01cial lists from the Belgian administration; (2) a list of terms extracted from a corpus of 115 administrative texts following the automatic extraction approach of Chung (2003) and then manually validated; and (3) a book describing various characteristics of the administrative style and listing administrative terms (Catherine, 1968). At the end of the collection phase, we\n\nobtained 3,382 terms, some of which could, however, not be considered as dif\ufb01cult (e.g. academy, degree, jury, trainee, etc.). We therefore \ufb01ltered the resource by excluding words found in the list of the 8000 simplest words in French (Gougenheim et al., 1964). As result, the \ufb01nal term database amounts to 2,481 entries.\nThirdly, abbreviations are automatically detected as they are known to produce reading errors, especially when they are used by specialized writers to communicate to non-specialized readers. For instance, Sinha et al. (2011) report that the Joint Commission on Accreditation of Healthcare Organizations estimated that 5% of medical errors are due to abbreviations. In our system, abbreviations are detected based on an abbreviation database, collected from Belgian public authorities. The database relate the extended version(s) of abbreviations (e.g. communaute\u00b4 franc\u00b8aise, Institutions publiques de protection de la jeunesse) with the corresponding abbreviated forms (e.g. comm. fr.; IPPJ and I.P.P.J. respectively). The list provided by public authorities was supplemented via a semiautomatic extraction process applied to our corpus of 115 administrative texts. This extraction process was based on manual rules maximizing the recall, in order to extract all forms prone to be abbreviations. Then, we \ufb01ltered out all forms already in our list and manually checked the remaining ones, obtaining a \ufb01nal database with 2,022 entries.\n3.2 Description of the platform\nLeveraging the NLP analysis described above, the AMesure platform provides four types of diagnoses about texts to its users, as illustrated in Figure 1. The \ufb01rst diagnosis (marked by the letter A in the Figure 1) is a global readability score for the text. It is computed by a readability formula, specialized for administrative texts, that we previously developed (Franc\u00b8ois et al., 2014). The output score ranges from 1 (for very easy texts) to 5 (for very complex texts) and is yielded by a support vector machine classi\ufb01er combining 10 linguistic features of the text (e.g. word frequency, proportion of complex words, type-token ratio, mean length of sentence, ratio of past participle forms, etc.).\nThe second type of diagnosis (letter B in Figure 1) is more detailed and includes 11 readability yardsticks, each corresponding to one linguistic characteristic of the text known to affect reading. The psycholinguistic rationales for the choice of\n\n3\n\nthese yardsticks have been discussed in length in Franc\u00b8ois (2011), who has de\ufb01ned a set of 344 variables. Among this set, we have retained 11 yardsticks based on a correlational analysis (Franc\u00b8ois et al., 2014). In the interface, the yardsticks are organised according to three linguistic dimensions of texts: lexicon, syntax, and textual aspects. The \ufb01ve lexical yardsticks capture (1) the percentage of dif\ufb01cult words, based on the list of 8000 simplest words in French (Gougenheim et al., 1964); (2) the number of rare words (see Section 3.1); (3) the density of abbreviations (see Section 3.1); (4) the proportion of unexplained abbreviations; and (5) the number of technical words (see Section 3.1). The four syntactic yardsticks include (1) the dif\ufb01culty of the syntactic structures estimated roughly as the ratio of conjunctions and pronouns; (2) the mean number of words per sentence; (3) the ratio of structures considered as complex by plain language guides among all syntactic structures detected (see Section 3.1); and (4) the total number of sentences. As regards the two textual yardsticks, they include (1) a score corresponding to the level of personalization of the texts (text using pronouns at the \ufb01rst or at the second person are considered to be more readable (Daoust et al., 1996)); and (2) a score corresponding to the average intersentential coherence of the text. It is measured as the average cosine score between all adjacent sentences of the text, each of them being represented as a vector in a latent space (Foltz et al., 1998).\nTo render all these yardsticks more visual and more understandable, we project each of them on a \ufb01ve-degree scale, represented by colored feathers. The more feathers a yardstick gets, the more complex this linguistic dimension is supposed to be for reading. To transform the yardstick values into a \ufb01ve-degree scale, we applied the following method. Our corpus of 115 administrative texts has been annotated by experts on a \ufb01ve-degree dif\ufb01culty scale (Franc\u00b8ois et al., 2014). For each of our 11 yardsticks, we then estimated its Gaussian distribution (mean and standard deviation) on the corpus for each of the \ufb01ve levels. At running time, we simply compute the probability of the yardstick score for a given text to be generated by each of these \ufb01ve Gaussians and assign it the level corresponding to the higher probability.\nThe third type of diagnosis allows to directly visualize the text in which all complex phenomena annotated during the analysis step (see Section 3.1)\n\nFigure 1: Result of a text analysis in AMesure.\nare underlined, namely the three types of subordinated clauses, passives, parentheticals, rare words, abbreviations, and technical terms. For each of these categories, AMesure allows the user to select a tab showing only the respective phenomenon. It also offers a global view of the text in which complex sentences are highlighted in various shades of yellow (see letter C in Figure 1): the darker the yellow, the more dif\ufb01cult the sentence is to read.\nFinally, the last functionality offers writing advice related to the complex phenomena detected (letter D in Figure 1). Two forms of advice are provided. On the one hand, we apply a list of 7 rules to \ufb01lter out syntactic structures detected during the NLP analysis that should not be considered as complex. For instance, in\ufb01nitive, participial, or even object clauses can be very short (e.g. quand on de\u00b4cide d\u2019avoir un be\u00b4be\u00b4 or le logement qu\u2019il occupe) and are therefore not at all a burden for reading. The \ufb01ltering rules were de\ufb01ned based on writing guidelines from three plain language guides for French (Gouvernement du Que\u00b4bec, 2006; Ministe`re de la Communaute\u00b4 franc\u00b8aise de Belgique, 2010; European Union, 2011). We also extracted\n\n4\n\nfrom these guides some pieces of advice that are shown to users of the platform when a dif\ufb01cult syntactic phenomenon is detected. Examples of advice are: \u201cThis sentence has 50 words. Please avoid sentences longer than 15 words\u201d or \u201cThis sentence has three subordinate clauses. Please avoid having so many subordinate clauses in a sentence\u201d. On the other hand, we also offer simpler synonyms for words detected as rare words or technical words. The synonyms are taken from ReSyf (Billami et al., 2018), a lexical resource in which synonyms are ranked by dif\ufb01culty. For now, we show the three simpler synonyms found in ReSyf for a given dif\ufb01cult word, letting the user to pick the best one. More advanced methods based on embeddings are, however, considered at the moment to improve the automatic selection.\n4 Evaluation of the system\nTo assess the performance of the various extraction algorithms included in our platform, three linguists manually annotated, in 24 administrative texts, the following \ufb01ve phenomena: passive structures, relative clauses, object clauses, adverbial clauses, and abbreviations6. The work of annotators was supported by guidelines focusing on dif\ufb01cult cases7. At the end of the annotation process, the expert agreement was evaluated using Fleiss\u2019 kappa (see Table 1). The agreement was high for the rather easy tasks of annotating abbreviations and passive clauses. Detecting subordinate clauses is, however, a much more complex task, if only because it is also necessary to identify the type of structures. A common reference version was then built through consensus-building.\nThis gold-standard version of the annotation was manually compared to the output of AMesure for the 24 texts in the test set. Table 1 reports the results of this evaluation in terms of recall, precision, and F1-score for the different types of structures. Performance for the detection of passive clauses, relative clauses, adverbial clauses and abbreviations are satisfactory (F1 is above .8). By comparison, Zilio et al. (2017), who detect syntactic structures in English, obtained a precision of 0.88\n6The detection of rare words and complex technical terms could not be assessed according to the same protocol as what matters is the psychological relevance of their identi\ufb01cation for a given audience of readers. Further experiments with subjects are required to assess these two dimensions.\n7For instance, in\ufb01nitive clauses led by a semi-modal auxiliary such as devoir (ought to) or pouvoir (can) were discussed, as contradictory points of view can be found in grammars.\n\nPhenomena\n\nR P F1 \u03ba\n\nPassive clauses\n\n0.92 0.92 0.92 0.92\n\nSubordinated clauses (all) 0.84 0.87 0.85 0.47\n\nRelative clauses\n\n0.83 0.88 0.86 /\n\nObject clauses\n\n0.56 0.42 0.48 /\n\nAdverbial clauses\n\n0.78 0.83 0.8 /\n\nAbbreviations\n\n0.73 0.9 0.8 0.97\n\nTotal (macro-average)\n\n0.83 0.9 0.86 0.79\n\nTable 1: Recall (R), precision (P), F1, percentage of agreement and Fleiss\u2019 \u03ba scores for the \ufb01ve phenomena detected in the platform.\n\nand a recall of 0.62 for the relative clauses and a recall of 0.66 and a precision of 0.94 for in\ufb01nitive clauses introduced by the particle \u201dTO\u201d. Chinkina and Meurers (2016) reached a recall of 0.83 and a precision of 0.71 for relative clauses. However, our system has trouble detecting object clauses, which have a F1-score of only 0.48. Investigation of the confusion matrix reveals that 77% of object clauses (37 out of 48) are correctly detected by AMesure, but 17 out of 37 are wrongly classi\ufb01ed as adverbial clauses. This is a limited issue, as advice can still be provided even if the system gets the type of clause wrong.\n5 Conclusion\nWe have presented the AMesure system, which automatically analyzes the readability of French administrative texts based on classic readability metrics, but also on guidelines from plain language books. The system is freely available through a web platform and is aimed to help writers of administrative texts to produce more accessible documents and forms. To that purpose, it offers a global readability score for the texts, 11 readability yardsticks, a detailed diagnosis in which dif\ufb01cult linguistic words and syntactic structures are highlighted, and some plain language advice. We also carried out a manual evaluation of the system based on 24 administrative texts annotated by linguists. Performance is satisfactory, except as regards the identi\ufb01cation of object clauses. More work is needed on this category, especially to distinguish it from adverbial clauses. We also plan to improve the system providing simpler synonyms by adding a semantic \ufb01lter based on embedding models. Finally, we plan to conduct a study with real writers of administrative texts to measure the perceived usefulness of AMesure as a whole, but also the usefulness of each functionality.\n\n5\n\nAcknowledgments\nWe would like to thank the \u201dDirection de la language franc\u00b8aise\u201d from the Federation WalloniaBrussels for its continued support to the AMesure project since 2014. We also want to acknowledge the wonderful work of Romain Pattyn, Gae\u00a8tan Ansotte, Baptiste Degryse on the interface and the great visuals of Brian Delme\u00b4e.\nReferences\nM. Adler. 2012. The plain language movement. In The Oxford handbook of language and law.\nR. Alarcon, L. Moreno, I. Segura-Bedmar, and P. Martinez. 2019. Lexical simpli\ufb01cation approach using easy-to-read resources. Procesamiento de Lenguaje Natural, 63:95\u2013102.\nM. Billami, T. Franc\u00b8ois, and N. Gala. 2018. ReSyf: A French lexicon with ranked synonyms. In Proceedings of COLING 2018.\nS. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012. Can Spanish Be Simpler? LexSiS: Lexical Simpli\ufb01cation for Spanish. pages 357\u2013374.\nM. Candito, J. Nivre, P. Denis, and E. H. Anguiano. 2010. Benchmarking of statistical dependency parsers for French. In Proceedings of COLING 2010, pages 108\u2013116.\nR. Catherine. 1968. Le style administratif. A. Michel.\nR. Chandrasekar, C. Doran, and B. Srinivas. 1996. Motivations and methods for text simpli\ufb01cation. In Proceedings of the 16th conference on Computational Linguistics, volume 2, pages 1041\u20131044.\nM. Chinkina and D. Meurers. 2016. Linguistically aware information retrieval: Providing input enrichment for second language learners. In Proceedings of BEA 2016, pages 188\u2013198.\nT.M. Chung. 2003. A corpus comparison approach for terminology extraction. Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication, 9(2):221\u2013246.\nW. Coster and D. Kauchak. 2011. Simple English Wikipedia: A new text simpli\ufb01cation task. In Proceedings of ACL 2011, pages 665\u2013669. Association for Computational Linguistics.\nF. Cusin-Berche. 2003. Les mots et leurs contextes. Presses Sorbonne nouvelle.\n\nF. Daoust, L. Laroche, and L. Ouellet. 1996. SATOCALIBRAGE: Pre\u00b4sentation d\u2019un outil d\u2019assistance au choix et a` la re\u00b4daction de textes pour l\u2019enseignement. Revue que\u00b4be\u00b4coise de linguistique, 25(1):205\u2013234.\nJ. De Belder and M.-F. Moens. 2010. Text simpli\ufb01cation for children. In Proceedings of the SIGIR workshop on accessible search systems, pages 19\u201326.\nP. Denis and B. Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art POS tagging. Language resources and evaluation, 46(4):721\u2013736.\nK. Desbiens. 2008. Les obstacles a` la simpli\ufb01cation: le cas des membres du centre d\u2019expertise des grands organismes. Langue, me\u00b4diation et ef\ufb01cacite\u00b4 communicationnelle. Que\u00b4bec.\nEuropean Union. 2011. How to write clearly . Publications Of\ufb01ce of the EU, Luxembourg.\nJ. Falkenjack, E. Rennes, D. Fahlborg, V. Johansson, and A. Jo\u00a8nsson. 2017. Services for text simpli\ufb01cation and analysis. In Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 309\u2013313.\nP.W. Foltz, W. Kintsch, and T.K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse processes, 25(2):285\u2013307.\nT. Franc\u00b8ois. 2011. Les apports du traitement automatique du langage a` la lisibilite\u00b4 du franc\u00b8ais langue e\u00b4trange`re. Ph.D. thesis, Universite\u00b4 Catholique de Louvain. Thesis Supervisors : Ce\u00b4drick Fairon and Anne Catherine Simon.\nT. Franc\u00b8ois, L. Brouwers, H. Naets, and C. Fairon. 2014. AMesure: une formule de lisibilite\u00b4 pour les textes administratifs. In Proceedings of TALN 2014.\nT. Franc\u00b8ois, A. Mu\u00a8ller, B. Degryse, and C. Fairon. 2018. Amesure : une plateforme web d\u2019assistance a` la re\u00b4daction simple de textes administratifs. Repe`res DoRiF, 16(1).\nG. Glavas\u02c7 and S. S\u02c7 tajner. 2015. Simplifying lexical simpli\ufb01cation: Do we need simpli\ufb01ed corpora? In Proceedings of ACL-IJCNLP 2015, pages 63\u201368.\nG. Gougenheim, R. Miche\u00b4a, P. Rivenc, and A. Sauvageot. 1964. L\u2019e\u00b4laboration du franc\u00b8ais fondamental (1er degre\u00b4). Didier, Paris.\nGouvernement du Que\u00b4bec. 2006. Re\u00b4diger simplement - Principes et recommandations pour une langue administrative de qualite\u00b4 . Bibliothe`ques et archives nationales du Que\u00b4bec, Que\u00b4bec.\nJ. Kimble. 1992. Plain english: A charter for clear writing. TM Cooley L. Rev., 9:1.\n\nM. Cutts. 2020. Oxford guide to plain English. Oxford J. Kimble. 1996. Writing for dollars, writing to please.\n\nUniversity Press, USA.\n\nScribes J. Leg. Writing, 6:1.\n\n6\n\nB. Labasse. 2001. L\u2019institution contre l\u2019auteur : pertinence et contraintes en re\u00b4daction professionnelle. In Congre`s annuel de l\u2019ACPRST/CATTW, Universite\u00b4 Laval, Que\u00b4bec.\nJ. Lee, W. Zhao, and W. Xie. 2016. A customizable editor for text simpli\ufb01cation. In Proceedings of COLING 2016: System Demonstrations, pages 93\u201397.\nR. Levy and G. Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of LREC 2006, pages 2231\u2013 2234.\nB. Madinier. 2009. Langage administratif et modernisation de l\u2019e\u00b4tat : pour une communication re\u00b4ussie. In La communication avec le citoyen : ef\ufb01cace et accessible ? Actes du colloque de Lie`ge 2009, pages 55\u201366.\nMiniste`re de la Communaute\u00b4 franc\u00b8aise de Belgique. 2010. E\u00b4crire pour e\u02c6tre lu - Comment re\u00b4diger des textes administratifs faciles a` comprendre ? Ingber, Damar, Bruxelles.\nB. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007. The use of \ufb01lm subtitles to estimate word frequencies. Applied Psycholinguistics, 28(04):661\u2013677.\nS. Nisioi, S. S\u02c7 tajner, S. P. Ponzetto, and L.P. Dinu. 2017. Exploring neural text simpli\ufb01cation models. In Proceedings of ACL2017: Short Papers), pages 85\u201391.\nA. Nord. 2018. Plain language and professional writing : A research overview. Technical report, Language Council of Sweden.\nOECD. 2016. Skills Matter : Further Results from the Survey of Adult Skills. OECD Publishing, Paris.\nG. Paetzold and L. Specia. 2016. Unsupervised lexical simpli\ufb01cation for non-native speakers. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence.\nG. Paetzold and L. Specia. 2017. Lexical simpli\ufb01cation with neural ranking. In Proceedings of EACL2017: Short Papers, pages 34\u201340.\n\nM. Shardlow. 2014. A survey of automated text simpli\ufb01cation. International Journal of Advanced Computer Science and Applications, 4(1):58\u201370.\nA. Siddharthan. 2011. Text simpli\ufb01cation using typed dependencies: A comparison of the robustness of different generation strategies. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 2\u201311.\nA. Siddharthan. 2014. A survey of research on text simpli\ufb01cation. ITL-International Journal of Applied Linguistics, 165(2):259\u2013298.\nM. da Silva Conrado, A. Di Felippo, T.A.S. Pardo, and S.O. Rezende. 2014. A survey of automatic term extraction for brazilian portuguese. Journal of the Brazilian Computer Society, 20(1):12.\nS. Sinha, F. McDermott, G. Srinivas, and P.W.J. Houghton. 2011. Use of abbreviations by healthcare professionals: what is the way forward? Postgraduate medical journal, 87(1029):450\u2013452.\nL. Specia. 2010. Translating from Complex to Simpli\ufb01ed Sentences. In Proceedings of Propor 2010., pages 30\u201339.\nS.M. Yimam and C. Biemann. 2018. Demonstrating par4sem-a semantic writing aid with adaptive paraphrasing. In Proceedings of EMNLP 2018: System Demonstrations, pages 48\u201353.\nX. Zhang and M. Lapata. 2017. Sentence simpli\ufb01cation with deep reinforcement learning. In Proceedings of EMNLP 2017, pages 584\u2013594.\nZ. Zhu, D. Bernhard, and I. Gurevych. 2010. A monolingual tree-based translation model for sentence simpli\ufb01cation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1353\u20131361. Association for Computational Linguistics.\nL. Zilio, R. Wilkens, and C. Fairon. 2017. Using nlp for enhancing second language acquisition. In Proceedings of RANLP 2017, pages 839\u2013846.\n\nPlain Language Action and Information Network. 2011. Federal plain language guidelines.\n\nJ. Qiang, Y. Li, Y. Zhu, Y. Yuan, and X. Wu. 2020. Lexical simpli\ufb01cation with pretrained encoders. In Proceedings of AAAI 2020, pages 8649\u20138656.\n\nH. Saggion. 2017. Automatic text simpli\ufb01cation. Synthesis Lectures on Human Language Technologies, 10(1):1\u2013137.\n\nC. Scarton, M. Oliveira, A. Candido Jr, C. Gasperin, and S. Alu\u00b4\u0131sio. 2010. Simpli\ufb01ca: a tool for authoring simpli\ufb01ed texts in brazilian portuguese guided by readability assessments. In Proceedings of the NAACL HLT 2010: Demonstration Session, pages 41\u201344.\n\n7\n\n",
        "hash_id": "7a436c340a130c951bf0b19a0b7ed49a"
    },
    {
        "key": "543GZRIT",
        "version": 6,
        "library": {
            "type": "group",
            "id": 4848934,
            "name": "Suffolk LIT Lab",
            "links": {
                "alternate": {
                    "href": "https://www.zotero.org/groups/suffolk_lit_lab",
                    "type": "text/html"
                }
            }
        },
        "links": {
            "self": {
                "href": "https://api.zotero.org/groups/4848934/items/543GZRIT",
                "type": "application/json"
            },
            "alternate": {
                "href": "https://www.zotero.org/groups/suffolk_lit_lab/items/543GZRIT",
                "type": "text/html"
            },
            "up": {
                "href": "https://api.zotero.org/groups/4848934/items/LD8EEMWR",
                "type": "application/json"
            },
            "enclosure": {
                "type": "application/pdf",
                "href": "https://api.zotero.org/groups/4848934/items/543GZRIT/file/view",
                "title": "Fran\u00e7ois et al. - 2020 - AMesure A Web Platform to Assist the Clear Writin.pdf",
                "length": 593576
            }
        },
        "meta": {
            "createdByUser": {
                "id": 7502536,
                "username": "qsteenhuis",
                "name": "",
                "links": {
                    "alternate": {
                        "href": "https://www.zotero.org/qsteenhuis",
                        "type": "text/html"
                    }
                }
            },
            "numChildren": 0
        },
        "citation": "<span>Full Text PDF, https://aclanthology.org/2020.aacl-demo.1.pdf (last visited Nov 9, 2022).</span>",
        "fulltext": "AMesure: a web platform to assist the clear writing of administrative texts\n\nThomas Franc\u00b8ois\n\nAdeline Mu\u00a8 ller\n\nIL&C, CENTAL/TeAMM IL&C, CENTAL\n\nUCLouvain\n\nUCLouvain\n\nEva Rolin IL&C, CENTAL\nUCLouvain\n\nMagali Norre\u00b4 IL&C, CENTAL\nUCLouvain\n\nAbstract\nThis article presents the AMesure platform, which aims to assist writers of French administrative texts in simplifying their writing. This platform includes a readability formula specialized for administrative texts and it also uses various natural language processing (NLP) tools to analyze texts and highlight a number of linguistic phenomena considered dif\ufb01cult to read. Finally, based on the dif\ufb01culties identi\ufb01ed, it offers pieces of advice coming from of\ufb01cial plain language guides to users. This paper describes the different components of the system and reports an evaluation of these components.\n1 Introduction\nIn our current society, written documents play a central role as an information channel, especially in the context of communication between institutions and their target audiences (Madinier, 2009). Unfortunately, although efforts to raise the education level of the population worldwide have increased in recent decades, reports (OECD, 2016) point out that a signi\ufb01cant proportion of citizens still have general reading dif\ufb01culties. As regards administrative texts, various reading issues have been reported. For instance, Kimble (1992) reported that, in a survey carried out in the US, 58% of the respondents admitted to dropping out of an administrative process due to the reading dif\ufb01culty.\nAdministrations have been aware of this issue for decades and have launched various initiatives to address it, the most prominent of which is the Plain Language movement. Plain language aims to increase the accessibility of legal documents for a general audience and has been shown to both reduce costs and please readers (Kimble, 1996). It has not only been promoted through various campaigns (e.g. Plain English Campaign in the UK) and writing guides (Gouvernement du Que\u00b4bec,\n\n2006; Ministe`re de la Communaute\u00b4 franc\u00b8aise de Belgique, 2010; European Union, 2011; Plain Language Action and Information Network, 2011; Cutts, 2020), but also incorporated in some legal principles. However, its widespread application is still undermined due to, for example, the efforts required to train writers (Desbiens, 2008), or the necessity to persuade writers \u2013 especially legal ones \u2013 to abandon their \ufb02owery style, which is seen as a determinant of the image of expertise they project in the reader\u2019s mind (Adler, 2012). This second reason, however, falls beyond the scope of the current study, which aims to address the \ufb01rst reason, i.e. writers\u2019 training.\nRecent research by Nord (2018) revealed that although several plain language guides are available to assist writers of administrative texts in their work, the guidelines provided in these guides are not always followed by writers, mainly because they are too vague and too numerous. To relieve writers from the need to keep all these guidelines in mind, we have designed a web platform, AMesure1, aimed at automatically identifying clear writing issues in administrative texts and providing simple writing advice that is contextually relevant. In its current state, the platform offers the three following functionalities: (1) providing an overall readability score based on a formula specialized to administrative texts; (2) identifying, in a text, linguistic phenomena that are assumed to have a negative effect on the comprehension of the text; (3) for the phenomena detected in step 2, proposing simpli\ufb01cation advice found in plain language guides.\nIn the following sections, we \ufb01rst refer to some related work (Section 2), before describing the NLP analyses carried out to operate the system (Section 3.1). Then, we introduce the system and the way suggestions are provided (Section 3.2). The paper\n1The platform is freely available online at https:// cental.uclouvain.be/amesure/.\n\n1\nProceedings of the 1st Conference of the Asia-Paci\ufb01c Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations, pages 1\u20137\nDecember 4 - 7, 2020. c 2020 Association for Computational Linguistics\n\nconcludes with a report about the system performance (Section 4).\n2 Related work\nThis work stands at the intersection between two very different \ufb01elds: writing studies \u2013 \u201cthe interdisciplinary science that studies all the processes and knowledge involved in the production of professional writings and their appropriateness for the addressees\u201d (Labasse, 2001) \u2013 and automatic text simpli\ufb01cation (ATS), a branch of NLP that aims to automatically adapt dif\ufb01cult linguistic structures while preserving the meaning to enhance text accessibility.\nRelevant facts from writing studies have already been covered in the introduction. As regards ATS, the last few years have witnessed the publication of numerous interesting studies, reviewed by Shardlow (2014), Siddharthan (2014), and Saggion (2017). In brief, the \ufb01eld has mainly focused on developing algorithms to automatically simplify complex words (lexical simpli\ufb01cation) and/or complex syntactic structures (syntactic simpli\ufb01cation). It has \ufb01rst relied on rule-based approaches (Chandrasekar et al., 1996; Siddharthan, 2011) in which a text is automatically parsed before being applied simpli\ufb01cation rules de\ufb01ned by experts. Later, ATS has been assimilated to a translation task (the original version is translated into a simpli\ufb01ed version) and addressed with statistical translation systems (Specia, 2010; Zhu et al., 2010). As neural machine translation has emerged under the impulse of deep learning, the Seq2Seq model has become prevalent for ATS too (Nisioi et al., 2017; Zhang and Lapata, 2017).\nSome work has speci\ufb01cally focused on the issue of lexical simpli\ufb01cation, which involves different techniques. Lexical simpli\ufb01cation is generally operated in four steps, the \ufb01rst one being the identi\ufb01cation of complex words. Some systems choose to consider all words as candidates for substitution (Bott et al., 2012); others use a list of complex words or machine learning techniques for classi\ufb01cation of complex words (Alarcon et al., 2019). Once complex words have been identi\ufb01ed, the next step is the generation of simpler synonyms for substitution, either by relying on lexical resources (De Belder and Moens, 2010; Billami et al., 2018), getting candidates from corpora (Coster and Kauchak, 2011), producing them with embeddings (Glavas\u02c7 and S\u02c7 tajner, 2015; Paetzold\n\nand Specia, 2016) or, more recently, with BERT (Qiang et al., 2020). In a next step, the candidates are semantically \ufb01ltered to \ufb01t the context and are ranked according to their dif\ufb01culty by classi\ufb01ers using various word characteristics (e.g. frequency, embedding, morphemes, syllabic structures, etc.) (Paetzold and Specia, 2017; Billami et al., 2018; Qiang et al., 2020).\nAlthough numerous ATS systems are described in publications, we have found only four of them that made their way through a web platform tailored to writers\u2019 needs. Scarton et al. (2010) developed a simpli\ufb01cation web platform for Portuguese, in which the user is able to either accept or reject simpli\ufb01cations done by the system. Similarly, Lee et al. (2016)\u2019s system performs lexical and syntactic simpli\ufb01cations for English and supports human post-editing. More recently, Falkenjack et al. (2017) introduced TeCST, which is able to perform simpli\ufb01cation at different levels, depending on the user. Finally, Yimam and Biemann (2018) implemented a semantic writing aid tool able to suggest context-aware lexical paraphrases to writers. None of these tools, however, have focused on writers of administrative texts, nor on French.\nAMesure could also be related to the family of writing assistants, such as Word or LibreOf\ufb01ce. However, only a few of them provides writing advice based on speci\ufb01c criteria or plain language guides. There are some examples of these tools available for the general public in French: (1) Plainly2; (2) Lirec3 which relies on the FALC guidelines, an equivalent of the Easy-to-Read language in French, tailored to people with a cognitive disability; or (3) Antidote4, which offers various writing advice to be clearer and includes \ufb01ve readability indexes. These are however commercial tools, whose scienti\ufb01c foundations are dif\ufb01cult to know and to compare to.\n3 The platform\nAMesure aims to help writers to produce clear and simple administrative texts for a general audience5.\nFor this purpose, it offers various diagnoses about the reading dif\ufb01culty of a text as well as\n2https://www.labrador-company.fr/ outil-langage-clair/\n3http://sioux.univ-paris8.fr/lirec/ 4https://www.antidote.info/fr 5People with low reading levels require even more simpli\ufb01ed texts (with shorter sentences, no subordinated clauses, etc.). This \u201doversimpli\ufb01cation\u201d falls under the scope of the Easy Language domain.\n\n2\n\nadvice on simpler ways of writing. Before moving to the description of the platform in Section 3.2, we \ufb01rst introduce the various NLP processes used to analyse the text and annotate dif\ufb01culties in Section 3.1.\n3.1 The analysis of the text\nAs soon as a text is uploaded on the platform, it is processed through various NLP tools to get a rich representation of the text, on which further rulebased processes are then applied. In a \ufb01rst step, the text is split into sentences and POS-tagged with MElt (Denis and Sagot, 2012), before being syntactically parsed with the Berkeley parser adapted for French (Candito et al., 2010). As a result, each sentence is represented as a dependency tree, on which we apply a set of handcrafted rules expressed in the form of regular expressions using the Tregex (Levy and Andrew, 2006) syntax. The rules currently implemented (Franc\u00b8ois et al., 2018) are able to identify four classes of complex syntactic structures: passive clauses, relative clauses, object clauses, and adverbial clauses. Identifying these four classes is motivated by the characteristics of administrative texts. Passive clauses and in\ufb01nitive verbs are often used in administrative texts to conceal the presence of the writer (Cusin-Berche, 2003), while other types of clause are used to provide the reader with as many detailed information as possible (Catherine, 1968). Parentheticals are also identi\ufb01ed, as they are prone to hinder the reading process.\nIn a second step, the tagged text is further processed to carry out lexical analyses of the text. During this step, three types of lexical dif\ufb01culties are identi\ufb01ed. Firstly, rare words are detected relying on frequencies from Lexique3 (New et al., 2007), based on a threshold set empirically.\nSecondly, technical terms are detected with some heuristics able to detect both simple terms and multi-word terms \u2013 a task that remains a challenge for current fully automatic approaches (da Silva Conrado et al., 2014) \u2013 that are included in a database. The database has been compiled from three different sources: (1) the of\ufb01cial lists from the Belgian administration; (2) a list of terms extracted from a corpus of 115 administrative texts following the automatic extraction approach of Chung (2003) and then manually validated; and (3) a book describing various characteristics of the administrative style and listing administrative terms (Catherine, 1968). At the end of the collection phase, we\n\nobtained 3,382 terms, some of which could, however, not be considered as dif\ufb01cult (e.g. academy, degree, jury, trainee, etc.). We therefore \ufb01ltered the resource by excluding words found in the list of the 8000 simplest words in French (Gougenheim et al., 1964). As result, the \ufb01nal term database amounts to 2,481 entries.\nThirdly, abbreviations are automatically detected as they are known to produce reading errors, especially when they are used by specialized writers to communicate to non-specialized readers. For instance, Sinha et al. (2011) report that the Joint Commission on Accreditation of Healthcare Organizations estimated that 5% of medical errors are due to abbreviations. In our system, abbreviations are detected based on an abbreviation database, collected from Belgian public authorities. The database relate the extended version(s) of abbreviations (e.g. communaute\u00b4 franc\u00b8aise, Institutions publiques de protection de la jeunesse) with the corresponding abbreviated forms (e.g. comm. fr.; IPPJ and I.P.P.J. respectively). The list provided by public authorities was supplemented via a semiautomatic extraction process applied to our corpus of 115 administrative texts. This extraction process was based on manual rules maximizing the recall, in order to extract all forms prone to be abbreviations. Then, we \ufb01ltered out all forms already in our list and manually checked the remaining ones, obtaining a \ufb01nal database with 2,022 entries.\n3.2 Description of the platform\nLeveraging the NLP analysis described above, the AMesure platform provides four types of diagnoses about texts to its users, as illustrated in Figure 1. The \ufb01rst diagnosis (marked by the letter A in the Figure 1) is a global readability score for the text. It is computed by a readability formula, specialized for administrative texts, that we previously developed (Franc\u00b8ois et al., 2014). The output score ranges from 1 (for very easy texts) to 5 (for very complex texts) and is yielded by a support vector machine classi\ufb01er combining 10 linguistic features of the text (e.g. word frequency, proportion of complex words, type-token ratio, mean length of sentence, ratio of past participle forms, etc.).\nThe second type of diagnosis (letter B in Figure 1) is more detailed and includes 11 readability yardsticks, each corresponding to one linguistic characteristic of the text known to affect reading. The psycholinguistic rationales for the choice of\n\n3\n\nthese yardsticks have been discussed in length in Franc\u00b8ois (2011), who has de\ufb01ned a set of 344 variables. Among this set, we have retained 11 yardsticks based on a correlational analysis (Franc\u00b8ois et al., 2014). In the interface, the yardsticks are organised according to three linguistic dimensions of texts: lexicon, syntax, and textual aspects. The \ufb01ve lexical yardsticks capture (1) the percentage of dif\ufb01cult words, based on the list of 8000 simplest words in French (Gougenheim et al., 1964); (2) the number of rare words (see Section 3.1); (3) the density of abbreviations (see Section 3.1); (4) the proportion of unexplained abbreviations; and (5) the number of technical words (see Section 3.1). The four syntactic yardsticks include (1) the dif\ufb01culty of the syntactic structures estimated roughly as the ratio of conjunctions and pronouns; (2) the mean number of words per sentence; (3) the ratio of structures considered as complex by plain language guides among all syntactic structures detected (see Section 3.1); and (4) the total number of sentences. As regards the two textual yardsticks, they include (1) a score corresponding to the level of personalization of the texts (text using pronouns at the \ufb01rst or at the second person are considered to be more readable (Daoust et al., 1996)); and (2) a score corresponding to the average intersentential coherence of the text. It is measured as the average cosine score between all adjacent sentences of the text, each of them being represented as a vector in a latent space (Foltz et al., 1998).\nTo render all these yardsticks more visual and more understandable, we project each of them on a \ufb01ve-degree scale, represented by colored feathers. The more feathers a yardstick gets, the more complex this linguistic dimension is supposed to be for reading. To transform the yardstick values into a \ufb01ve-degree scale, we applied the following method. Our corpus of 115 administrative texts has been annotated by experts on a \ufb01ve-degree dif\ufb01culty scale (Franc\u00b8ois et al., 2014). For each of our 11 yardsticks, we then estimated its Gaussian distribution (mean and standard deviation) on the corpus for each of the \ufb01ve levels. At running time, we simply compute the probability of the yardstick score for a given text to be generated by each of these \ufb01ve Gaussians and assign it the level corresponding to the higher probability.\nThe third type of diagnosis allows to directly visualize the text in which all complex phenomena annotated during the analysis step (see Section 3.1)\n\nFigure 1: Result of a text analysis in AMesure.\nare underlined, namely the three types of subordinated clauses, passives, parentheticals, rare words, abbreviations, and technical terms. For each of these categories, AMesure allows the user to select a tab showing only the respective phenomenon. It also offers a global view of the text in which complex sentences are highlighted in various shades of yellow (see letter C in Figure 1): the darker the yellow, the more dif\ufb01cult the sentence is to read.\nFinally, the last functionality offers writing advice related to the complex phenomena detected (letter D in Figure 1). Two forms of advice are provided. On the one hand, we apply a list of 7 rules to \ufb01lter out syntactic structures detected during the NLP analysis that should not be considered as complex. For instance, in\ufb01nitive, participial, or even object clauses can be very short (e.g. quand on de\u00b4cide d\u2019avoir un be\u00b4be\u00b4 or le logement qu\u2019il occupe) and are therefore not at all a burden for reading. The \ufb01ltering rules were de\ufb01ned based on writing guidelines from three plain language guides for French (Gouvernement du Que\u00b4bec, 2006; Ministe`re de la Communaute\u00b4 franc\u00b8aise de Belgique, 2010; European Union, 2011). We also extracted\n\n4\n\nfrom these guides some pieces of advice that are shown to users of the platform when a dif\ufb01cult syntactic phenomenon is detected. Examples of advice are: \u201cThis sentence has 50 words. Please avoid sentences longer than 15 words\u201d or \u201cThis sentence has three subordinate clauses. Please avoid having so many subordinate clauses in a sentence\u201d. On the other hand, we also offer simpler synonyms for words detected as rare words or technical words. The synonyms are taken from ReSyf (Billami et al., 2018), a lexical resource in which synonyms are ranked by dif\ufb01culty. For now, we show the three simpler synonyms found in ReSyf for a given dif\ufb01cult word, letting the user to pick the best one. More advanced methods based on embeddings are, however, considered at the moment to improve the automatic selection.\n4 Evaluation of the system\nTo assess the performance of the various extraction algorithms included in our platform, three linguists manually annotated, in 24 administrative texts, the following \ufb01ve phenomena: passive structures, relative clauses, object clauses, adverbial clauses, and abbreviations6. The work of annotators was supported by guidelines focusing on dif\ufb01cult cases7. At the end of the annotation process, the expert agreement was evaluated using Fleiss\u2019 kappa (see Table 1). The agreement was high for the rather easy tasks of annotating abbreviations and passive clauses. Detecting subordinate clauses is, however, a much more complex task, if only because it is also necessary to identify the type of structures. A common reference version was then built through consensus-building.\nThis gold-standard version of the annotation was manually compared to the output of AMesure for the 24 texts in the test set. Table 1 reports the results of this evaluation in terms of recall, precision, and F1-score for the different types of structures. Performance for the detection of passive clauses, relative clauses, adverbial clauses and abbreviations are satisfactory (F1 is above .8). By comparison, Zilio et al. (2017), who detect syntactic structures in English, obtained a precision of 0.88\n6The detection of rare words and complex technical terms could not be assessed according to the same protocol as what matters is the psychological relevance of their identi\ufb01cation for a given audience of readers. Further experiments with subjects are required to assess these two dimensions.\n7For instance, in\ufb01nitive clauses led by a semi-modal auxiliary such as devoir (ought to) or pouvoir (can) were discussed, as contradictory points of view can be found in grammars.\n\nPhenomena\n\nR P F1 \u03ba\n\nPassive clauses\n\n0.92 0.92 0.92 0.92\n\nSubordinated clauses (all) 0.84 0.87 0.85 0.47\n\nRelative clauses\n\n0.83 0.88 0.86 /\n\nObject clauses\n\n0.56 0.42 0.48 /\n\nAdverbial clauses\n\n0.78 0.83 0.8 /\n\nAbbreviations\n\n0.73 0.9 0.8 0.97\n\nTotal (macro-average)\n\n0.83 0.9 0.86 0.79\n\nTable 1: Recall (R), precision (P), F1, percentage of agreement and Fleiss\u2019 \u03ba scores for the \ufb01ve phenomena detected in the platform.\n\nand a recall of 0.62 for the relative clauses and a recall of 0.66 and a precision of 0.94 for in\ufb01nitive clauses introduced by the particle \u201dTO\u201d. Chinkina and Meurers (2016) reached a recall of 0.83 and a precision of 0.71 for relative clauses. However, our system has trouble detecting object clauses, which have a F1-score of only 0.48. Investigation of the confusion matrix reveals that 77% of object clauses (37 out of 48) are correctly detected by AMesure, but 17 out of 37 are wrongly classi\ufb01ed as adverbial clauses. This is a limited issue, as advice can still be provided even if the system gets the type of clause wrong.\n5 Conclusion\nWe have presented the AMesure system, which automatically analyzes the readability of French administrative texts based on classic readability metrics, but also on guidelines from plain language books. The system is freely available through a web platform and is aimed to help writers of administrative texts to produce more accessible documents and forms. To that purpose, it offers a global readability score for the texts, 11 readability yardsticks, a detailed diagnosis in which dif\ufb01cult linguistic words and syntactic structures are highlighted, and some plain language advice. We also carried out a manual evaluation of the system based on 24 administrative texts annotated by linguists. Performance is satisfactory, except as regards the identi\ufb01cation of object clauses. More work is needed on this category, especially to distinguish it from adverbial clauses. We also plan to improve the system providing simpler synonyms by adding a semantic \ufb01lter based on embedding models. Finally, we plan to conduct a study with real writers of administrative texts to measure the perceived usefulness of AMesure as a whole, but also the usefulness of each functionality.\n\n5\n\nAcknowledgments\nWe would like to thank the \u201dDirection de la language franc\u00b8aise\u201d from the Federation WalloniaBrussels for its continued support to the AMesure project since 2014. We also want to acknowledge the wonderful work of Romain Pattyn, Gae\u00a8tan Ansotte, Baptiste Degryse on the interface and the great visuals of Brian Delme\u00b4e.\nReferences\nM. Adler. 2012. The plain language movement. In The Oxford handbook of language and law.\nR. Alarcon, L. Moreno, I. Segura-Bedmar, and P. Martinez. 2019. Lexical simpli\ufb01cation approach using easy-to-read resources. Procesamiento de Lenguaje Natural, 63:95\u2013102.\nM. Billami, T. Franc\u00b8ois, and N. Gala. 2018. ReSyf: A French lexicon with ranked synonyms. In Proceedings of COLING 2018.\nS. Bott, L. Rello, B. Drndarevic, and H. Saggion. 2012. Can Spanish Be Simpler? LexSiS: Lexical Simpli\ufb01cation for Spanish. pages 357\u2013374.\nM. Candito, J. Nivre, P. Denis, and E. H. Anguiano. 2010. Benchmarking of statistical dependency parsers for French. In Proceedings of COLING 2010, pages 108\u2013116.\nR. Catherine. 1968. Le style administratif. A. Michel.\nR. Chandrasekar, C. Doran, and B. Srinivas. 1996. Motivations and methods for text simpli\ufb01cation. In Proceedings of the 16th conference on Computational Linguistics, volume 2, pages 1041\u20131044.\nM. Chinkina and D. Meurers. 2016. Linguistically aware information retrieval: Providing input enrichment for second language learners. In Proceedings of BEA 2016, pages 188\u2013198.\nT.M. Chung. 2003. A corpus comparison approach for terminology extraction. Terminology. International Journal of Theoretical and Applied Issues in Specialized Communication, 9(2):221\u2013246.\nW. Coster and D. Kauchak. 2011. Simple English Wikipedia: A new text simpli\ufb01cation task. In Proceedings of ACL 2011, pages 665\u2013669. Association for Computational Linguistics.\nF. Cusin-Berche. 2003. Les mots et leurs contextes. Presses Sorbonne nouvelle.\n\nF. Daoust, L. Laroche, and L. Ouellet. 1996. SATOCALIBRAGE: Pre\u00b4sentation d\u2019un outil d\u2019assistance au choix et a` la re\u00b4daction de textes pour l\u2019enseignement. Revue que\u00b4be\u00b4coise de linguistique, 25(1):205\u2013234.\nJ. De Belder and M.-F. Moens. 2010. Text simpli\ufb01cation for children. In Proceedings of the SIGIR workshop on accessible search systems, pages 19\u201326.\nP. Denis and B. Sagot. 2012. Coupling an annotated corpus and a lexicon for state-of-the-art POS tagging. Language resources and evaluation, 46(4):721\u2013736.\nK. Desbiens. 2008. Les obstacles a` la simpli\ufb01cation: le cas des membres du centre d\u2019expertise des grands organismes. Langue, me\u00b4diation et ef\ufb01cacite\u00b4 communicationnelle. Que\u00b4bec.\nEuropean Union. 2011. How to write clearly . Publications Of\ufb01ce of the EU, Luxembourg.\nJ. Falkenjack, E. Rennes, D. Fahlborg, V. Johansson, and A. Jo\u00a8nsson. 2017. Services for text simpli\ufb01cation and analysis. In Proceedings of the 21st Nordic Conference on Computational Linguistics, pages 309\u2013313.\nP.W. Foltz, W. Kintsch, and T.K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse processes, 25(2):285\u2013307.\nT. Franc\u00b8ois. 2011. Les apports du traitement automatique du langage a` la lisibilite\u00b4 du franc\u00b8ais langue e\u00b4trange`re. Ph.D. thesis, Universite\u00b4 Catholique de Louvain. Thesis Supervisors : Ce\u00b4drick Fairon and Anne Catherine Simon.\nT. Franc\u00b8ois, L. Brouwers, H. Naets, and C. Fairon. 2014. AMesure: une formule de lisibilite\u00b4 pour les textes administratifs. In Proceedings of TALN 2014.\nT. Franc\u00b8ois, A. Mu\u00a8ller, B. Degryse, and C. Fairon. 2018. Amesure : une plateforme web d\u2019assistance a` la re\u00b4daction simple de textes administratifs. Repe`res DoRiF, 16(1).\nG. Glavas\u02c7 and S. S\u02c7 tajner. 2015. Simplifying lexical simpli\ufb01cation: Do we need simpli\ufb01ed corpora? In Proceedings of ACL-IJCNLP 2015, pages 63\u201368.\nG. Gougenheim, R. Miche\u00b4a, P. Rivenc, and A. Sauvageot. 1964. L\u2019e\u00b4laboration du franc\u00b8ais fondamental (1er degre\u00b4). Didier, Paris.\nGouvernement du Que\u00b4bec. 2006. Re\u00b4diger simplement - Principes et recommandations pour une langue administrative de qualite\u00b4 . Bibliothe`ques et archives nationales du Que\u00b4bec, Que\u00b4bec.\nJ. Kimble. 1992. Plain english: A charter for clear writing. TM Cooley L. Rev., 9:1.\n\nM. Cutts. 2020. Oxford guide to plain English. Oxford J. Kimble. 1996. Writing for dollars, writing to please.\n\nUniversity Press, USA.\n\nScribes J. Leg. Writing, 6:1.\n\n6\n\nB. Labasse. 2001. L\u2019institution contre l\u2019auteur : pertinence et contraintes en re\u00b4daction professionnelle. In Congre`s annuel de l\u2019ACPRST/CATTW, Universite\u00b4 Laval, Que\u00b4bec.\nJ. Lee, W. Zhao, and W. Xie. 2016. A customizable editor for text simpli\ufb01cation. In Proceedings of COLING 2016: System Demonstrations, pages 93\u201397.\nR. Levy and G. Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of LREC 2006, pages 2231\u2013 2234.\nB. Madinier. 2009. Langage administratif et modernisation de l\u2019e\u00b4tat : pour une communication re\u00b4ussie. In La communication avec le citoyen : ef\ufb01cace et accessible ? Actes du colloque de Lie`ge 2009, pages 55\u201366.\nMiniste`re de la Communaute\u00b4 franc\u00b8aise de Belgique. 2010. E\u00b4crire pour e\u02c6tre lu - Comment re\u00b4diger des textes administratifs faciles a` comprendre ? Ingber, Damar, Bruxelles.\nB. New, M. Brysbaert, J. Veronis, and C. Pallier. 2007. The use of \ufb01lm subtitles to estimate word frequencies. Applied Psycholinguistics, 28(04):661\u2013677.\nS. Nisioi, S. S\u02c7 tajner, S. P. Ponzetto, and L.P. Dinu. 2017. Exploring neural text simpli\ufb01cation models. In Proceedings of ACL2017: Short Papers), pages 85\u201391.\nA. Nord. 2018. Plain language and professional writing : A research overview. Technical report, Language Council of Sweden.\nOECD. 2016. Skills Matter : Further Results from the Survey of Adult Skills. OECD Publishing, Paris.\nG. Paetzold and L. Specia. 2016. Unsupervised lexical simpli\ufb01cation for non-native speakers. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence.\nG. Paetzold and L. Specia. 2017. Lexical simpli\ufb01cation with neural ranking. In Proceedings of EACL2017: Short Papers, pages 34\u201340.\n\nM. Shardlow. 2014. A survey of automated text simpli\ufb01cation. International Journal of Advanced Computer Science and Applications, 4(1):58\u201370.\nA. Siddharthan. 2011. Text simpli\ufb01cation using typed dependencies: A comparison of the robustness of different generation strategies. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 2\u201311.\nA. Siddharthan. 2014. A survey of research on text simpli\ufb01cation. ITL-International Journal of Applied Linguistics, 165(2):259\u2013298.\nM. da Silva Conrado, A. Di Felippo, T.A.S. Pardo, and S.O. Rezende. 2014. A survey of automatic term extraction for brazilian portuguese. Journal of the Brazilian Computer Society, 20(1):12.\nS. Sinha, F. McDermott, G. Srinivas, and P.W.J. Houghton. 2011. Use of abbreviations by healthcare professionals: what is the way forward? Postgraduate medical journal, 87(1029):450\u2013452.\nL. Specia. 2010. Translating from Complex to Simpli\ufb01ed Sentences. In Proceedings of Propor 2010., pages 30\u201339.\nS.M. Yimam and C. Biemann. 2018. Demonstrating par4sem-a semantic writing aid with adaptive paraphrasing. In Proceedings of EMNLP 2018: System Demonstrations, pages 48\u201353.\nX. Zhang and M. Lapata. 2017. Sentence simpli\ufb01cation with deep reinforcement learning. In Proceedings of EMNLP 2017, pages 584\u2013594.\nZ. Zhu, D. Bernhard, and I. Gurevych. 2010. A monolingual tree-based translation model for sentence simpli\ufb01cation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 1353\u20131361. Association for Computational Linguistics.\nL. Zilio, R. Wilkens, and C. Fairon. 2017. Using nlp for enhancing second language acquisition. In Proceedings of RANLP 2017, pages 839\u2013846.\n\nPlain Language Action and Information Network. 2011. Federal plain language guidelines.\n\nJ. Qiang, Y. Li, Y. Zhu, Y. Yuan, and X. Wu. 2020. Lexical simpli\ufb01cation with pretrained encoders. In Proceedings of AAAI 2020, pages 8649\u20138656.\n\nH. Saggion. 2017. Automatic text simpli\ufb01cation. Synthesis Lectures on Human Language Technologies, 10(1):1\u2013137.\n\nC. Scarton, M. Oliveira, A. Candido Jr, C. Gasperin, and S. Alu\u00b4\u0131sio. 2010. Simpli\ufb01ca: a tool for authoring simpli\ufb01ed texts in brazilian portuguese guided by readability assessments. In Proceedings of the NAACL HLT 2010: Demonstration Session, pages 41\u201344.\n\n7\n\n",
        "hash_id": "7a436c340a130c951bf0b19a0b7ed49a"
    }
]